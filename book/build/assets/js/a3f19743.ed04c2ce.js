"use strict";(globalThis.webpackChunkbook_temp=globalThis.webpackChunkbook_temp||[]).push([[795],{839:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module_4/4_2_llm_planning","title":"Chapter 4.2: LLM Cognitive Planning","description":"Implementing large language model cognitive planning for humanoid robots","source":"@site/docs/04_module_4/4_2_llm_planning.mdx","sourceDirName":"04_module_4","slug":"/module_4/4_2_llm_planning","permalink":"/physical-ai-humanoid-robotics-book/docs/module_4/4_2_llm_planning","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/04_module_4/4_2_llm_planning.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Chapter 4.2: LLM Cognitive Planning","description":"Implementing large language model cognitive planning for humanoid robots"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4.1: Voice Input with Whisper","permalink":"/physical-ai-humanoid-robotics-book/docs/module_4/4_1_whisper_voice"},"next":{"title":"Chapter 4.3: Multi-Modal Perception & Decision Making","permalink":"/physical-ai-humanoid-robotics-book/docs/module_4/4_3_multi_modal"}}');var s=t(4848),o=t(8453);const a={sidebar_position:2,title:"Chapter 4.2: LLM Cognitive Planning",description:"Implementing large language model cognitive planning for humanoid robots"},r="Chapter 4.2: LLM Cognitive Planning",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Overview of LLMs in Robotics",id:"overview-of-llms-in-robotics",level:2},{value:"Why LLMs for Robot Planning?",id:"why-llms-for-robot-planning",level:3},{value:"LLM Integration Architecture",id:"llm-integration-architecture",level:2},{value:"System Overview",id:"system-overview",level:3},{value:"Plan Representation",id:"plan-representation",level:3},{value:"Implementation Approach",id:"implementation-approach",level:2},{value:"LLM Selection",id:"llm-selection",level:3},{value:"Prompt Engineering",id:"prompt-engineering",level:3},{value:"Technical Implementation",id:"technical-implementation",level:2},{value:"Custom ROS 2 Message Types",id:"custom-ros-2-message-types",level:3},{value:"LLM Planning Node Implementation",id:"llm-planning-node-implementation",level:3},{value:"LLM Planning Node",id:"llm-planning-node",level:3},{value:"Practical Implementation",id:"practical-implementation",level:2},{value:"Basic Node Structure",id:"basic-node-structure",level:3},{value:"Plan Execution Integration",id:"plan-execution-integration",level:3},{value:"Challenges and Solutions",id:"challenges-and-solutions",level:2},{value:"Response Consistency",id:"response-consistency",level:3},{value:"Real-time Constraints",id:"real-time-constraints",level:3},{value:"Safety and Validation",id:"safety-and-validation",level:3},{value:"Knowledge Integration",id:"knowledge-integration",level:3},{value:"Integration with VLA System",id:"integration-with-vla-system",level:2},{value:"Voice-to-Plan Pipeline",id:"voice-to-plan-pipeline",level:3},{value:"Multi-modal Coordination",id:"multi-modal-coordination",level:3},{value:"Plan Execution and Monitoring",id:"plan-execution-and-monitoring",level:3},{value:"Deployment Considerations",id:"deployment-considerations",level:2},{value:"Edge vs. Cloud Deployment",id:"edge-vs-cloud-deployment",level:3},{value:"Resource Management",id:"resource-management",level:3},{value:"Model Optimization",id:"model-optimization",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"Unit Testing",id:"unit-testing",level:3},{value:"Integration Testing",id:"integration-testing",level:3},{value:"Performance Testing",id:"performance-testing",level:3},{value:"Next Steps",id:"next-steps",level:2},{value:"Summary",id:"summary",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-42-llm-cognitive-planning",children:"Chapter 4.2: LLM Cognitive Planning"})}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:"In this chapter, we'll explore how to implement cognitive planning for humanoid robots using Large Language Models (LLMs). LLMs provide powerful reasoning capabilities that can be leveraged to generate high-level plans and strategies for robot behavior. By integrating LLMs with our ROS 2 system, we create a cognitive layer that can interpret human instructions and generate executable robot plans."}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this chapter, you will:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand the fundamentals of LLMs and their applications in robotics"}),"\n",(0,s.jsx)(n.li,{children:"Learn how to integrate LLMs with ROS 2 for cognitive planning"}),"\n",(0,s.jsx)(n.li,{children:"Implement an LLM planning node that generates robot plans from text"}),"\n",(0,s.jsx)(n.li,{children:"Create custom ROS 2 messages for plan representation"}),"\n",(0,s.jsx)(n.li,{children:"Design prompt engineering techniques for robotics applications"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"overview-of-llms-in-robotics",children:"Overview of LLMs in Robotics"}),"\n",(0,s.jsx)(n.p,{children:"Large Language Models have revolutionized artificial intelligence by demonstrating remarkable capabilities in understanding and generating human language. In robotics, LLMs can serve as cognitive planners that:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Interpret natural language commands"}),"\n",(0,s.jsx)(n.li,{children:"Generate high-level task plans"}),"\n",(0,s.jsx)(n.li,{children:"Reason about object affordances and spatial relationships"}),"\n",(0,s.jsx)(n.li,{children:"Handle ambiguous or incomplete instructions"}),"\n",(0,s.jsx)(n.li,{children:"Adapt to new situations through few-shot learning"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"why-llms-for-robot-planning",children:"Why LLMs for Robot Planning?"}),"\n",(0,s.jsx)(n.p,{children:"LLMs offer several advantages for robot planning:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Natural Language Understanding"}),": Direct interpretation of human commands"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Common-Sense Reasoning"}),": Understanding of physical and social world"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Generalization"}),": Ability to handle novel situations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Knowledge Integration"}),": Access to vast amounts of world knowledge"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"llm-integration-architecture",children:"LLM Integration Architecture"}),"\n",(0,s.jsx)(n.h3,{id:"system-overview",children:"System Overview"}),"\n",(0,s.jsx)(n.p,{children:"Our LLM planning system will consist of:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Input Processing"}),": Receive text commands from voice recognition or direct input"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LLM Interface"}),": Query the LLM with appropriate prompts"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Plan Generation"}),": Convert LLM responses to executable robot plans"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Plan Execution"}),": Interface with ROS 2 navigation and manipulation systems"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"plan-representation",children:"Plan Representation"}),"\n",(0,s.jsx)(n.p,{children:"Robot plans will be represented as sequences of:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Navigation goals"}),"\n",(0,s.jsx)(n.li,{children:"Manipulation actions"}),"\n",(0,s.jsx)(n.li,{children:"Conditional behaviors"}),"\n",(0,s.jsx)(n.li,{children:"Safety constraints"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"implementation-approach",children:"Implementation Approach"}),"\n",(0,s.jsx)(n.h3,{id:"llm-selection",children:"LLM Selection"}),"\n",(0,s.jsx)(n.p,{children:"For our implementation, we'll consider several LLM options:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"OpenAI GPT"}),": High quality but requires API access"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Open Source Models"}),": Llama, Mistral, or Phi for local deployment"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Specialized Robotics Models"}),": Models fine-tuned for robotics tasks"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"prompt-engineering",children:"Prompt Engineering"}),"\n",(0,s.jsx)(n.p,{children:"Effective prompt engineering is crucial for robotics applications:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Provide clear examples of desired behavior"}),"\n",(0,s.jsx)(n.li,{children:"Include robot capabilities and constraints"}),"\n",(0,s.jsx)(n.li,{children:"Structure requests for consistent output format"}),"\n",(0,s.jsx)(n.li,{children:"Handle error cases and edge conditions"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"technical-implementation",children:"Technical Implementation"}),"\n",(0,s.jsx)(n.h3,{id:"custom-ros-2-message-types",children:"Custom ROS 2 Message Types"}),"\n",(0,s.jsx)(n.p,{children:"We'll define custom message types for plan representation. First, let's create the message definition files:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# In robotics_ws/src/vla_llm_planner/msg/LLMPlan.msg\nstring plan_id\nstring description\nbuiltin_interfaces/Time timestamp\nfloat32 confidence\nPlanStep[] steps\n\n# In robotics_ws/src/vla_llm_planner/msg/PlanStep.msg\nstring action_type  # "navigation", "manipulation", "perception", "wait", etc.\nstring action_name   # "move_to", "pick_object", "detect_person", "open_door", etc.\nstring[] action_params  # Additional parameters for the action\ngeometry_msgs/Pose target_pose\nbuiltin_interfaces/Time expected_duration\nstring[] preconditions  # Conditions that must be true before executing\nstring[] effects        # Effects that will be true after executing\nfloat32 success_probability  # Estimated probability of successful execution\n'})}),"\n",(0,s.jsx)(n.h3,{id:"llm-planning-node-implementation",children:"LLM Planning Node Implementation"}),"\n",(0,s.jsx)(n.p,{children:"The LLM planning node will handle the complete pipeline from text commands to structured robot plans:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nimport openai\nimport json\nimport time\nfrom std_msgs.msg import String\nfrom vla_msgs.msg import LLMPlan, PlanStep\nfrom geometry_msgs.msg import Pose\nfrom builtin_interfaces.msg import Time\nimport re\nimport os\nfrom ament_index_python.packages import get_package_share_directory\n\nclass LLMPlannerNode(Node):\n    def __init__(self):\n        super().__init__(\'llm_planner_node\')\n\n        # Initialize LLM client (multiple options supported)\n        self.llm_provider = self.declare_parameter(\'llm_provider\', \'openai\').value\n        self.setup_llm_client()\n\n        # Parameters\n        self.model_name = self.declare_parameter(\'model_name\', \'gpt-3.5-turbo\').value\n        self.max_tokens = self.declare_parameter(\'max_tokens\', 1000).value\n        self.temperature = self.declare_parameter(\'temperature\', 0.3).value\n        self.robot_capabilities_file = self.declare_parameter(\n            \'robot_capabilities_file\',\n            os.path.join(get_package_share_directory(\'vla_llm_planner\'), \'config\', \'robot_capabilities.json\')\n        ).value\n\n        # Load robot capabilities\n        self.robot_capabilities = self.load_robot_capabilities()\n\n        # Publishers and subscribers\n        self.command_sub = self.create_subscription(\n            String,\n            \'command_input\',\n            self.command_callback,\n            10\n        )\n\n        self.plan_pub = self.create_publisher(\n            LLMPlan,\n            \'robot_plan\',\n            10\n        )\n\n        # Optional plan validation feedback\n        self.validation_sub = self.create_subscription(\n            String,\n            \'plan_validation\',\n            self.validation_callback,\n            10\n        )\n\n        self.get_logger().info(f"LLM Planner node initialized with {self.llm_provider} provider")\n\n    def setup_llm_client(self):\n        """Setup LLM client based on provider"""\n        if self.llm_provider == \'openai\':\n            api_key = self.declare_parameter(\'openai_api_key\', \'\').value\n            if api_key:\n                openai.api_key = api_key\n            else:\n                self.get_logger().warn("No OpenAI API key provided - using mock responses")\n        elif self.llm_provider == \'local\':\n            # Setup for local model (e.g., using transformers)\n            from transformers import pipeline\n            self.local_pipeline = pipeline("text-generation", model="microsoft/DialoGPT-medium")\n        # Add other providers as needed\n\n    def load_robot_capabilities(self):\n        """Load robot capabilities from configuration file"""\n        try:\n            with open(self.robot_capabilities_file, \'r\') as f:\n                return json.load(f)\n        except FileNotFoundError:\n            self.get_logger().warn(f"Robot capabilities file not found: {self.robot_capabilities_file}")\n            # Return default capabilities\n            return {\n                "navigation": {\n                    "max_speed": 1.0,\n                    "min_turn_radius": 0.5,\n                    "supported_terrains": ["indoor", "flat", "carpet", "tile"]\n                },\n                "manipulation": {\n                    "max_payload": 5.0,\n                    "reach": 1.2,\n                    "gripper_types": ["parallel", "suction"]\n                },\n                "perception": {\n                    "camera_range": 10.0,\n                    "object_detection": ["person", "cup", "bottle", "chair", "table"],\n                    "language_understanding": True\n                }\n            }\n\n    def command_callback(self, msg):\n        """Process incoming text commands and generate plans"""\n        command = msg.data\n        self.get_logger().info(f"Received command: {command}")\n\n        # Generate plan using LLM\n        start_time = time.time()\n        plan = self.generate_plan(command)\n        generation_time = time.time() - start_time\n\n        if plan:\n            self.get_logger().info(f"Generated plan in {generation_time:.2f}s with {len(plan.steps)} steps")\n            self.plan_pub.publish(plan)\n            self.get_logger().info("Published plan to execution system")\n        else:\n            self.get_logger().error("Failed to generate plan for command")\n\n    def generate_plan(self, command):\n        """Generate robot plan using LLM"""\n        # Create prompt for the LLM with robot capabilities\n        prompt = self.create_prompt(command)\n\n        try:\n            if self.llm_provider == \'openai\' and openai.api_key:\n                # Use OpenAI API\n                response = openai.ChatCompletion.create(\n                    model=self.model_name,\n                    messages=[\n                        {"role": "system", "content": self.get_system_prompt()},\n                        {"role": "user", "content": prompt}\n                    ],\n                    max_tokens=self.max_tokens,\n                    temperature=self.temperature\n                )\n\n                llm_response = response.choices[0].message[\'content\']\n            elif self.llm_provider == \'local\':\n                # Use local model (simplified example)\n                llm_response = self.local_generate(prompt)\n            else:\n                # Mock response for demonstration\n                llm_response = self.mock_plan_response(command)\n\n            # Parse LLM response into structured plan\n            return self.parse_plan_response(llm_response)\n\n        except Exception as e:\n            self.get_logger().error(f"LLM query failed: {e}")\n            return None\n\n    def create_prompt(self, command):\n        """Create structured prompt for LLM with robot capabilities"""\n        return f"""\n        Given the following command: "{command}"\n\n        Generate a step-by-step plan for a humanoid robot with these capabilities:\n        {json.dumps(self.robot_capabilities, indent=2)}\n\n        The robot can perform these high-level actions:\n        - Navigation: move to specific locations, avoid obstacles\n        - Manipulation: grasp objects, open doors, press buttons\n        - Perception: detect objects, recognize people, assess environment\n        - Communication: speak, gesture, acknowledge commands\n\n        Respond in JSON format with the following structure:\n        {{\n            "description": "Brief description of the plan",\n            "steps": [\n                {{\n                    "action_type": "navigation|manipulation|perception|communication",\n                    "action_name": "move_to|pick_object|detect_person|speak|etc",\n                    "action_params": ["param1", "param2"],\n                    "target_pose": {{"x": 0.0, "y": 0.0, "z": 0.0, "qx": 0.0, "qy": 0.0, "qz": 0.0, "qw": 1.0}},\n                    "expected_duration": 10.0,\n                    "preconditions": ["robot_is_idle", "object_in_reach"],\n                    "effects": ["robot_at_location", "object_grasped"],\n                    "success_probability": 0.9\n                }}\n            ]\n        }}\n\n        Ensure all actions are feasible given the robot\'s capabilities.\n        Include safety considerations in the plan.\n        """\n\n    def get_system_prompt(self):\n        """Get system prompt for LLM"""\n        return f"""\n        You are a helpful assistant that generates robot action plans.\n        The robot is a humanoid with the capabilities defined above.\n        Always respond in the exact JSON format requested.\n        Ensure all actions are feasible for the robot given its physical and sensory limitations.\n        Include only actions that the robot is capable of performing based on the provided capabilities.\n        Consider safety constraints in all plans.\n        """\n\n    def parse_plan_response(self, response):\n        """Parse LLM response into structured plan"""\n        try:\n            # Extract JSON from response if needed\n            json_match = re.search(r\'\\{.*\\}\', response, re.DOTALL)\n            if json_match:\n                plan_data = json.loads(json_match.group())\n            else:\n                plan_data = json.loads(response)\n\n            # Create LLMPlan message\n            plan = LLMPlan()\n            plan.plan_id = f"plan_{self.get_clock().now().nanoseconds}"\n            plan.description = plan_data.get("description", "Generated plan")\n            plan.timestamp = self.get_clock().now().to_msg()\n            plan.confidence = 0.8  # Default confidence, could be derived from LLM response\n\n            # Create plan steps\n            for step_data in plan_data.get("steps", []):\n                step = PlanStep()\n                step.action_type = step_data.get("action_type", "unknown")\n                step.action_name = step_data.get("action_name", "unknown")\n                step.action_params = step_data.get("action_params", [])\n                step.expected_duration = float(step_data.get("expected_duration", 10.0))\n                step.preconditions = step_data.get("preconditions", [])\n                step.effects = step_data.get("effects", [])\n                step.success_probability = float(step_data.get("success_probability", 0.8))\n\n                # Parse target pose if provided\n                target_pose_data = step_data.get("target_pose", {})\n                if target_pose_data:\n                    step.target_pose.position.x = target_pose_data.get("x", 0.0)\n                    step.target_pose.position.y = target_pose_data.get("y", 0.0)\n                    step.target_pose.position.z = target_pose_data.get("z", 0.0)\n                    step.target_pose.orientation.x = target_pose_data.get("qx", 0.0)\n                    step.target_pose.orientation.y = target_pose_data.get("qy", 0.0)\n                    step.target_pose.orientation.z = target_pose_data.get("qz", 0.0)\n                    step.target_pose.orientation.w = target_pose_data.get("qw", 1.0)\n\n                plan.steps.append(step)\n\n            return plan\n\n        except json.JSONDecodeError as e:\n            self.get_logger().error(f"Failed to decode JSON response: {e}")\n            self.get_logger().debug(f"Response was: {response}")\n            return None\n        except Exception as e:\n            self.get_logger().error(f"Failed to parse plan response: {e}")\n            return None\n\n    def local_generate(self, prompt):\n        """Generate response using local model (placeholder implementation)"""\n        # This is a simplified placeholder - in practice you\'d use transformers or similar\n        return f\'{{"description": "Local plan for: {prompt[:50]}...", "steps": [{{"action_type": "navigation", "action_name": "move_to", "action_params": ["location"], "target_pose": {{"x": 1.0, "y": 1.0, "z": 0.0, "qx": 0.0, "qy": 0.0, "qz": 0.0, "qw": 1.0}}, "expected_duration": 30.0, "preconditions": ["robot_is_idle"], "effects": ["robot_at_location"], "success_probability": 0.85}}]}}\'\n\n    def mock_plan_response(self, command):\n        """Generate mock plan for demonstration"""\n        return f"""\n        {{\n            "description": "Mock plan for: {command}",\n            "steps": [\n                {{\n                    "action_type": "navigation",\n                    "action_name": "move_to",\n                    "action_params": ["kitchen"],\n                    "target_pose": {{"x": 2.0, "y": 1.5, "z": 0.0, "qx": 0.0, "qy": 0.0, "qz": 0.0, "qw": 1.0}},\n                    "expected_duration": 30.0,\n                    "preconditions": ["robot_is_idle"],\n                    "effects": ["robot_at_kitchen"],\n                    "success_probability": 0.9\n                }}\n            ]\n        }}\n        """\n\n    def validation_callback(self, msg):\n        """Handle plan validation feedback"""\n        feedback = msg.data\n        self.get_logger().info(f"Received plan validation feedback: {feedback}")\n        # Could be used to improve future planning or adjust confidence\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = LLMPlannerNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info("LLM Planner node shutting down...")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"llm-planning-node",children:"LLM Planning Node"}),"\n",(0,s.jsx)(n.p,{children:"The LLM planning node will:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Subscribe to text commands"}),"\n",(0,s.jsx)(n.li,{children:"Format prompts for the LLM"}),"\n",(0,s.jsx)(n.li,{children:"Process LLM responses into structured plans"}),"\n",(0,s.jsx)(n.li,{children:"Publish plans to execution systems"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"practical-implementation",children:"Practical Implementation"}),"\n",(0,s.jsx)(n.p,{children:"Let's implement the LLM planning node in our ROS 2 workspace."}),"\n",(0,s.jsx)(n.h3,{id:"basic-node-structure",children:"Basic Node Structure"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nimport openai\nimport json\nfrom std_msgs.msg import String\nfrom vla_msgs.msg import LLMPlan, PlanStep\nfrom geometry_msgs.msg import Pose\nimport re\n\nclass LLMPlannerNode(Node):\n    def __init__(self):\n        super().__init__(\'llm_planner_node\')\n\n        # Initialize LLM client (using OpenAI as example)\n        api_key = self.declare_parameter(\'openai_api_key\', \'\').value\n        if api_key:\n            openai.api_key = api_key\n        else:\n            self.get_logger().warn("No OpenAI API key provided - using mock responses")\n\n        # Parameters\n        self.model_name = self.declare_parameter(\'model_name\', \'gpt-3.5-turbo\').value\n        self.max_tokens = self.declare_parameter(\'max_tokens\', 1000).value\n\n        # Publishers and subscribers\n        self.command_sub = self.create_subscription(\n            String,\n            \'command_input\',\n            self.command_callback,\n            10\n        )\n\n        self.plan_pub = self.create_publisher(\n            LLMPlan,\n            \'robot_plan\',\n            10\n        )\n\n        self.get_logger().info("LLM Planner node initialized")\n\n    def command_callback(self, msg):\n        """Process incoming text commands and generate plans"""\n        command = msg.data\n        self.get_logger().info(f"Received command: {command}")\n\n        # Generate plan using LLM\n        plan = self.generate_plan(command)\n\n        if plan:\n            self.plan_pub.publish(plan)\n            self.get_logger().info(f"Published plan with {len(plan.steps)} steps")\n\n    def generate_plan(self, command):\n        """Generate robot plan using LLM"""\n        # Create prompt for the LLM\n        prompt = self.create_prompt(command)\n\n        try:\n            if openai.api_key:  # Use actual API if available\n                response = openai.ChatCompletion.create(\n                    model=self.model_name,\n                    messages=[\n                        {"role": "system", "content": self.get_system_prompt()},\n                        {"role": "user", "content": prompt}\n                    ],\n                    max_tokens=self.max_tokens,\n                    temperature=0.3\n                )\n\n                llm_response = response.choices[0].message[\'content\']\n            else:\n                # Mock response for demonstration\n                llm_response = self.mock_plan_response(command)\n\n            # Parse LLM response into structured plan\n            return self.parse_plan_response(llm_response)\n\n        except Exception as e:\n            self.get_logger().error(f"LLM query failed: {e}")\n            return None\n\n    def create_prompt(self, command):\n        """Create structured prompt for LLM"""\n        return f"""\n        Given the following command: "{command}"\n\n        Generate a step-by-step plan for a humanoid robot to execute this command.\n        The robot has these capabilities:\n        - Navigation to specific locations\n        - Object manipulation (grasping, lifting, placing)\n        - Door opening/closing\n        - Human interaction\n\n        Respond in JSON format with the following structure:\n        {{\n            "description": "Brief description of the plan",\n            "steps": [\n                {{\n                    "action_type": "navigation|manipulation|wait",\n                    "action_name": "move_to|pick_object|open_door|etc",\n                    "action_params": ["param1", "param2"],\n                    "target_pose": {{"x": 0.0, "y": 0.0, "z": 0.0, "qx": 0.0, "qy": 0.0, "qz": 0.0, "qw": 1.0}},\n                    "expected_duration": 10.0,\n                    "preconditions": ["condition1", "condition2"],\n                    "effects": ["effect1", "effect2"]\n                }}\n            ]\n        }}\n        """\n\n    def get_system_prompt(self):\n        """Get system prompt for LLM"""\n        return """\n        You are a helpful assistant that generates robot action plans.\n        Always respond in the exact JSON format requested.\n        Ensure all actions are feasible for a humanoid robot.\n        Include only actions that the robot is capable of performing.\n        """\n\n    def parse_plan_response(self, response):\n        """Parse LLM response into structured plan"""\n        try:\n            # Extract JSON from response if needed\n            json_match = re.search(r\'\\{.*\\}\', response, re.DOTALL)\n            if json_match:\n                plan_data = json.loads(json_match.group())\n            else:\n                plan_data = json.loads(response)\n\n            # Create LLMPlan message\n            plan = LLMPlan()\n            plan.plan_id = f"plan_{self.get_clock().now().nanoseconds}"\n            plan.description = plan_data.get("description", "Generated plan")\n            plan.timestamp = self.get_clock().now().to_msg()\n            plan.confidence = 0.8  # Default confidence\n\n            # Create plan steps\n            for step_data in plan_data.get("steps", []):\n                step = PlanStep()\n                step.action_type = step_data.get("action_type", "unknown")\n                step.action_name = step_data.get("action_name", "unknown")\n                step.action_params = step_data.get("action_params", [])\n                step.expected_duration = float(step_data.get("expected_duration", 10.0))\n                step.preconditions = step_data.get("preconditions", [])\n                step.effects = step_data.get("effects", [])\n\n                # Parse target pose if provided\n                target_pose_data = step_data.get("target_pose", {})\n                if target_pose_data:\n                    step.target_pose.position.x = target_pose_data.get("x", 0.0)\n                    step.target_pose.position.y = target_pose_data.get("y", 0.0)\n                    step.target_pose.position.z = target_pose_data.get("z", 0.0)\n                    step.target_pose.orientation.x = target_pose_data.get("qx", 0.0)\n                    step.target_pose.orientation.y = target_pose_data.get("qy", 0.0)\n                    step.target_pose.orientation.z = target_pose_data.get("qz", 0.0)\n                    step.target_pose.orientation.w = target_pose_data.get("qw", 1.0)\n\n                plan.steps.append(step)\n\n            return plan\n\n        except Exception as e:\n            self.get_logger().error(f"Failed to parse plan response: {e}")\n            return None\n\n    def mock_plan_response(self, command):\n        """Generate mock plan for demonstration"""\n        return f"""\n        {{\n            "description": "Mock plan for: {command}",\n            "steps": [\n                {{\n                    "action_type": "navigation",\n                    "action_name": "move_to",\n                    "action_params": ["kitchen"],\n                    "target_pose": {{"x": 2.0, "y": 1.5, "z": 0.0, "qx": 0.0, "qy": 0.0, "qz": 0.0, "qw": 1.0}},\n                    "expected_duration": 30.0,\n                    "preconditions": ["robot_is_idle"],\n                    "effects": ["robot_at_kitchen"]\n                }}\n            ]\n        }}\n        """\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = LLMPlannerNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"plan-execution-integration",children:"Plan Execution Integration"}),"\n",(0,s.jsx)(n.p,{children:"The generated plans will be executed by:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Navigation systems for movement tasks"}),"\n",(0,s.jsx)(n.li,{children:"Manipulation systems for object interaction"}),"\n",(0,s.jsx)(n.li,{children:"State machines for complex behaviors"}),"\n",(0,s.jsx)(n.li,{children:"Safety systems for constraint enforcement"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"challenges-and-solutions",children:"Challenges and Solutions"}),"\n",(0,s.jsx)(n.h3,{id:"response-consistency",children:"Response Consistency"}),"\n",(0,s.jsx)(n.p,{children:"LLMs can generate inconsistent responses. Solutions include:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Structured prompting"}),": Use detailed examples and clear formatting requirements"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Response validation"}),": Validate JSON structure and content before execution"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Confidence scoring"}),": Assess plan quality and reliability"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fallback strategies"}),": Use default behaviors for invalid plans"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fine-tuning"}),": Adapt models to robotics-specific tasks and terminology"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"real-time-constraints",children:"Real-time Constraints"}),"\n",(0,s.jsx)(n.p,{children:"LLMs can be slow to respond. Consider:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Caching"}),": Store common plans for frequent commands"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Asynchronous planning"}),": Generate plans in background"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Hierarchical planning"}),": Use lightweight models for simple tasks, complex models for complex tasks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pre-planning"}),": Generate plans for common scenarios in advance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Local execution"}),": Run smaller models on robot hardware for faster response"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"safety-and-validation",children:"Safety and Validation"}),"\n",(0,s.jsx)(n.p,{children:"Robot safety is paramount. Implement:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Plan validation"}),": Check feasibility against robot capabilities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety constraint checking"}),": Ensure plans comply with safety protocols"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Human-in-the-loop"}),": Allow human override for critical decisions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Emergency stop capabilities"}),": Immediate stop functionality"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simulation validation"}),": Test plans in simulation before real execution"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"knowledge-integration",children:"Knowledge Integration"}),"\n",(0,s.jsx)(n.p,{children:"LLMs may lack current context about the environment:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"World state integration"}),": Provide current environment state in prompts"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory systems"}),": Implement episodic memory for ongoing tasks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-modal grounding"}),": Combine LLM reasoning with real-time sensor data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Verification loops"}),": Confirm assumptions through perception"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"integration-with-vla-system",children:"Integration with VLA System"}),"\n",(0,s.jsx)(n.h3,{id:"voice-to-plan-pipeline",children:"Voice-to-Plan Pipeline"}),"\n",(0,s.jsx)(n.p,{children:"The complete pipeline connects:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Voice input \u2192 Whisper transcription"}),"\n",(0,s.jsx)(n.li,{children:"Text command \u2192 LLM planning"}),"\n",(0,s.jsx)(n.li,{children:"Plan \u2192 ROS 2 execution"}),"\n",(0,s.jsx)(n.li,{children:"Execution feedback \u2192 plan refinement"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"multi-modal-coordination",children:"Multi-modal Coordination"}),"\n",(0,s.jsx)(n.p,{children:"LLM plans will coordinate with:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Vision systems for object recognition and scene understanding"}),"\n",(0,s.jsx)(n.li,{children:"Navigation systems for path planning and obstacle avoidance"}),"\n",(0,s.jsx)(n.li,{children:"Manipulation systems for grasping and interaction"}),"\n",(0,s.jsx)(n.li,{children:"State monitoring for execution feedback and replanning"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"plan-execution-and-monitoring",children:"Plan Execution and Monitoring"}),"\n",(0,s.jsx)(n.p,{children:"The execution system will:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Monitor plan progress in real-time"}),"\n",(0,s.jsx)(n.li,{children:"Detect execution failures and deviations"}),"\n",(0,s.jsx)(n.li,{children:"Request plan revisions when needed"}),"\n",(0,s.jsx)(n.li,{children:"Provide feedback to improve future planning"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"deployment-considerations",children:"Deployment Considerations"}),"\n",(0,s.jsx)(n.h3,{id:"edge-vs-cloud-deployment",children:"Edge vs. Cloud Deployment"}),"\n",(0,s.jsx)(n.p,{children:"Consider the trade-offs between edge and cloud deployment:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Edge"}),": Lower latency, offline capability, privacy"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cloud"}),": More powerful models, easier updates, higher costs"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"resource-management",children:"Resource Management"}),"\n",(0,s.jsx)(n.p,{children:"LLM inference requires significant resources:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory"}),": Large models require substantial RAM"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Compute"}),": GPU acceleration for faster inference"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Power"}),": Consider power consumption on mobile robots"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Thermal"}),": Manage heat generation during sustained operation"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"model-optimization",children:"Model Optimization"}),"\n",(0,s.jsx)(n.p,{children:"For robotics deployment:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Quantization"}),": Reduce model size and improve speed"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pruning"}),": Remove unnecessary parameters"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Distillation"}),": Create smaller, faster student models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Caching"}),": Store frequently used responses"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,s.jsx)(n.h3,{id:"unit-testing",children:"Unit Testing"}),"\n",(0,s.jsx)(n.p,{children:"Test individual components:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Prompt generation and formatting"}),"\n",(0,s.jsx)(n.li,{children:"Response parsing and validation"}),"\n",(0,s.jsx)(n.li,{children:"Message publishing and subscription"}),"\n",(0,s.jsx)(n.li,{children:"Error handling and fallbacks"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"integration-testing",children:"Integration Testing"}),"\n",(0,s.jsx)(n.p,{children:"Test complete pipeline:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"End-to-end voice-to-action flow"}),"\n",(0,s.jsx)(n.li,{children:"Plan feasibility validation"}),"\n",(0,s.jsx)(n.li,{children:"Execution monitoring and feedback"}),"\n",(0,s.jsx)(n.li,{children:"Safety system integration"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"performance-testing",children:"Performance Testing"}),"\n",(0,s.jsx)(n.p,{children:"Evaluate system performance:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Planning latency under various conditions"}),"\n",(0,s.jsx)(n.li,{children:"Success rates for different command types"}),"\n",(0,s.jsx)(n.li,{children:"Resource utilization during operation"}),"\n",(0,s.jsx)(n.li,{children:"Robustness to environmental variations"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(n.p,{children:"In the next chapter, we'll integrate voice input and LLM planning with vision systems to create a complete multi-modal perception and decision-making system. This will form the core of our Vision-Language-Action (VLA) framework by connecting all three modalities into a unified cognitive system."}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"In this chapter, we've learned how to implement LLM-based cognitive planning for humanoid robots. We covered the complete pipeline from text commands to executable robot plans, including prompt engineering, response parsing, and integration with ROS 2 systems. We explored technical implementation details, challenges, and solutions for deploying LLMs in robotics applications. This cognitive planning capability is essential for creating robots that can understand and execute complex human instructions in natural language, forming a critical component of our Vision-Language-Action (VLA) system."})]})}function d(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var i=t(6540);const s={},o=i.createContext(s);function a(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);