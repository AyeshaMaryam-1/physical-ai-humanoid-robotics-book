"use strict";(globalThis.webpackChunkbook_temp=globalThis.webpackChunkbook_temp||[]).push([[705],{4655:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module_4/4_4_capstone","title":"Chapter 4.4: Capstone Project: The Autonomous Humanoid","description":"Building a fully autonomous humanoid robot system with integrated VLA capabilities","source":"@site/docs/04_module_4/4_4_capstone.mdx","sourceDirName":"04_module_4","slug":"/module_4/4_4_capstone","permalink":"/physical-ai-humanoid-robotics-book/docs/module_4/4_4_capstone","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/04_module_4/4_4_capstone.mdx","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Chapter 4.4: Capstone Project: The Autonomous Humanoid","description":"Building a fully autonomous humanoid robot system with integrated VLA capabilities"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4.3: Multi-Modal Perception & Decision Making","permalink":"/physical-ai-humanoid-robotics-book/docs/module_4/4_3_multi_modal"}}');var i=t(4848),a=t(8453);const o={sidebar_position:4,title:"Chapter 4.4: Capstone Project: The Autonomous Humanoid",description:"Building a fully autonomous humanoid robot system with integrated VLA capabilities"},r="Chapter 4.4: Capstone Project: The Autonomous Humanoid",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"System Architecture Overview",id:"system-architecture-overview",level:2},{value:"Integrated VLA System",id:"integrated-vla-system",level:3},{value:"Key Integration Points",id:"key-integration-points",level:3},{value:"Technical Implementation",id:"technical-implementation",level:2},{value:"System Orchestrator Node",id:"system-orchestrator-node",level:3},{value:"Complex Multi-Stage Task Design",id:"complex-multi-stage-task-design",level:2},{value:"Task: &quot;Go to kitchen, find cup, bring to me&quot;",id:"task-go-to-kitchen-find-cup-bring-to-me",level:3},{value:"Stage 1: Navigation to Kitchen",id:"stage-1-navigation-to-kitchen",level:4},{value:"Stage 2: Object Search and Recognition",id:"stage-2-object-search-and-recognition",level:4},{value:"Stage 3: Object Manipulation",id:"stage-3-object-manipulation",level:4},{value:"Stage 4: Return Navigation",id:"stage-4-return-navigation",level:4},{value:"Stage 5: Task Completion",id:"stage-5-task-completion",level:4},{value:"Implementation of Multi-Stage Task",id:"implementation-of-multi-stage-task",level:3},{value:"System Integration and Testing",id:"system-integration-and-testing",level:2},{value:"Integration Testing Approach",id:"integration-testing-approach",level:3},{value:"Performance Metrics",id:"performance-metrics",level:3},{value:"Safety and Reliability Considerations",id:"safety-and-reliability-considerations",level:2},{value:"Safety Architecture",id:"safety-architecture",level:3},{value:"Reliability Features",id:"reliability-features",level:3},{value:"Evaluation and Demonstration",id:"evaluation-and-demonstration",level:2},{value:"Demonstration Protocol",id:"demonstration-protocol",level:3},{value:"Success Criteria",id:"success-criteria",level:3},{value:"Next Steps and Future Work",id:"next-steps-and-future-work",level:2},{value:"Advanced Capabilities",id:"advanced-capabilities",level:3},{value:"Research Directions",id:"research-directions",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-44-capstone-project-the-autonomous-humanoid",children:"Chapter 4.4: Capstone Project: The Autonomous Humanoid"})}),"\n",(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(n.p,{children:"In this final chapter of Module 4, we'll bring together all the components we've developed throughout the Vision-Language-Action (VLA) module to create a fully autonomous humanoid robot system. This capstone project integrates voice input, LLM planning, and multi-modal perception into a cohesive system capable of complex autonomous behaviors."}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this chapter, you will:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Understand how to orchestrate all VLA components into a unified system"}),"\n",(0,i.jsx)(n.li,{children:"Learn to design complex multi-stage tasks for humanoid robots"}),"\n",(0,i.jsx)(n.li,{children:"Implement system-level integration and coordination"}),"\n",(0,i.jsx)(n.li,{children:"Create a demonstration of autonomous humanoid capabilities"}),"\n",(0,i.jsx)(n.li,{children:"Evaluate the performance of the complete VLA system"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"system-architecture-overview",children:"System Architecture Overview"}),"\n",(0,i.jsx)(n.h3,{id:"integrated-vla-system",children:"Integrated VLA System"}),"\n",(0,i.jsx)(n.p,{children:"Our complete system architecture includes:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Voice Input   \u2502\u2500\u2500\u2500\u25b6\u2502 Multi-Modal      \u2502\u2500\u2500\u2500\u25b6\u2502   LLM Planner   \u2502\n\u2502   (Whisper)     \u2502    \u2502 Fusion Node      \u2502    \u2502   (OpenAI/LLM)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502                        \u2502\n         \u25bc                       \u25bc                        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Audio Stream    \u2502    \u2502 Task Context     \u2502    \u2502 Robot Plan      \u2502\n\u2502 Processing      \u2502    \u2502 Creation         \u2502    \u2502 Generation      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                      \u2502\n                                                      \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Navigation      \u2502\u25c0\u2500\u2500\u2500\u2502 Plan Execution   \u2502\u25c0\u2500\u2500\u2500\u2502 Plan Refinement \u2502\n\u2502 System          \u2502    \u2502 Framework        \u2502    \u2502 & Adaptation    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502                        \u2502\n         \u25bc                       \u25bc                        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Robot Actions   \u2502    \u2502 System Monitor   \u2502    \u2502 Human Feedback  \u2502\n\u2502 (Move, Grasp,   \u2502    \u2502 & Safety         \u2502    \u2502 Integration     \u2502\n\u2502 Speak)          \u2502    \u2502 Supervisor       \u2502    \u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.h3,{id:"key-integration-points",children:"Key Integration Points"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Real-time Coordination"}),": Synchronize all components for responsive behavior"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Adaptive Planning"}),": Modify plans based on real-time feedback"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Safety Integration"}),": Ensure safe operation throughout execution"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Human-in-the-Loop"}),": Incorporate human feedback and corrections"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"technical-implementation",children:"Technical Implementation"}),"\n",(0,i.jsx)(n.h3,{id:"system-orchestrator-node",children:"System Orchestrator Node"}),"\n",(0,i.jsx)(n.p,{children:"We'll create an orchestrator node that manages the overall system behavior. First, let's create the necessary ROS 2 package:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"mkdir -p robotics_ws/src/vla_system_manager/vla_system_manager\n"})}),"\n",(0,i.jsx)(n.p,{children:"The package.xml file:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<?xml-model href="http://download.ros.org/schema/package_format3.xsd" schematypens="http://www.w3.org/2001/XMLSchema"?>\n<package format="3">\n  <name>vla_system_manager</name>\n  <version>0.0.1</version>\n  <description>Vision-Language-Action system manager and orchestrator</description>\n  <maintainer email="maintainer@todo.todo">Maintainer</maintainer>\n  <license>TODO</license>\n\n  <depend>rclpy</depend>\n  <depend>std_msgs</depend>\n  <depend>sensor_msgs</depend>\n  <depend>vision_msgs</depend>\n  <depend>geometry_msgs</depend>\n  <depend>vla_msgs</depend>\n\n  <test_depend>ament_copyright</test_depend>\n  <test_depend>ament_flake8</test_depend>\n  <test_depend>ament_pep257</test_depend>\n  <test_depend>python3-pytest</test_depend>\n\n  <export>\n    <build_type>ament_python</build_type>\n  </export>\n</package>\n'})}),"\n",(0,i.jsx)(n.p,{children:"Now the orchestrator node implementation:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray\nfrom vla_msgs.msg import LLMPlan\nfrom geometry_msgs.msg import Pose\nimport json\nimport time\nfrom threading import Lock\nimport traceback\n\n\nclass VLAOrchestratorNode(Node):\n    def __init__(self):\n        super().__init__('vla_orchestrator_node')\n\n        # System state management\n        self.current_state = 'idle'  # idle, listening, planning, executing, error\n        self.current_plan = None\n        self.system_lock = Lock()\n\n        # Parameters\n        self.max_execution_time = self.declare_parameter('max_execution_time', 300.0).value  # 5 minutes\n        self.enable_human_feedback = self.declare_parameter('enable_human_feedback', True).value\n        self.safety_check_interval = self.declare_parameter('safety_check_interval', 1.0).value\n        self.step_timeout = self.declare_parameter('step_timeout', 60.0).value  # 1 minute per step\n\n        # Publishers and subscribers\n        self.voice_command_pub = self.create_publisher(\n            String,\n            'transcribed_text',\n            10\n        )\n\n        self.command_input_pub = self.create_publisher(\n            String,\n            'command_input',\n            10\n        )\n\n        self.refined_command_pub = self.create_publisher(\n            String,\n            'refined_command',\n            10\n        )\n\n        self.plan_sub = self.create_subscription(\n            LLMPlan,\n            'robot_plan',\n            self.plan_callback,\n            10\n        )\n\n        self.execution_status_sub = self.create_subscription(\n            String,\n            'execution_status',\n            self.execution_status_callback,\n            10\n        )\n\n        self.human_feedback_sub = self.create_subscription(\n            String,\n            'human_feedback',\n            self.human_feedback_callback,\n            10\n        )\n\n        # System control publishers\n        self.emergency_stop_pub = self.create_publisher(\n            Bool,\n            'emergency_stop',\n            10\n        )\n\n        # Status publisher\n        self.system_status_pub = self.create_publisher(\n            String,\n            'system_status',\n            10\n        )\n\n        # Timer for system monitoring\n        self.system_monitor_timer = self.create_timer(0.1, self.system_monitor_callback)\n        self.safety_timer = self.create_timer(self.safety_check_interval, self.safety_check_callback)\n\n        # Execution tracking\n        self.plan_start_time = None\n        self.current_step_index = 0\n        self.step_start_time = None\n\n        # Task history\n        self.task_history = []\n\n        self.get_logger().info(\"VLA Orchestrator node initialized\")\n\n    def plan_callback(self, msg):\n        \"\"\"Handle received plans from the LLM planner\"\"\"\n        with self.system_lock:\n            self.current_plan = msg\n            self.current_state = 'executing'\n            self.plan_start_time = self.get_clock().now().to_msg()\n            self.current_step_index = 0\n            self.step_start_time = self.get_clock().now().to_msg()\n\n            self.get_logger().info(f\"Received new plan: {msg.description}\")\n            self.get_logger().info(f\"Plan has {len(msg.steps)} steps\")\n\n            # Publish system status\n            status_msg = String()\n            status_msg.data = json.dumps({\n                'state': self.current_state,\n                'plan_description': msg.description,\n                'total_steps': len(msg.steps),\n                'timestamp': self.get_clock().now().to_msg().sec\n            })\n            self.system_status_pub.publish(status_msg)\n\n            # Execute first step\n            self.execute_next_step()\n\n    def execution_status_callback(self, msg):\n        \"\"\"Handle execution status updates\"\"\"\n        try:\n            status_data = json.loads(msg.data) if msg.data.startswith('{') else {'status': msg.data}\n        except json.JSONDecodeError:\n            self.get_logger().error(f\"Failed to parse execution status: {msg.data}\")\n            return\n\n        with self.system_lock:\n            if self.current_plan and self.current_step_index < len(self.current_plan.steps):\n                current_step = self.current_plan.steps[self.current_step_index]\n\n                if status_data.get('status') == 'completed':\n                    self.get_logger().info(f\"Completed step: {current_step.action_name}\")\n\n                    # Record step completion in history\n                    self.task_history.append({\n                        'step': self.current_step_index,\n                        'action': current_step.action_name,\n                        'status': 'completed',\n                        'timestamp': self.get_clock().now().to_msg().sec\n                    })\n\n                    self.current_step_index += 1\n                    self.step_start_time = self.get_clock().now().to_msg()\n\n                    if self.current_step_index >= len(self.current_plan.steps):\n                        # All steps completed\n                        self.get_logger().info(\"Plan execution completed successfully!\")\n                        self.task_history.append({\n                            'task': 'complete',\n                            'status': 'success',\n                            'timestamp': self.get_clock().now().to_msg().sec\n                        })\n                        self.current_state = 'idle'\n                        self.current_plan = None\n\n                        # Publish completion status\n                        status_msg = String()\n                        status_msg.data = json.dumps({\n                            'state': 'completed',\n                            'result': 'success',\n                            'timestamp': self.get_clock().now().to_msg().sec\n                        })\n                        self.system_status_pub.publish(status_msg)\n                    else:\n                        # Move to next step\n                        self.execute_next_step()\n                elif status_data.get('status') == 'failed':\n                    self.get_logger().error(f\"Step failed: {current_step.action_name}\")\n                    self.handle_plan_failure(status_data.get('error', 'Unknown error'))\n                elif status_data.get('status') == 'in_progress':\n                    # Update status but continue execution\n                    pass\n\n    def human_feedback_callback(self, msg):\n        \"\"\"Handle human feedback and corrections\"\"\"\n        if not self.enable_human_feedback:\n            return\n\n        feedback = msg.data.lower()\n        self.get_logger().info(f\"Received human feedback: {feedback}\")\n\n        with self.system_lock:\n            if 'stop' in feedback or 'cancel' in feedback or 'abort' in feedback:\n                self.emergency_stop()\n            elif 'repeat' in feedback or 'again' in feedback:\n                self.repeat_current_step()\n            elif 'different' in feedback or 'change' in feedback or 'modify' in feedback:\n                self.request_plan_revision()\n            elif 'status' in feedback or 'progress' in feedback:\n                self.report_status()\n\n    def execute_next_step(self):\n        \"\"\"Execute the next step in the current plan\"\"\"\n        if not self.current_plan or self.current_step_index >= len(self.current_plan.steps):\n            return\n\n        step = self.current_plan.steps[self.current_step_index]\n        self.get_logger().info(f\"Executing step {self.current_step_index + 1}: {step.action_name}\")\n\n        # Publish command to execute the step\n        step_cmd = String()\n        step_cmd.data = json.dumps({\n            'action_type': step.action_type,\n            'action_name': step.action_name,\n            'action_params': step.action_params,\n            'target_pose': {\n                'x': step.target_pose.position.x,\n                'y': step.target_pose.position.y,\n                'z': step.target_pose.position.z,\n                'qx': step.target_pose.orientation.x,\n                'qy': step.target_pose.orientation.y,\n                'qz': step.target_pose.orientation.z,\n                'qw': step.target_pose.orientation.w\n            },\n            'expected_duration': step.expected_duration,\n            'step_index': self.current_step_index\n        })\n\n        # Publish to appropriate execution system based on action type\n        if step.action_type == 'navigation':\n            self.publish_to_navigation_system(step_cmd)\n        elif step.action_type == 'manipulation':\n            self.publish_to_manipulation_system(step_cmd)\n        elif step.action_type == 'perception':\n            self.publish_to_perception_system(step_cmd)\n        elif step.action_type == 'communication':\n            self.publish_to_communication_system(step_cmd)\n        else:\n            self.publish_to_generic_system(step_cmd)\n\n    def publish_to_navigation_system(self, cmd):\n        \"\"\"Publish navigation commands\"\"\"\n        # Publish to navigation system\n        nav_pub = self.create_publisher(String, 'navigation_command', 10)\n        nav_pub.publish(cmd)\n\n    def publish_to_manipulation_system(self, cmd):\n        \"\"\"Publish manipulation commands\"\"\"\n        # Publish to manipulation system\n        manip_pub = self.create_publisher(String, 'manipulation_command', 10)\n        manip_pub.publish(cmd)\n\n    def publish_to_perception_system(self, cmd):\n        \"\"\"Publish perception commands\"\"\"\n        # Publish to perception system\n        percep_pub = self.create_publisher(String, 'perception_command', 10)\n        percep_pub.publish(cmd)\n\n    def publish_to_communication_system(self, cmd):\n        \"\"\"Publish communication commands\"\"\"\n        # Publish to communication system\n        comm_pub = self.create_publisher(String, 'communication_command', 10)\n        comm_pub.publish(cmd)\n\n    def publish_to_generic_system(self, cmd):\n        \"\"\"Publish to generic action system\"\"\"\n        # Publish to generic action system\n        action_pub = self.create_publisher(String, 'action_command', 10)\n        action_pub.publish(cmd)\n\n    def handle_plan_failure(self, error_msg=\"Unknown error\"):\n        \"\"\"Handle plan execution failure\"\"\"\n        self.get_logger().error(f\"Plan execution failed: {error_msg}\")\n\n        # Record failure in history\n        if self.current_plan and self.current_step_index < len(self.current_plan.steps):\n            current_step = self.current_plan.steps[self.current_step_index]\n            self.task_history.append({\n                'step': self.current_step_index,\n                'action': current_step.action_name,\n                'status': 'failed',\n                'error': error_msg,\n                'timestamp': self.get_clock().now().to_msg().sec\n            })\n\n        # Request plan revision from LLM with failure context\n        if self.current_plan:\n            failure_context = f\"Plan failed at step {self.current_step_index} with error: {error_msg}. Plan: {self.current_plan.description}\"\n            failure_cmd = String()\n            failure_cmd.data = f\"Revise plan based on failure: {failure_context}\"\n            self.command_input_pub.publish(failure_cmd)\n\n    def request_plan_revision(self):\n        \"\"\"Request plan revision based on human feedback\"\"\"\n        self.get_logger().info(\"Human requested plan revision\")\n\n        if self.current_plan:\n            revision_cmd = String()\n            revision_cmd.data = f\"Revise current plan based on human feedback: {self.current_plan.description}\"\n            self.command_input_pub.publish(revision_cmd)\n\n    def repeat_current_step(self):\n        \"\"\"Repeat the current step\"\"\"\n        if self.current_plan and self.current_step_index > 0:\n            # Go back to previous step to repeat\n            self.current_step_index -= 1\n            self.execute_next_step()\n\n    def report_status(self):\n        \"\"\"Report current system status\"\"\"\n        status = {\n            'state': self.current_state,\n            'current_plan': self.current_plan.description if self.current_plan else 'None',\n            'current_step': self.current_step_index,\n            'total_steps': len(self.current_plan.steps) if self.current_plan else 0,\n            'timestamp': self.get_clock().now().to_msg().sec\n        }\n\n        status_msg = String()\n        status_msg.data = json.dumps(status)\n        self.system_status_pub.publish(status_msg)\n\n    def emergency_stop(self):\n        \"\"\"Emergency stop all systems\"\"\"\n        self.get_logger().warn(\"Emergency stop activated!\")\n        self.current_state = 'error'\n\n        # Send emergency stop to all systems\n        stop_msg = Bool()\n        stop_msg.data = True\n        self.emergency_stop_pub.publish(stop_msg)\n\n        # Record emergency stop in history\n        self.task_history.append({\n            'event': 'emergency_stop',\n            'timestamp': self.get_clock().now().to_msg().sec\n        })\n\n        # Clear current plan\n        self.current_plan = None\n\n        # Publish error status\n        status_msg = String()\n        status_msg.data = json.dumps({\n            'state': 'error',\n            'result': 'emergency_stop',\n            'timestamp': self.get_clock().now().to_msg().sec\n        })\n        self.system_status_pub.publish(status_msg)\n\n    def system_monitor_callback(self):\n        \"\"\"Monitor system state and progress\"\"\"\n        with self.system_lock:\n            if self.current_state == 'executing' and self.plan_start_time:\n                current_time = self.get_clock().now().to_msg()\n                elapsed_time = (current_time.sec - self.plan_start_time.sec) + \\\n                              (current_time.nanosec - self.plan_start_time.nanosec) / 1e9\n\n                if elapsed_time > self.max_execution_time:\n                    self.get_logger().warn(\"Plan execution timeout!\")\n                    self.emergency_stop()\n\n            # Check for step timeout\n            if self.current_state == 'executing' and self.step_start_time and self.current_plan:\n                current_time = self.get_clock().now().to_msg()\n                step_elapsed = (current_time.sec - self.step_start_time.sec) + \\\n                              (current_time.nanosec - self.step_start_time.nanosec) / 1e9\n\n                if step_elapsed > self.step_timeout:\n                    self.get_logger().warn(f\"Step {self.current_step_index} timeout!\")\n                    self.handle_plan_failure(f\"Step timeout after {self.step_timeout} seconds\")\n\n    def safety_check_callback(self):\n        \"\"\"Perform safety checks\"\"\"\n        # This would integrate with safety systems to check:\n        # - Collision avoidance\n        # - Robot health\n        # - Environment safety\n        # - Human safety\n        # For now, just log that safety check is running\n        pass\n\n    def process_voice_command(self, command):\n        \"\"\"Process a voice command through the VLA system\"\"\"\n        with self.system_lock:\n            if self.current_state in ['planning', 'executing']:\n                self.get_logger().warn(\"System busy, cannot accept new command\")\n                return False\n\n            self.current_state = 'listening'\n\n            # Publish to voice input system\n            voice_msg = String()\n            voice_msg.data = command\n            self.voice_command_pub.publish(voice_msg)\n\n            return True\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VLAOrchestratorNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info(\"VLA Orchestrator node shutting down...\")\n        node.get_logger().info(f\"Task history: {len(node.task_history)} events recorded\")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"complex-multi-stage-task-design",children:"Complex Multi-Stage Task Design"}),"\n",(0,i.jsx)(n.h3,{id:"task-go-to-kitchen-find-cup-bring-to-me",children:'Task: "Go to kitchen, find cup, bring to me"'}),"\n",(0,i.jsx)(n.p,{children:"Let's design a complex multi-stage task that demonstrates the full capabilities of our autonomous humanoid:"}),"\n",(0,i.jsx)(n.h4,{id:"stage-1-navigation-to-kitchen",children:"Stage 1: Navigation to Kitchen"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use voice command to identify destination"}),"\n",(0,i.jsx)(n.li,{children:"Plan navigation route using Nav2"}),"\n",(0,i.jsx)(n.li,{children:"Execute navigation while avoiding obstacles"}),"\n",(0,i.jsx)(n.li,{children:"Confirm arrival at destination"}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"stage-2-object-search-and-recognition",children:"Stage 2: Object Search and Recognition"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use vision system to scan environment"}),"\n",(0,i.jsx)(n.li,{children:"Detect and identify cup-like objects"}),"\n",(0,i.jsx)(n.li,{children:"Verify object properties and accessibility"}),"\n",(0,i.jsx)(n.li,{children:"Select appropriate cup to retrieve"}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"stage-3-object-manipulation",children:"Stage 3: Object Manipulation"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Plan grasping approach based on object pose"}),"\n",(0,i.jsx)(n.li,{children:"Execute manipulation to pick up cup"}),"\n",(0,i.jsx)(n.li,{children:"Verify successful grasp"}),"\n",(0,i.jsx)(n.li,{children:"Maintain stable grip during transport"}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"stage-4-return-navigation",children:"Stage 4: Return Navigation"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Plan return route to user"}),"\n",(0,i.jsx)(n.li,{children:"Navigate while maintaining object stability"}),"\n",(0,i.jsx)(n.li,{children:"Approach user at appropriate distance"}),"\n",(0,i.jsx)(n.li,{children:"Present object to user"}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"stage-5-task-completion",children:"Stage 5: Task Completion"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Release object to user"}),"\n",(0,i.jsx)(n.li,{children:"Confirm task completion"}),"\n",(0,i.jsx)(n.li,{children:"Return to idle state"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"implementation-of-multi-stage-task",children:"Implementation of Multi-Stage Task"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class MultiStageTaskManager:\n    def __init__(self, orchestrator_node):\n        self.orchestrator = orchestrator_node\n        self.current_stage = 0\n        self.task_stages = [\n            self.navigation_stage,\n            self.search_stage,\n            self.manipulation_stage,\n            self.return_stage,\n            self.completion_stage\n        ]\n\n    def navigation_stage(self):\n        """Navigate to kitchen"""\n        self.orchestrator.get_logger().info("Starting navigation to kitchen...")\n        # Command: go to kitchen\n        command = String()\n        command.data = "Navigate to the kitchen"\n        self.orchestrator.command_input_pub.publish(command)\n\n    def search_stage(self):\n        """Search for cup in kitchen"""\n        self.orchestrator.get_logger().info("Searching for cup in kitchen...")\n        # Command: find cup\n        command = String()\n        command.data = "Find a cup in the current location"\n        self.orchestrator.command_input_pub.publish(command)\n\n    def manipulation_stage(self):\n        """Pick up the cup"""\n        self.orchestrator.get_logger().info("Picking up the cup...")\n        # Command: grasp cup\n        command = String()\n        command.data = "Pick up the cup you found"\n        self.orchestrator.command_input_pub.publish(command)\n\n    def return_stage(self):\n        """Return to user"""\n        self.orchestrator.get_logger().info("Returning to user...")\n        # Command: go back to user\n        command = String()\n        command.data = "Return to the person who gave you the command"\n        self.orchestrator.command_input_pub.publish(command)\n\n    def completion_stage(self):\n        """Complete task"""\n        self.orchestrator.get_logger().info("Completing task...")\n        # Command: give cup to user\n        command = String()\n        command.data = "Give the cup to the person in front of you"\n        self.orchestrator.command_input_pub.publish(command)\n\n    def execute_task(self):\n        """Execute the complete multi-stage task"""\n        if self.current_stage < len(self.task_stages):\n            self.task_stages[self.current_stage]()\n            self.current_stage += 1\n'})}),"\n",(0,i.jsx)(n.h2,{id:"system-integration-and-testing",children:"System Integration and Testing"}),"\n",(0,i.jsx)(n.h3,{id:"integration-testing-approach",children:"Integration Testing Approach"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Component Testing"}),": Test each VLA component individually"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Interface Testing"}),": Test communication between components"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"System Testing"}),": Test complete end-to-end functionality"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Scenario Testing"}),": Test various real-world scenarios"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Task Completion Rate"}),": Percentage of tasks completed successfully"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Response Time"}),": Time from command to action initiation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Plan Quality"}),": Effectiveness and efficiency of generated plans"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"System Reliability"}),": Consistency of system behavior"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"User Satisfaction"}),": Subjective measure of system usability"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"safety-and-reliability-considerations",children:"Safety and Reliability Considerations"}),"\n",(0,i.jsx)(n.h3,{id:"safety-architecture",children:"Safety Architecture"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fail-Safe Mechanisms"}),": Automatic emergency stops"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Collision Avoidance"}),": Real-time obstacle detection"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Human Safety"}),": Maintain safe distances from humans"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"System Health"}),": Monitor robot operational status"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"reliability-features",children:"Reliability Features"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Error Recovery"}),": Automatic recovery from common failures"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Graceful Degradation"}),": Continue operation with reduced capabilities"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Redundancy"}),": Backup systems for critical functions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Monitoring"}),": Continuous system state monitoring"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"evaluation-and-demonstration",children:"Evaluation and Demonstration"}),"\n",(0,i.jsx)(n.h3,{id:"demonstration-protocol",children:"Demonstration Protocol"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Setup Phase"}),": Configure environment and robot"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Calibration"}),": Calibrate sensors and systems"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Task Execution"}),": Execute multi-stage task"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Performance Recording"}),": Log all metrics"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Analysis"}),": Evaluate results and identify improvements"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"success-criteria",children:"Success Criteria"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Complete task with >80% success rate"}),"\n",(0,i.jsx)(n.li,{children:"Respond to voice commands within 5 seconds"}),"\n",(0,i.jsx)(n.li,{children:"Navigate safely without collisions"}),"\n",(0,i.jsx)(n.li,{children:"Manipulate objects successfully"}),"\n",(0,i.jsx)(n.li,{children:"Handle unexpected situations gracefully"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"next-steps-and-future-work",children:"Next Steps and Future Work"}),"\n",(0,i.jsx)(n.h3,{id:"advanced-capabilities",children:"Advanced Capabilities"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Learning from Demonstration"}),": Learn new tasks from human examples"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Long-term Memory"}),": Remember preferences and learned information"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Social Interaction"}),": More natural human-robot interaction"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Collaborative Tasks"}),": Work with multiple robots or humans"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"research-directions",children:"Research Directions"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Improved Multi-modal Fusion"}),": Better integration of sensory modalities"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Adaptive Learning"}),": Continuous learning from experience"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Human-Robot Collaboration"}),": More sophisticated team behaviors"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Ethical AI"}),": Ensuring responsible AI deployment"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"In this capstone chapter, we've brought together all the components of our Vision-Language-Action (VLA) system to create a fully autonomous humanoid robot. We've integrated voice input, LLM planning, and multi-modal perception into a cohesive system capable of complex autonomous behaviors. The system demonstrates advanced capabilities in natural language understanding, scene perception, planning, and execution."}),"\n",(0,i.jsx)(n.p,{children:"The complete VLA system represents a significant step toward truly autonomous humanoid robots that can understand and execute complex human instructions in natural language while perceiving and interacting with the real world. This foundation provides a platform for further development of advanced robotic capabilities and applications."}),"\n",(0,i.jsx)(n.p,{children:"Throughout Module 4, we've built a comprehensive system that showcases the power of integrating multiple AI modalities for robotics applications. The combination of vision, language, and action creates a robot that can engage in natural human-robot interaction while performing complex tasks in real-world environments."})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var s=t(6540);const i={},a=s.createContext(i);function o(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);