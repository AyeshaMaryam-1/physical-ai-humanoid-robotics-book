"use strict";(globalThis.webpackChunkbook_temp=globalThis.webpackChunkbook_temp||[]).push([[731],{5555:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>t,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module_2/2_2_sim_sensors","title":"Sensors in Simulation","description":"Purpose","source":"@site/docs/02_module_2/2_2_sim_sensors.mdx","sourceDirName":"02_module_2","slug":"/module_2/2_2_sim_sensors","permalink":"/physical-ai-humanoid-robotics-book/docs/module_2/2_2_sim_sensors","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/02_module_2/2_2_sim_sensors.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Sensors in Simulation"},"sidebar":"tutorialSidebar","previous":{"title":"Simulation Fundamentals in Gazebo","permalink":"/physical-ai-humanoid-robotics-book/docs/module_2/2_1_gazebo_fundamentals"},"next":{"title":"High-Fidelity Visualization (Unity)","permalink":"/physical-ai-humanoid-robotics-book/docs/module_2/2_3_unity_viz"}}');var r=i(4848),o=i(8453);const t={sidebar_position:2,title:"Sensors in Simulation"},s="Sensors in Simulation",l={},c=[{value:"Purpose",id:"purpose",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Inputs",id:"inputs",level:2},{value:"Outputs",id:"outputs",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Chapter Outline",id:"chapter-outline",level:2},{value:"1. Camera Integration",id:"1-camera-integration",level:3},{value:"2. Range Sensors",id:"2-range-sensors",level:3},{value:"3. Inertial Sensors",id:"3-inertial-sensors",level:3},{value:"4. Sensor Processing",id:"4-sensor-processing",level:3},{value:"Hands-On Lab",id:"hands-on-lab",level:2},{value:"Adding Sensors to Humanoid Robot in Gazebo",id:"adding-sensors-to-humanoid-robot-in-gazebo",level:3},{value:"Safety Notes",id:"safety-notes",level:2},{value:"Evaluation Criteria",id:"evaluation-criteria",level:2}];function m(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"sensors-in-simulation",children:"Sensors in Simulation"})}),"\n",(0,r.jsx)(e.h2,{id:"purpose",children:"Purpose"}),"\n",(0,r.jsx)(e.p,{children:"This chapter focuses on integrating realistic sensors into our Gazebo simulation environment. We'll add simulated cameras, LiDAR, IMUs, and other sensors to our humanoid robot, enabling perception capabilities in simulation that mirror real-world sensor systems."}),"\n",(0,r.jsx)(e.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,r.jsx)(e.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Add realistic sensor models to URDF for simulation"}),"\n",(0,r.jsx)(e.li,{children:"Configure simulated cameras, LiDAR, and IMU sensors in Gazebo"}),"\n",(0,r.jsx)(e.li,{children:"Process and interpret simulated sensor data in ROS 2"}),"\n",(0,r.jsx)(e.li,{children:"Implement sensor fusion techniques for improved perception"}),"\n",(0,r.jsx)(e.li,{children:"Validate sensor data quality and accuracy in simulation"}),"\n",(0,r.jsx)(e.li,{children:"Integrate sensor data with robot state estimation systems"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Completed Module 1 (ROS 2 fundamentals and humanoid URDF)"}),"\n",(0,r.jsx)(e.li,{children:"Completed Chapter 2.1 (Gazebo simulation fundamentals)"}),"\n",(0,r.jsx)(e.li,{children:"Understanding of sensor types and their applications in robotics"}),"\n",(0,r.jsx)(e.li,{children:"Knowledge of ROS 2 sensor message types and processing"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"inputs",children:"Inputs"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Working humanoid robot model with Gazebo integration"}),"\n",(0,r.jsx)(e.li,{children:"Understanding of Gazebo sensor plugins"}),"\n",(0,r.jsx)(e.li,{children:"Knowledge of ROS 2 sensor message types (sensor_msgs)"}),"\n",(0,r.jsx)(e.li,{children:"Completed simulation environment from Chapter 2.1"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"outputs",children:"Outputs"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Humanoid robot model enhanced with simulated sensors"}),"\n",(0,r.jsx)(e.li,{children:"Working sensor data streams from simulation"}),"\n",(0,r.jsx)(e.li,{children:"Sensor processing nodes for data interpretation"}),"\n",(0,r.jsx)(e.li,{children:"Integration with robot state estimation systems"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Sensor Simulation"}),": Accurate modeling of real-world sensors in virtual environments"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Gazebo Sensor Plugins"}),": Specialized plugins for different sensor types (camera, LiDAR, IMU, etc.)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Sensor Noise Models"}),": Realistic noise characteristics to match real sensor behavior"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Sensor Data Processing"}),": ROS 2 nodes for interpreting and utilizing sensor data"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Point Clouds"}),": 3D spatial data representation from depth sensors and LiDAR"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Sensor Fusion"}),": Combining data from multiple sensors for improved perception"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Calibration"}),": Ensuring simulated sensors match real-world characteristics"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"chapter-outline",children:"Chapter Outline"}),"\n",(0,r.jsx)(e.p,{children:"This chapter builds sensor capabilities into our simulation:"}),"\n",(0,r.jsx)(e.h3,{id:"1-camera-integration",children:"1. Camera Integration"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"RGB camera simulation with realistic parameters"}),"\n",(0,r.jsx)(e.li,{children:"Depth camera simulation for 3D perception"}),"\n",(0,r.jsx)(e.li,{children:"Stereo vision setup for depth estimation"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"2-range-sensors",children:"2. Range Sensors"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"LiDAR simulation with configurable parameters"}),"\n",(0,r.jsx)(e.li,{children:"Sonar and infrared sensor simulation"}),"\n",(0,r.jsx)(e.li,{children:"Multi-beam range sensor arrays"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"3-inertial-sensors",children:"3. Inertial Sensors"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"IMU simulation for orientation and acceleration"}),"\n",(0,r.jsx)(e.li,{children:"Gyroscope and accelerometer modeling"}),"\n",(0,r.jsx)(e.li,{children:"Sensor fusion for state estimation"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"4-sensor-processing",children:"4. Sensor Processing"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Real-time sensor data processing"}),"\n",(0,r.jsx)(e.li,{children:"Noise filtering and validation"}),"\n",(0,r.jsx)(e.li,{children:"Integration with perception systems"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"hands-on-lab",children:"Hands-On Lab"}),"\n",(0,r.jsx)(e.h3,{id:"adding-sensors-to-humanoid-robot-in-gazebo",children:"Adding Sensors to Humanoid Robot in Gazebo"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["Update the URDF file to include simulated sensors in ",(0,r.jsx)(e.code,{children:"humanoid_description/urdf/simple_humanoid.urdf"}),":"]}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<robot name="simple_humanoid_with_sensors">\n  \x3c!-- Base link --\x3e\n  <link name="base_link">\n    <visual>\n      <geometry>\n        <box size="0.3 0.2 0.1"/>\n      </geometry>\n      <material name="blue">\n        <color rgba="0 0 1 0.8"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <box size="0.3 0.2 0.1"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="5.0"/>\n      <inertia ixx="0.1" ixy="0.0" ixz="0.0" iyy="0.1" iyz="0.0" izz="0.1"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Torso --\x3e\n  <link name="torso">\n    <visual>\n      <geometry>\n        <box size="0.2 0.15 0.4"/>\n      </geometry>\n      <material name="gray">\n        <color rgba="0.5 0.5 0.5 0.8"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <box size="0.2 0.15 0.4"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="3.0"/>\n      <inertia ixx="0.05" ixy="0.0" ixz="0.0" iyy="0.05" iyz="0.0" izz="0.05"/>\n    </inertial>\n  </link>\n\n  <joint name="base_to_torso" type="fixed">\n    <parent link="base_link"/>\n    <child link="torso"/>\n    <origin xyz="0 0 0.25"/>\n  </joint>\n\n  \x3c!-- Head with sensors --\x3e\n  <link name="head">\n    <visual>\n      <geometry>\n        <sphere radius="0.1"/>\n      </geometry>\n      <material name="white">\n        <color rgba="1 1 1 0.8"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <sphere radius="0.1"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="1.0"/>\n      <inertia ixx="0.01" ixy="0.0" ixz="0.0" iyy="0.01" iyz="0.0" izz="0.01"/>\n    </inertial>\n  </link>\n\n  <joint name="torso_to_head" type="fixed">\n    <parent link="torso"/>\n    <child link="head"/>\n    <origin xyz="0 0 0.3"/>\n  </joint>\n\n  \x3c!-- RGB Camera on head --\x3e\n  <link name="camera_link">\n    <visual>\n      <geometry>\n        <box size="0.05 0.05 0.05"/>\n      </geometry>\n      <material name="black">\n        <color rgba="0 0 0 0.8"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <box size="0.05 0.05 0.05"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="0.1"/>\n      <inertia ixx="0.001" ixy="0.0" ixz="0.0" iyy="0.001" iyz="0.0" izz="0.001"/>\n    </inertial>\n  </link>\n\n  <joint name="head_to_camera" type="fixed">\n    <parent link="head"/>\n    <child link="camera_link"/>\n    <origin xyz="0.05 0 0" rpy="0 0 0"/>\n  </joint>\n\n  \x3c!-- Gazebo plugin for RGB camera --\x3e\n  <gazebo reference="camera_link">\n    <sensor name="camera" type="camera">\n      <always_on>true</always_on>\n      <visualize>true</visualize>\n      <camera>\n        <horizontal_fov>1.047</horizontal_fov> \x3c!-- 60 degrees --\x3e\n        <image>\n          <width>640</width>\n          <height>480</height>\n          <format>R8G8B8</format>\n        </image>\n        <clip>\n          <near>0.1</near>\n          <far>30.0</far>\n        </clip>\n      </camera>\n      <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\n        <frame_name>camera_link</frame_name>\n        <topic_name>camera/image_raw</topic_name>\n        <hack_baseline>0.07</hack_baseline>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n  \x3c!-- Depth Camera on head --\x3e\n  <link name="depth_camera_link">\n    <visual>\n      <geometry>\n        <box size="0.05 0.05 0.05"/>\n      </geometry>\n      <material name="red">\n        <color rgba="1 0 0 0.8"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <box size="0.05 0.05 0.05"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="0.1"/>\n      <inertia ixx="0.001" ixy="0.0" ixz="0.0" iyy="0.001" iyz="0.0" izz="0.001"/>\n    </inertial>\n  </link>\n\n  <joint name="head_to_depth_camera" type="fixed">\n    <parent link="head"/>\n    <child link="depth_camera_link"/>\n    <origin xyz="0.05 0.05 0" rpy="0 0 0"/>\n  </joint>\n\n  \x3c!-- Gazebo plugin for depth camera --\x3e\n  <gazebo reference="depth_camera_link">\n    <sensor name="depth_camera" type="depth">\n      <always_on>true</always_on>\n      <visualize>true</visualize>\n      <camera>\n        <horizontal_fov>1.047</horizontal_fov> \x3c!-- 60 degrees --\x3e\n        <image>\n          <width>640</width>\n          <height>480</height>\n          <format>R8G8B8</format>\n        </image>\n        <clip>\n          <near>0.1</near>\n          <far>10.0</far>\n        </clip>\n        <noise>\n          <type>gaussian</type>\n          <mean>0.0</mean>\n          <stddev>0.007</stddev>\n        </noise>\n      </camera>\n      <plugin name="depth_camera_controller" filename="libgazebo_ros_depth_camera.so">\n        <frame_name>depth_camera_link</frame_name>\n        <rgb_topic_name>depth_camera/image_raw</rgb_topic_name>\n        <depth_image_topic_name>depth_camera/depth/image_raw</depth_image_topic_name>\n        <point_cloud_topic_name>depth_camera/points</point_cloud_topic_name>\n        <camera_info_topic_name>depth_camera/camera_info</camera_info_topic_name>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n  \x3c!-- 3D LiDAR on head --\x3e\n  <link name="lidar_link">\n    <visual>\n      <geometry>\n        <cylinder radius="0.03" length="0.05"/>\n      </geometry>\n      <material name="green">\n        <color rgba="0 1 0 0.8"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder radius="0.03" length="0.05"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="0.2"/>\n      <inertia ixx="0.001" ixy="0.0" ixz="0.0" iyy="0.001" iyz="0.0" izz="0.002"/>\n    </inertial>\n  </link>\n\n  <joint name="head_to_lidar" type="fixed">\n    <parent link="head"/>\n    <child link="lidar_link"/>\n    <origin xyz="0.05 -0.05 0.025" rpy="0 0 0"/>\n  </joint>\n\n  \x3c!-- Gazebo plugin for 3D LiDAR --\x3e\n  <gazebo reference="lidar_link">\n    <sensor name="lidar" type="ray">\n      <always_on>true</always_on>\n      <visualize>true</visualize>\n      <ray>\n        <scan>\n          <horizontal>\n            <samples>640</samples>\n            <resolution>1</resolution>\n            <min_angle>-1.570796</min_angle> \x3c!-- -90 degrees --\x3e\n            <max_angle>1.570796</max_angle>   \x3c!-- 90 degrees --\x3e\n          </horizontal>\n          <vertical>\n            <samples>16</samples>\n            <resolution>1</resolution>\n            <min_angle>-0.174533</min_angle> \x3c!-- -10 degrees --\x3e\n            <max_angle>0.174533</max_angle>   \x3c!-- 10 degrees --\x3e\n          </vertical>\n        </scan>\n        <range>\n          <min>0.1</min>\n          <max>30.0</max>\n          <resolution>0.01</resolution>\n        </range>\n      </ray>\n      <plugin name="lidar_controller" filename="libgazebo_ros_ray_sensor.so">\n        <ros>\n          <namespace>lidar</namespace>\n          <remapping>~/out:=scan</remapping>\n        </ros>\n        <output_type>sensor_msgs/LaserScan</output_type>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n  \x3c!-- IMU sensor in torso --\x3e\n  <link name="imu_link">\n    <inertial>\n      <mass value="0.01"/>\n      <inertia ixx="0.0001" ixy="0.0" ixz="0.0" iyy="0.0001" iyz="0.0" izz="0.0001"/>\n    </inertial>\n  </link>\n\n  <joint name="torso_to_imu" type="fixed">\n    <parent link="torso"/>\n    <child link="imu_link"/>\n    <origin xyz="0 0 0"/>\n  </joint>\n\n  \x3c!-- Gazebo plugin for IMU --\x3e\n  <gazebo reference="imu_link">\n    <sensor name="imu_sensor" type="imu">\n      <always_on>true</always_on>\n      <update_rate>100</update_rate>\n      <visualize>false</visualize>\n      <imu>\n        <angular_velocity>\n          <x>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>0.0017</stddev>\n            </noise>\n          </x>\n          <y>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>0.0017</stddev>\n            </noise>\n          </y>\n          <z>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>0.0017</stddev>\n            </noise>\n          </z>\n        </angular_velocity>\n        <linear_acceleration>\n          <x>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>0.017</stddev>\n            </noise>\n          </x>\n          <y>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>0.017</stddev>\n            </noise>\n          </y>\n          <z>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>0.017</stddev>\n            </noise>\n          </z>\n        </linear_acceleration>\n      </imu>\n      <plugin name="imu_controller" filename="libgazebo_ros_imu_sensor.so">\n        <ros>\n          <namespace>imu</namespace>\n          <remapping>~/out:=data</remapping>\n        </ros>\n        <frame_name>imu_link</frame_name>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n  \x3c!-- Left Arm --\x3e\n  <link name="left_upper_arm">\n    <visual>\n      <geometry>\n        <cylinder length="0.3" radius="0.05"/>\n      </geometry>\n      <material name="red">\n        <color rgba="1 0 0 0.8"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder length="0.3" radius="0.05"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="0.8"/>\n      <inertia ixx="0.01" ixy="0.0" ixz="0.0" iyy="0.01" iyz="0.0" izz="0.001"/>\n    </inertial>\n  </link>\n\n  <joint name="left_shoulder" type="revolute">\n    <parent link="torso"/>\n    <child link="left_upper_arm"/>\n    <origin xyz="0.15 0.1 0.1"/>\n    <axis xyz="0 1 0"/>\n    <limit lower="-1.57" upper="1.57" effort="100" velocity="1"/>\n  </joint>\n\n  <link name="left_lower_arm">\n    <visual>\n      <geometry>\n        <cylinder length="0.3" radius="0.04"/>\n      </geometry>\n      <material name="red">\n        <color rgba="1 0 0 0.8"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder length="0.3" radius="0.04"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="0.6"/>\n      <inertia ixx="0.008" ixy="0.0" ixz="0.0" iyy="0.008" iyz="0.0" izz="0.001"/>\n    </inertial>\n  </link>\n\n  <joint name="left_elbow" type="revolute">\n    <parent link="left_upper_arm"/>\n    <child link="left_lower_arm"/>\n    <origin xyz="0 0 -0.3"/>\n    <axis xyz="0 1 0"/>\n    <limit lower="0" upper="2.35" effort="100" velocity="1"/>\n  </joint>\n\n  \x3c!-- Right Arm --\x3e\n  <link name="right_upper_arm">\n    <visual>\n      <geometry>\n        <cylinder length="0.3" radius="0.05"/>\n      </geometry>\n      <material name="red">\n        <color rgba="1 0 0 0.8"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder length="0.3" radius="0.05"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="0.8"/>\n      <inertia ixx="0.01" ixy="0.0" ixz="0.0" iyy="0.01" iyz="0.0" izz="0.001"/>\n    </inertial>\n  </link>\n\n  <joint name="right_shoulder" type="revolute">\n    <parent link="torso"/>\n    <child link="right_upper_arm"/>\n    <origin xyz="0.15 -0.1 0.1"/>\n    <axis xyz="0 1 0"/>\n    <limit lower="-1.57" upper="1.57" effort="100" velocity="1"/>\n  </joint>\n\n  <link name="right_lower_arm">\n    <visual>\n      <geometry>\n        <cylinder length="0.3" radius="0.04"/>\n      </geometry>\n      <material name="red">\n        <color rgba="1 0 0 0.8"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder length="0.3" radius="0.04"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="0.6"/>\n      <inertia ixx="0.008" ixy="0.0" ixz="0.0" iyy="0.008" iyz="0.0" izz="0.001"/>\n    </inertial>\n  </link>\n\n  <joint name="right_elbow" type="revolute">\n    <parent link="right_upper_arm"/>\n    <child link="right_lower_arm"/>\n    <origin xyz="0 0 -0.3"/>\n    <axis xyz="0 1 0"/>\n    <limit lower="0" upper="2.35" effort="100" velocity="1"/>\n  </joint>\n\n  \x3c!-- Left Leg --\x3e\n  <link name="left_upper_leg">\n    <visual>\n      <geometry>\n        <cylinder length="0.4" radius="0.06"/>\n      </geometry>\n      <material name="green">\n        <color rgba="0 1 0 0.8"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder length="0.4" radius="0.06"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="1.2"/>\n      <inertia ixx="0.02" ixy="0.0" ixz="0.0" iyy="0.02" iyz="0.0" izz="0.002"/>\n    </inertial>\n  </link>\n\n  <joint name="left_hip" type="revolute">\n    <parent link="base_link"/>\n    <child link="left_upper_leg"/>\n    <origin xyz="-0.1 0.1 -0.05"/>\n    <axis xyz="0 1 0"/>\n    <limit lower="-1.57" upper="1.57" effort="100" velocity="1"/>\n  </joint>\n\n  <link name="left_lower_leg">\n    <visual>\n      <geometry>\n        <cylinder length="0.4" radius="0.05"/>\n      </geometry>\n      <material name="green">\n        <color rgba="0 1 0 0.8"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder length="0.4" radius="0.05"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="1.0"/>\n      <inertia ixx="0.015" ixy="0.0" ixz="0.0" iyy="0.015" iyz="0.0" izz="0.0015"/>\n    </inertial>\n  </link>\n\n  <joint name="left_knee" type="revolute">\n    <parent link="left_upper_leg"/>\n    <child link="left_lower_leg"/>\n    <origin xyz="0 0 -0.4"/>\n    <axis xyz="0 1 0"/>\n    <limit lower="0" upper="2.35" effort="100" velocity="1"/>\n  </joint>\n\n  \x3c!-- Right Leg --\x3e\n  <link name="right_upper_leg">\n    <visual>\n      <geometry>\n        <cylinder length="0.4" radius="0.06"/>\n      </geometry>\n      <material name="green">\n        <color rgba="0 1 0 0.8"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder length="0.4" radius="0.06"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="1.2"/>\n      <inertia ixx="0.02" ixy="0.0" ixz="0.0" iyy="0.02" iyz="0.0" izz="0.002"/>\n    </inertial>\n  </link>\n\n  <joint name="right_hip" type="revolute">\n    <parent link="base_link"/>\n    <child link="right_upper_leg"/>\n    <origin xyz="-0.1 -0.1 -0.05"/>\n    <axis xyz="0 1 0"/>\n    <limit lower="-1.57" upper="1.57" effort="100" velocity="1"/>\n  </joint>\n\n  <link name="right_lower_leg">\n    <visual>\n      <geometry>\n        <cylinder length="0.4" radius="0.05"/>\n      </geometry>\n      <material name="green">\n        <color rgba="0 1 0 0.8"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder length="0.4" radius="0.05"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="1.0"/>\n      <inertia ixx="0.015" ixy="0.0" ixz="0.0" iyy="0.015" iyz="0.0" izz="0.0015"/>\n    </inertial>\n  </link>\n\n  <joint name="right_knee" type="revolute">\n    <parent link="right_upper_leg"/>\n    <child link="right_lower_leg"/>\n    <origin xyz="0 0 -0.4"/>\n    <axis xyz="0 1 0"/>\n    <limit lower="0" upper="2.35" effort="100" velocity="1"/>\n  </joint>\n\n  \x3c!-- ros2_control interface --\x3e\n  <ros2_control name="GazeboSystem" type="system">\n    <hardware>\n      <plugin>gazebo_ros2_control/GazeboSystem</plugin>\n    </hardware>\n    <joint name="left_shoulder">\n      <command_interface name="position">\n        <param name="min">-1.57</param>\n        <param name="max">1.57</param>\n      </command_interface>\n      <state_interface name="position"/>\n      <state_interface name="velocity"/>\n    </joint>\n    <joint name="left_elbow">\n      <command_interface name="position">\n        <param name="min">0</param>\n        <param name="max">2.35</param>\n      </command_interface>\n      <state_interface name="position"/>\n      <state_interface name="velocity"/>\n    </joint>\n    <joint name="right_shoulder">\n      <command_interface name="position">\n        <param name="min">-1.57</param>\n        <param name="max">1.57</param>\n      </command_interface>\n      <state_interface name="position"/>\n      <state_interface name="velocity"/>\n    </joint>\n    <joint name="right_elbow">\n      <command_interface name="position">\n        <param name="min">0</param>\n        <param name="max">2.35</param>\n      </command_interface>\n      <state_interface name="position"/>\n      <state_interface name="velocity"/>\n    </joint>\n    <joint name="left_hip">\n      <command_interface name="position">\n        <param name="min">-1.57</param>\n        <param name="max">1.57</param>\n      </command_interface>\n      <state_interface name="position"/>\n      <state_interface name="velocity"/>\n    </joint>\n    <joint name="left_knee">\n      <command_interface name="position">\n        <param name="min">0</param>\n        <param name="max">2.35</param>\n      </command_interface>\n      <state_interface name="position"/>\n      <state_interface name="velocity"/>\n    </joint>\n    <joint name="right_hip">\n      <command_interface name="position">\n        <param name="min">-1.57</param>\n        <param name="max">1.57</param>\n      </command_interface>\n      <state_interface name="position"/>\n      <state_interface name="velocity"/>\n    </joint>\n    <joint name="right_knee">\n      <command_interface name="position">\n        <param name="min">0</param>\n        <param name="max">2.35</param>\n      </command_interface>\n      <state_interface name="position"/>\n      <state_interface name="velocity"/>\n    </joint>\n  </ros2_control>\n</robot>\n'})}),"\n",(0,r.jsxs)(e.ol,{start:"2",children:["\n",(0,r.jsxs)(e.li,{children:["Create a sensor processing node to demonstrate sensor data interpretation in ",(0,r.jsx)(e.code,{children:"ai_control_agent/ai_control_agent/sensor_processor.py"}),":"]}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan, Imu, PointCloud2, CameraInfo\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport math\n\n\nclass SensorProcessorNode(Node):\n    def __init__(self):\n        super().__init__('sensor_processor')\n\n        # Create subscribers for different sensor types\n        self.camera_subscriber = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.camera_callback,\n            10\n        )\n\n        self.depth_camera_subscriber = self.create_subscription(\n            Image,\n            '/depth_camera/depth/image_raw',\n            self.depth_camera_callback,\n            10\n        )\n\n        self.lidar_subscriber = self.create_subscription(\n            LaserScan,\n            '/lidar/scan',\n            self.lidar_callback,\n            10\n        )\n\n        self.imu_subscriber = self.create_subscription(\n            Imu,\n            '/imu/data',\n            self.imu_callback,\n            10\n        )\n\n        # Create publishers for processed sensor data\n        self.obstacle_publisher = self.create_publisher(\n            LaserScan,\n            '/processed_scan',\n            10\n        )\n\n        # Initialize CV Bridge for image processing\n        self.cv_bridge = CvBridge()\n\n        # Statistics tracking\n        self.scan_count = 0\n        self.camera_count = 0\n        self.imu_count = 0\n\n        self.get_logger().info('Sensor Processor Node initialized - Subscribed to all sensor topics')\n\n    def camera_callback(self, msg):\n        \"\"\"Process RGB camera data\"\"\"\n        try:\n            # Convert ROS Image message to OpenCV image\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, \"bgr8\")\n\n            # Process image (simple example: get image dimensions)\n            height, width, channels = cv_image.shape\n\n            self.camera_count += 1\n            if self.camera_count % 30 == 0:  # Log every 30th image\n                self.get_logger().info(f'Camera: Received image {self.camera_count} - Size: {width}x{height}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing camera image: {str(e)}')\n\n    def depth_camera_callback(self, msg):\n        \"\"\"Process depth camera data\"\"\"\n        try:\n            # Convert ROS Image message to OpenCV image (depth data)\n            depth_image = self.cv_bridge.imgmsg_to_cv2(msg, \"32FC1\")\n\n            # Get some statistics from the depth image\n            valid_depths = depth_image[depth_image > 0]  # Filter out invalid depths\n\n            if len(valid_depths) > 0:\n                avg_depth = np.mean(valid_depths)\n                min_depth = np.min(valid_depths)\n                max_depth = np.max(valid_depths)\n\n                if self.camera_count % 30 == 0:  # Log every 30th depth image\n                    self.get_logger().info(\n                        f'Depth Camera: Avg: {avg_depth:.2f}m, Min: {min_depth:.2f}m, Max: {max_depth:.2f}m'\n                    )\n        except Exception as e:\n            self.get_logger().error(f'Error processing depth camera image: {str(e)}')\n\n    def lidar_callback(self, msg):\n        \"\"\"Process LiDAR data for obstacle detection\"\"\"\n        try:\n            # Process laser scan data\n            ranges = np.array(msg.ranges)\n\n            # Filter out invalid ranges (inf, nan) and get valid distances\n            valid_ranges = ranges[np.isfinite(ranges)]\n\n            if len(valid_ranges) > 0:\n                min_distance = np.min(valid_ranges)\n                avg_distance = np.mean(valid_ranges)\n\n                # Check for obstacles in front (first 30 degrees)\n                front_ranges = ranges[:len(ranges)//12]  # First 30 degrees (360/12)\n                front_valid = front_ranges[np.isfinite(front_ranges)]\n\n                if len(front_valid) > 0 and np.min(front_valid) < 1.0:  # Obstacle within 1m\n                    self.get_logger().warn(f'OBSTACLE DETECTED: {np.min(front_valid):.2f}m ahead!')\n                else:\n                    self.get_logger().info(f'Clear path ahead: min distance {min_distance:.2f}m')\n\n                # Publish processed scan with obstacle information\n                processed_scan = LaserScan()\n                processed_scan.header = msg.header\n                processed_scan.angle_min = msg.angle_min\n                processed_scan.angle_max = msg.angle_max\n                processed_scan.angle_increment = msg.angle_increment\n                processed_scan.time_increment = msg.time_increment\n                processed_scan.scan_time = msg.scan_time\n                processed_scan.range_min = msg.range_min\n                processed_scan.range_max = msg.range_max\n                processed_scan.ranges = msg.ranges\n\n                self.obstacle_publisher.publish(processed_scan)\n\n                self.scan_count += 1\n                if self.scan_count % 10 == 0:  # Log every 10th scan\n                    self.get_logger().info(\n                        f'LiDAR: {len(valid_ranges)} valid readings, min: {min_distance:.2f}m, avg: {avg_distance:.2f}m'\n                    )\n        except Exception as e:\n            self.get_logger().error(f'Error processing LiDAR data: {str(e)}')\n\n    def imu_callback(self, msg):\n        \"\"\"Process IMU data for orientation and acceleration\"\"\"\n        try:\n            # Extract orientation (quaternion)\n            orientation = msg.orientation\n            # Convert quaternion to Euler angles (simplified)\n            # Note: In a real implementation, use tf2 for proper conversion\n            roll = math.atan2(\n                2.0 * (orientation.w * orientation.x + orientation.y * orientation.z),\n                1.0 - 2.0 * (orientation.x * orientation.x + orientation.y * orientation.y)\n            )\n            pitch = math.asin(\n                2.0 * (orientation.w * orientation.y - orientation.z * orientation.x)\n            )\n            yaw = math.atan2(\n                2.0 * (orientation.w * orientation.z + orientation.x * orientation.y),\n                1.0 - 2.0 * (orientation.y * orientation.y + orientation.z * orientation.z)\n            )\n\n            # Extract linear acceleration\n            linear_acc = msg.linear_acceleration\n\n            self.imu_count += 1\n            if self.imu_count % 50 == 0:  # Log every 50th IMU reading\n                self.get_logger().info(\n                    f'IMU: Roll: {math.degrees(roll):.1f}\xb0, Pitch: {math.degrees(pitch):.1f}\xb0, '\n                    f'Yaw: {math.degrees(yaw):.1f}\xb0, Acc: [{linear_acc.x:.2f}, {linear_acc.y:.2f}, {linear_acc.z:.2f}]'\n                )\n        except Exception as e:\n            self.get_logger().error(f'Error processing IMU data: {str(e)}')\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SensorProcessorNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info('Shutting down Sensor Processor Node')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsxs)(e.ol,{start:"3",children:["\n",(0,r.jsx)(e.li,{children:"Update the setup.py file to include the new sensor processor:"}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"import os\nfrom glob import glob\nfrom setuptools import setup\nfrom setuptools import find_packages\n\npackage_name = 'ai_control_agent'\n\nsetup(\n    name=package_name,\n    version='0.0.0',\n    packages=find_packages(exclude=['test']),\n    data_files=[\n        ('share/ament_index/resource_index/packages',\n            ['resource/' + package_name]),\n        ('share/' + package_name, ['package.xml']),\n    ],\n    install_requires=['setuptools', 'cv-bridge', 'numpy'],\n    zip_safe=True,\n    maintainer='Humanoid Robotics Book',\n    maintainer_email='humanoid-robotics-book@example.com',\n    description='AI control agent for the Physical AI & Humanoid Robotics book',\n    license='Apache License 2.0',\n    tests_require=['pytest'],\n    entry_points={\n        'console_scripts': [\n            'simple_ai_agent = ai_control_agent.simple_ai_agent:main',\n            'gazebo_commander = ai_control_agent.gazebo_commander:main',\n            'sensor_processor = ai_control_agent.sensor_processor:main',\n        ],\n    },\n)\n"})}),"\n",(0,r.jsxs)(e.ol,{start:"4",children:["\n",(0,r.jsx)(e.li,{children:"Create a launch file to start the sensor simulation with the humanoid robot:"}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"mkdir -p ~/robotics_ws/src/ai_control_agent/launch\n"})}),"\n",(0,r.jsxs)(e.p,{children:["Create ",(0,r.jsx)(e.code,{children:"ai_control_agent/launch/sensor_simulation.launch.py"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription, RegisterEventHandler\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch_ros.actions import Node\nfrom launch_ros.substitutions import FindPackageShare\nfrom ament_index_python.packages import get_package_share_directory\nfrom launch.event_handlers import OnProcessExit\nfrom launch.actions import ExecuteProcess\nimport os\n\n\ndef generate_launch_description():\n    # Get the package share directory\n    pkg_share = get_package_share_directory('humanoid_description')\n    default_model_path = os.path.join(pkg_share, 'urdf/simple_humanoid.urdf')\n    default_world_path = os.path.join(pkg_share, 'worlds/empty_world.world')\n\n    # Declare launch arguments\n    model_arg = DeclareLaunchArgument(\n        name='model',\n        default_value=default_model_path,\n        description='Absolute path to robot urdf file'\n    )\n\n    world_arg = DeclareLaunchArgument(\n        name='world',\n        default_value=default_world_path,\n        description='Absolute path to world file'\n    )\n\n    # Start Gazebo server and client\n    gazebo = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            PathJoinSubstitution([\n                FindPackageShare('gazebo_ros'),\n                'launch',\n                'gazebo.launch.py'\n            ])\n        ]),\n        launch_arguments={\n            'world': LaunchConfiguration('world'),\n            'verbose': 'false',\n        }.items()\n    )\n\n    # Robot State Publisher node\n    robot_state_publisher_node = Node(\n        package='robot_state_publisher',\n        executable='robot_state_publisher',\n        parameters=[{\n            'robot_description': open(default_model_path).read()\n        }]\n    )\n\n    # Spawn the robot in Gazebo\n    spawn_entity_node = Node(\n        package='gazebo_ros',\n        executable='spawn_entity.py',\n        arguments=[\n            '-file', LaunchConfiguration('model'),\n            '-entity', 'simple_humanoid',\n            '-x', '0.0',\n            '-y', '0.0',\n            '-z', '1.0'\n        ],\n        output='screen'\n    )\n\n    # Sensor Processor node\n    sensor_processor_node = Node(\n        package='ai_control_agent',\n        executable='sensor_processor',\n        name='sensor_processor',\n        output='screen'\n    )\n\n    # Joint State Publisher GUI node\n    joint_state_publisher_gui_node = Node(\n        package='joint_state_publisher_gui',\n        executable='joint_state_publisher_gui',\n        name='joint_state_publisher_gui',\n        output='screen'\n    )\n\n    # RViz2 node\n    rviz_node = Node(\n        package='rviz2',\n        executable='rviz2',\n        name='rviz2',\n        output='screen'\n    )\n\n    return LaunchDescription([\n        model_arg,\n        world_arg,\n        gazebo,\n        robot_state_publisher_node,\n        spawn_entity_node,\n        sensor_processor_node,\n        joint_state_publisher_gui_node,\n        rviz_node\n    ])\n"})}),"\n",(0,r.jsxs)(e.ol,{start:"5",children:["\n",(0,r.jsx)(e.li,{children:"Build the package:"}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"cd ~/robotics_ws\ncolcon build --packages-select humanoid_description ai_control_agent\nsource install/setup.bash\n"})}),"\n",(0,r.jsxs)(e.ol,{start:"6",children:["\n",(0,r.jsx)(e.li,{children:"Test the sensor simulation:"}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"ros2 launch ai_control_agent sensor_simulation.launch.py\n"})}),"\n",(0,r.jsx)(e.h2,{id:"safety-notes",children:"Safety Notes"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Ensure sensor noise parameters match real-world characteristics for realistic simulation"}),"\n",(0,r.jsx)(e.li,{children:"Validate sensor ranges and fields of view are appropriate for the robot's intended use"}),"\n",(0,r.jsx)(e.li,{children:"Implement proper error handling for sensor data processing"}),"\n",(0,r.jsx)(e.li,{children:"Monitor sensor data rates to avoid overwhelming the system"}),"\n",(0,r.jsx)(e.li,{children:"Include sensor health checks and validation in robot control systems"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"evaluation-criteria",children:"Evaluation Criteria"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Are all sensors properly integrated into the URDF model?"}),"\n",(0,r.jsx)(e.li,{children:"Do sensor data streams publish correctly from the simulation?"}),"\n",(0,r.jsx)(e.li,{children:"Can the sensor processor node interpret and process sensor data?"}),"\n",(0,r.jsx)(e.li,{children:"Are sensor noise characteristics realistic?"}),"\n",(0,r.jsx)(e.li,{children:"Is the simulation running at acceptable performance with all sensors active?"}),"\n"]})]})}function d(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(m,{...n})}):m(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>t,x:()=>s});var a=i(6540);const r={},o=a.createContext(r);function t(n){const e=a.useContext(o);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:t(n.components),a.createElement(o.Provider,{value:e},n.children)}}}]);