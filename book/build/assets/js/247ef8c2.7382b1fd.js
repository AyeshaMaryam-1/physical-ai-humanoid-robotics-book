"use strict";(globalThis.webpackChunkbook_temp=globalThis.webpackChunkbook_temp||[]).push([[552],{8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>l});var t=i(6540);const o={},s=t.createContext(o);function a(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),t.createElement(s.Provider,{value:n},e.children)}},9792:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>r,contentTitle:()=>l,default:()=>m,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module_4/4_3_multi_modal","title":"Chapter 4.3: Multi-Modal Perception & Decision Making","description":"Synthesizing voice, vision, and LLM reasoning for intelligent robot behavior","source":"@site/docs/04_module_4/4_3_multi_modal.mdx","sourceDirName":"04_module_4","slug":"/module_4/4_3_multi_modal","permalink":"/physical-ai-humanoid-robotics-book/docs/module_4/4_3_multi_modal","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/04_module_4/4_3_multi_modal.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Chapter 4.3: Multi-Modal Perception & Decision Making","description":"Synthesizing voice, vision, and LLM reasoning for intelligent robot behavior"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4.2: LLM Cognitive Planning","permalink":"/physical-ai-humanoid-robotics-book/docs/module_4/4_2_llm_planning"},"next":{"title":"Chapter 4.4: Capstone Project: The Autonomous Humanoid","permalink":"/physical-ai-humanoid-robotics-book/docs/module_4/4_4_capstone"}}');var o=i(4848),s=i(8453);const a={sidebar_position:3,title:"Chapter 4.3: Multi-Modal Perception & Decision Making",description:"Synthesizing voice, vision, and LLM reasoning for intelligent robot behavior"},l="Chapter 4.3: Multi-Modal Perception & Decision Making",r={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Multi-Modal Integration Architecture",id:"multi-modal-integration-architecture",level:2},{value:"System Overview",id:"system-overview",level:3},{value:"Fusion Strategies",id:"fusion-strategies",level:3},{value:"Technical Implementation",id:"technical-implementation",level:2},{value:"Multi-Modal Fusion Node",id:"multi-modal-fusion-node",level:3},{value:"Vision Processing Integration",id:"vision-processing-integration",level:3},{value:"Adaptive Decision Making",id:"adaptive-decision-making",level:3},{value:"Demonstration Task",id:"demonstration-task",level:2},{value:"Task: Fetch and Deliver",id:"task-fetch-and-deliver",level:3},{value:"Implementation",id:"implementation",level:3},{value:"Integration with Existing Systems",id:"integration-with-existing-systems",level:2},{value:"Voice-Vision Coordination",id:"voice-vision-coordination",level:3},{value:"Vision-LLM Coordination",id:"vision-llm-coordination",level:3},{value:"Voice-LLM Coordination",id:"voice-llm-coordination",level:3},{value:"Challenges and Solutions",id:"challenges-and-solutions",level:2},{value:"Data Synchronization",id:"data-synchronization",level:3},{value:"Confidence Integration",id:"confidence-integration",level:3},{value:"Computational Complexity",id:"computational-complexity",level:3},{value:"Evaluation Metrics",id:"evaluation-metrics",level:2},{value:"Performance Metrics",id:"performance-metrics",level:3},{value:"Quality Metrics",id:"quality-metrics",level:3},{value:"Next Steps",id:"next-steps",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-43-multi-modal-perception--decision-making",children:"Chapter 4.3: Multi-Modal Perception & Decision Making"})}),"\n",(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(n.p,{children:"In this chapter, we'll explore how to synthesize multi-modal sensor data with LLM reasoning to create intelligent robot behaviors. We'll combine voice input from Chapter 4.1, LLM planning from Chapter 4.2, and vision systems to create a comprehensive multi-modal perception and decision-making system. This integration forms the core of our Vision-Language-Action (VLA) framework."}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"By the end of this chapter, you will:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Understand how to integrate multiple sensory modalities in robotics"}),"\n",(0,o.jsx)(n.li,{children:"Learn to implement multi-modal fusion for enhanced perception"}),"\n",(0,o.jsx)(n.li,{children:"Create a system that combines voice, vision, and LLM reasoning"}),"\n",(0,o.jsx)(n.li,{children:"Implement adaptive decision-making based on multi-modal inputs"}),"\n",(0,o.jsx)(n.li,{children:"Design a demonstration task that showcases multi-modal capabilities"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"multi-modal-integration-architecture",children:"Multi-Modal Integration Architecture"}),"\n",(0,o.jsx)(n.h3,{id:"system-overview",children:"System Overview"}),"\n",(0,o.jsx)(n.p,{children:"Our multi-modal system will integrate:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Voice Input"}),": Natural language commands and queries"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Vision Systems"}),": Object recognition, scene understanding"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"LLM Reasoning"}),": High-level planning and reasoning"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robot Actions"}),": Navigation, manipulation, communication"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"fusion-strategies",children:"Fusion Strategies"}),"\n",(0,o.jsx)(n.p,{children:"There are several approaches to multi-modal fusion:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Early Fusion"}),": Combine raw sensor data before processing"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Late Fusion"}),": Process modalities separately, combine decisions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Intermediate Fusion"}),": Combine at feature or semantic level"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Decision-Level Fusion"}),": Combine final outputs from each modality"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"For our VLA system, we'll use a combination of intermediate and decision-level fusion."}),"\n",(0,o.jsx)(n.h2,{id:"technical-implementation",children:"Technical Implementation"}),"\n",(0,o.jsx)(n.h3,{id:"multi-modal-fusion-node",children:"Multi-Modal Fusion Node"}),"\n",(0,o.jsx)(n.p,{children:"We'll create a fusion node that receives inputs from all modalities and coordinates the overall behavior. First, let's create the ROS 2 package for the multi-modal system:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"mkdir -p robotics_ws/src/vla_multi_modal/vla_multi_modal\n"})}),"\n",(0,o.jsx)(n.p,{children:"The package.xml file:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<?xml-model href="http://download.ros.org/schema/package_format3.xsd" schematypens="http://www.w3.org/2001/XMLSchema"?>\n<package format="3">\n  <name>vla_multi_modal</name>\n  <version>0.0.1</version>\n  <description>Vision-Language-Action multi-modal fusion package</description>\n  <maintainer email="maintainer@todo.todo">Maintainer</maintainer>\n  <license>TODO</license>\n\n  <depend>rclpy</depend>\n  <depend>std_msgs</depend>\n  <depend>sensor_msgs</depend>\n  <depend>vision_msgs</depend>\n  <depend>geometry_msgs</depend>\n  <depend>cv_bridge</depend>\n  <depend>vla_msgs</depend>\n\n  <exec_depend>python3-opencv</exec_depend>\n  <exec_depend>python3-numpy</exec_depend>\n\n  <test_depend>ament_copyright</test_depend>\n  <test_depend>ament_flake8</test_depend>\n  <test_depend>ament_pep257</test_depend>\n  <test_depend>python3-pytest</test_depend>\n\n  <export>\n    <build_type>ament_python</build_type>\n  </export>\n</package>\n'})}),"\n",(0,o.jsx)(n.p,{children:"Now the fusion node implementation:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray\nfrom geometry_msgs.msg import Point\nfrom builtin_interfaces.msg import Time\nfrom vla_msgs.msg import LLMPlan, PlanStep\nimport cv2\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport json\nfrom collections import deque\nimport time\n\n\nclass MultiModalFusionNode(Node):\n    def __init__(self):\n        super().__init__('multi_modal_fusion_node')\n\n        # Initialize components\n        self.bridge = CvBridge()\n\n        # Parameters\n        self.vision_timeout = self.declare_parameter('vision_timeout', 5.0).value\n        self.voice_timeout = self.declare_parameter('voice_timeout', 10.0).value\n        self.fusion_strategy = self.declare_parameter('fusion_strategy', 'intermediate').value\n        self.confidence_threshold = self.declare_parameter('confidence_threshold', 0.7).value\n        self.max_context_history = self.declare_parameter('max_context_history', 10).value\n\n        # Data storage for multi-modal context\n        self.latest_image = None\n        self.latest_detections = None\n        self.latest_voice_command = None\n        self.latest_plan = None\n        self.context_history = deque(maxlen=self.max_context_history)\n\n        # Time stamps for data freshness\n        self.image_timestamp = None\n        self.detections_timestamp = None\n        self.voice_timestamp = None\n        self.plan_timestamp = None\n\n        # Publishers and subscribers\n        self.image_sub = self.create_subscription(\n            Image,\n            'camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        self.detections_sub = self.create_subscription(\n            Detection2DArray,\n            'vision/detections',\n            self.detections_callback,\n            10\n        )\n\n        self.voice_sub = self.create_subscription(\n            String,\n            'transcribed_text',\n            self.voice_callback,\n            10\n        )\n\n        self.plan_sub = self.create_subscription(\n            LLMPlan,\n            'robot_plan',\n            self.plan_callback,\n            10\n        )\n\n        # Publishers for fused decisions\n        self.decision_pub = self.create_publisher(\n            String,\n            'fused_decision',\n            10\n        )\n\n        # Publisher for task context\n        self.task_context_pub = self.create_publisher(\n            String,\n            'task_context',\n            10\n        )\n\n        # Publisher for refined commands to LLM\n        self.refined_command_pub = self.create_publisher(\n            String,\n            'refined_command',\n            10\n        )\n\n        # Timer for fusion processing\n        self.fusion_timer = self.create_timer(0.1, self.fusion_callback)\n\n        self.get_logger().info(\"Multi-modal fusion node initialized\")\n\n    def image_callback(self, msg):\n        \"\"\"Handle incoming camera images\"\"\"\n        try:\n            self.latest_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n            self.image_timestamp = msg.header.stamp\n        except Exception as e:\n            self.get_logger().error(f\"Error processing image: {e}\")\n\n    def detections_callback(self, msg):\n        \"\"\"Handle incoming object detections\"\"\"\n        self.latest_detections = msg\n        self.detections_timestamp = msg.header.stamp\n\n    def voice_callback(self, msg):\n        \"\"\"Handle incoming voice commands\"\"\"\n        self.latest_voice_command = msg.data\n        self.voice_timestamp = self.get_clock().now().to_msg()\n\n    def plan_callback(self, msg):\n        \"\"\"Handle incoming LLM plans\"\"\"\n        self.latest_plan = msg\n        self.plan_timestamp = msg.timestamp\n\n    def fusion_callback(self):\n        \"\"\"Main fusion logic - called periodically\"\"\"\n        # Check if we have recent data from multiple modalities\n        current_time = self.get_clock().now().to_msg()\n\n        # Check for recent voice command\n        if (self.latest_voice_command and\n            self.time_diff(current_time, self.voice_timestamp) < self.voice_timeout):\n\n            # Create task context combining all available information\n            task_context = self.create_task_context()\n\n            # Store in history\n            self.context_history.append(task_context)\n\n            # Publish task context for decision making\n            context_msg = String()\n            context_msg.data = json.dumps(task_context)\n            self.task_context_pub.publish(context_msg)\n\n            # Make fused decision based on all modalities\n            decision = self.make_fused_decision(task_context)\n            if decision:\n                decision_msg = String()\n                decision_msg.data = decision\n                self.decision_pub.publish(decision_msg)\n\n                # If decision requires refined planning, send to LLM\n                if self.should_refine_plan(decision, task_context):\n                    refined_command = self.create_refined_command(decision, task_context)\n                    refined_cmd_msg = String()\n                    refined_cmd_msg.data = refined_command\n                    self.refined_command_pub.publish(refined_cmd_msg)\n\n    def create_task_context(self):\n        \"\"\"Create comprehensive task context from all modalities\"\"\"\n        context = {\n            'timestamp': self.get_clock().now().to_msg().sec,\n            'voice_command': self.latest_voice_command,\n            'vision_data': self.extract_vision_context(),\n            'robot_state': self.get_robot_state(),\n            'environment_context': self.extract_environment_context(),\n            'previous_context': list(self.context_history)[-3:] if self.context_history else []  # Last 3 contexts\n        }\n        return context\n\n    def extract_vision_context(self):\n        \"\"\"Extract relevant information from vision data\"\"\"\n        if not self.latest_detections:\n            return {}\n\n        vision_context = {\n            'objects': [],\n            'spatial_relations': [],\n            'scene_description': 'No detections available',\n            'confidence_summary': 0.0\n        }\n\n        # Extract object information\n        total_confidence = 0.0\n        for detection in self.latest_detections.detections:\n            if detection.results:  # Check if results exist\n                hypothesis = detection.results[0].hypothesis\n                obj_info = {\n                    'label': hypothesis.class_id,\n                    'confidence': hypothesis.score,\n                    'bbox': {\n                        'x': detection.bbox.center.x,\n                        'y': detection.bbox.center.y,\n                        'width': detection.bbox.size_x,\n                        'height': detection.bbox.size_y\n                    },\n                    'center_point': {\n                        'x': detection.bbox.center.x,\n                        'y': detection.bbox.center.y\n                    }\n                }\n                vision_context['objects'].append(obj_info)\n                total_confidence += hypothesis.score\n\n        # Calculate average confidence\n        if vision_context['objects']:\n            vision_context['confidence_summary'] = total_confidence / len(vision_context['objects'])\n\n        # Analyze spatial relationships between objects\n        if len(vision_context['objects']) > 1:\n            vision_context['spatial_relations'] = self.compute_spatial_relations(\n                vision_context['objects']\n            )\n\n        return vision_context\n\n    def compute_spatial_relations(self, objects):\n        \"\"\"Compute spatial relationships between detected objects\"\"\"\n        relations = []\n        for i, obj1 in enumerate(objects):\n            for j, obj2 in enumerate(objects):\n                if i != j:\n                    # Calculate relative positions\n                    dx = obj2['center_point']['x'] - obj1['center_point']['x']\n                    dy = obj2['center_point']['y'] - obj1['center_point']['y']\n\n                    # Determine spatial relationship\n                    distance = np.sqrt(dx*dx + dy*dy)\n                    angle = np.arctan2(dy, dx) * 180 / np.pi  # Convert to degrees\n\n                    relation = {\n                        'from': obj1['label'],\n                        'to': obj2['label'],\n                        'distance': distance,\n                        'angle_degrees': angle,\n                        'relative_position': self.determine_relative_position(dx, dy)\n                    }\n                    relations.append(relation)\n        return relations\n\n    def determine_relative_position(self, dx, dy):\n        \"\"\"Determine relative position based on dx, dy\"\"\"\n        # Define sectors (8 directions)\n        angle = np.arctan2(dy, dx)\n        angle_deg = angle * 180 / np.pi\n\n        if -22.5 <= angle_deg < 22.5:\n            return \"right\"\n        elif 22.5 <= angle_deg < 67.5:\n            return \"bottom-right\"\n        elif 67.5 <= angle_deg < 112.5:\n            return \"bottom\"\n        elif 112.5 <= angle_deg < 157.5:\n            return \"bottom-left\"\n        elif 157.5 <= angle_deg or angle_deg < -157.5:\n            return \"left\"\n        elif -157.5 <= angle_deg < -112.5:\n            return \"top-left\"\n        elif -112.5 <= angle_deg < -67.5:\n            return \"top\"\n        else:  # -67.5 <= angle_deg < -22.5\n            return \"top-right\"\n\n    def get_robot_state(self):\n        \"\"\"Get current robot state (position, battery, etc.)\"\"\"\n        # This would typically come from robot state publisher\n        # For simulation purposes, return mock data\n        return {\n            'position': {'x': 0.0, 'y': 0.0, 'theta': 0.0},\n            'battery_level': 0.8,\n            'current_task': 'idle',\n            'capabilities': ['navigation', 'manipulation', 'communication'],\n            'manipulation_status': 'available',\n            'navigation_status': 'ready'\n        }\n\n    def extract_environment_context(self):\n        \"\"\"Extract environment context from available data\"\"\"\n        # This would integrate with mapping and localization systems\n        return {\n            'room_type': 'unknown',  # Would be determined from scene analysis\n            'lighting_condition': 'normal',  # Would be analyzed from image\n            'obstacles': [],  # Would come from navigation system\n            'navigable_areas': [],  # Would come from mapping system\n            'landmarks': []  # Would come from SLAM system\n        }\n\n    def make_fused_decision(self, context):\n        \"\"\"Make decision by fusing information from all modalities\"\"\"\n        voice_cmd = context.get('voice_command', '').lower()\n        vision_data = context.get('vision_data', {})\n        robot_state = context.get('robot_state', {})\n\n        # Analyze confidence levels\n        vision_confidence = vision_data.get('confidence_summary', 0.0)\n        voice_confidence = 1.0  # Assume voice input is reliable\n\n        # If vision confidence is too low, request clarification or exploration\n        if vision_confidence < self.confidence_threshold and 'where' not in voice_cmd and 'what' not in voice_cmd:\n            return f\"request_visual_clarification: {voice_cmd}\"\n\n        # Example decision logic based on multi-modal input\n        decision = \"\"\n\n        if 'bring me' in voice_cmd or 'get me' in voice_cmd or 'fetch' in voice_cmd:\n            # Object retrieval task\n            target_object = self.extract_target_object(voice_cmd)\n            if target_object:\n                # Check if object is visible\n                visible_objects = [obj['label'] for obj in vision_data.get('objects', [])]\n                if target_object in visible_objects:\n                    # Find the specific object instance\n                    target_obj_data = None\n                    for obj in vision_data.get('objects', []):\n                        if obj['label'] == target_object:\n                            target_obj_data = obj\n                            break\n\n                    if target_obj_data:\n                        decision = f\"object_retrieval_confirmed: {target_object}, position: ({target_obj_data['center_point']['x']}, {target_obj_data['center_point']['y']})\"\n                    else:\n                        decision = f\"object_found: {target_object}\"\n                else:\n                    decision = f\"searching_for: {target_object}\"\n            else:\n                decision = \"object_not_specified\"\n\n        elif 'go to' in voice_cmd or 'move to' in voice_cmd or 'navigate to' in voice_cmd:\n            # Navigation task\n            target_location = self.extract_target_location(voice_cmd)\n            if target_location:\n                decision = f\"navigate_to: {target_location}\"\n            else:\n                decision = \"location_not_specified\"\n\n        elif 'what do you see' in voice_cmd or 'describe scene' in voice_cmd:\n            # Scene description task\n            objects = [obj['label'] for obj in vision_data.get('objects', [])]\n            if objects:\n                decision = f\"scene_description: I see {', '.join(objects)}\"\n            else:\n                decision = \"no_objects_detected\"\n\n        elif 'where is' in voice_cmd or 'find' in voice_cmd:\n            # Object localization task\n            target_object = self.extract_target_object(voice_cmd)\n            if target_object:\n                visible_objects = [obj for obj in vision_data.get('objects', []) if obj['label'] == target_object]\n                if visible_objects:\n                    obj_info = visible_objects[0]\n                    decision = f\"object_location: The {target_object} is at coordinates ({obj_info['center_point']['x']}, {obj_info['center_point']['y']}) with confidence {obj_info['confidence']:.2f}\"\n                else:\n                    decision = f\"object_not_found: {target_object}\"\n            else:\n                decision = \"object_not_specified\"\n\n        else:\n            # Default case - pass to LLM planner with context\n            decision = f\"llm_planning_needed_with_context: {voice_cmd}\"\n\n        return decision\n\n    def extract_target_object(self, command):\n        \"\"\"Extract target object from voice command using keyword matching\"\"\"\n        # Extended list of common objects\n        common_objects = [\n            'cup', 'bottle', 'water bottle', 'coffee cup', 'mug', 'glass',\n            'book', 'phone', 'mobile', 'cellphone', 'keys', 'wallet',\n            'pen', 'pencil', 'paper', 'notebook', 'laptop', 'tablet',\n            'chair', 'table', 'sofa', 'couch', 'bed', 'desk',\n            'apple', 'banana', 'orange', 'fruit', 'snack', 'food',\n            'ball', 'toy', 'remote', 'tv remote', 'bowl', 'plate',\n            'box', 'bag', 'backpack', 'purse', 'hat', 'shoe'\n        ]\n\n        command_lower = command.lower()\n        for obj in common_objects:\n            if obj in command_lower:\n                return obj\n\n        # If no exact match, try partial matching\n        for obj in common_objects:\n            if any(word in command_lower for word in obj.split()):\n                return obj\n\n        return None\n\n    def extract_target_location(self, command):\n        \"\"\"Extract target location from voice command\"\"\"\n        # Extended list of common locations\n        common_locations = [\n            'kitchen', 'living room', 'bedroom', 'office', 'bathroom',\n            'dining room', 'hallway', 'garage', 'garden', 'patio',\n            'entrance', 'dining area', 'work area', 'study', 'library',\n            'dormitory', 'classroom', 'laboratory', 'workshop'\n        ]\n\n        command_lower = command.lower()\n        for loc in common_locations:\n            if loc in command_lower:\n                return loc\n\n        # If no exact match, try partial matching\n        for loc in common_locations:\n            if any(word in command_lower for word in loc.split()):\n                return loc\n\n        return None\n\n    def should_refine_plan(self, decision, context):\n        \"\"\"Determine if the decision requires refined planning\"\"\"\n        return ('object_retrieval_confirmed' in decision or\n                'navigate_to' in decision or\n                'llm_planning_needed_with_context' in decision)\n\n    def create_refined_command(self, decision, context):\n        \"\"\"Create a refined command for the LLM planner with multi-modal context\"\"\"\n        original_command = context.get('voice_command', '')\n        vision_data = context.get('vision_data', {})\n\n        if 'object_retrieval_confirmed' in decision:\n            # Extract object and position\n            import re\n            obj_match = re.search(r'object_retrieval_confirmed: ([^,]+)', decision)\n            pos_match = re.search(r'position: \\(([^)]+)\\)', decision)\n\n            if obj_match and pos_match:\n                target_obj = obj_match.group(1)\n                pos_str = pos_match.group(1)\n                pos_parts = pos_str.split(', ')\n                if len(pos_parts) >= 2:\n                    x, y = float(pos_parts[0]), float(pos_parts[1])\n                    return f\"Go to position ({x}, {y}) and pick up the {target_obj}, then bring it to me\"\n\n        elif 'navigate_to' in decision:\n            # Extract target location\n            import re\n            loc_match = re.search(r'navigate_to: (.+)', decision)\n            if loc_match:\n                target_loc = loc_match.group(1)\n                return f\"Navigate to the {target_loc} and wait for further instructions\"\n\n        # Default: use original command with context\n        return f\"{original_command}. The current context is: {json.dumps(vision_data, indent=2)[:500]}...\"\n\n    def time_diff(self, time1, time2):\n        \"\"\"Calculate time difference in seconds\"\"\"\n        if not time2:\n            return float('inf')\n        return abs(time1.sec - time2.sec + (time1.nanosec - time2.nanosec) / 1e9)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = MultiModalFusionNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info(\"Multi-modal fusion node shutting down...\")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"vision-processing-integration",children:"Vision Processing Integration"}),"\n",(0,o.jsx)(n.p,{children:"Our vision system will provide:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Object detection and recognition"}),"\n",(0,o.jsx)(n.li,{children:"Spatial relationship analysis"}),"\n",(0,o.jsx)(n.li,{children:"Scene understanding"}),"\n",(0,o.jsx)(n.li,{children:"Visual attention mechanisms"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"adaptive-decision-making",children:"Adaptive Decision Making"}),"\n",(0,o.jsx)(n.p,{children:"The system will adapt its behavior based on:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Confidence levels from different modalities"}),"\n",(0,o.jsx)(n.li,{children:"Environmental conditions"}),"\n",(0,o.jsx)(n.li,{children:"Task requirements"}),"\n",(0,o.jsx)(n.li,{children:"Robot capabilities"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"demonstration-task",children:"Demonstration Task"}),"\n",(0,o.jsx)(n.p,{children:"Let's implement a simple task that combines all three modalities: voice, vision, and LLM planning."}),"\n",(0,o.jsx)(n.h3,{id:"task-fetch-and-deliver",children:"Task: Fetch and Deliver"}),"\n",(0,o.jsx)(n.p,{children:"The robot will:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:'Listen for voice command: "Please bring me the red cup"'}),"\n",(0,o.jsx)(n.li,{children:"Use vision to locate the red cup in the environment"}),"\n",(0,o.jsx)(n.li,{children:"Use LLM to plan the sequence of actions needed"}),"\n",(0,o.jsx)(n.li,{children:"Execute the plan to fetch and deliver the object"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"implementation",children:"Implementation"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class MultiModalDemoNode(Node):\n    def __init__(self):\n        super().__init__('multi_modal_demo_node')\n\n        # State machine for the demo\n        self.state = 'waiting_for_command'  # waiting_for_command, locating_object, planning, executing, completed\n\n        # Subscribers\n        self.decision_sub = self.create_subscription(\n            String,\n            'fused_decision',\n            self.decision_callback,\n            10\n        )\n\n        # Publishers\n        self.command_pub = self.create_publisher(\n            String,\n            'command_input',\n            10\n        )\n\n        # Timer for state management\n        self.demo_timer = self.create_timer(1.0, self.demo_callback)\n\n    def decision_callback(self, msg):\n        \"\"\"Handle fused decisions\"\"\"\n        decision = msg.data\n        self.get_logger().info(f\"Demo received decision: {decision}\")\n\n        if self.state == 'waiting_for_command':\n            if 'object_found' in decision:\n                self.state = 'planning'\n                # Trigger LLM planning\n                command_msg = String()\n                command_msg.data = f\"Pick up the object and bring it to the user\"\n                self.command_pub.publish(command_msg)\n            elif 'searching_for' in decision:\n                self.state = 'locating_object'\n                # Robot should navigate to search for the object\n                pass\n\n    def demo_callback(self):\n        \"\"\"Manage demo state\"\"\"\n        self.get_logger().info(f\"Demo state: {self.state}\")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = MultiModalDemoNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"integration-with-existing-systems",children:"Integration with Existing Systems"}),"\n",(0,o.jsx)(n.h3,{id:"voice-vision-coordination",children:"Voice-Vision Coordination"}),"\n",(0,o.jsx)(n.p,{children:"The system will:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Use voice commands to direct visual attention"}),"\n",(0,o.jsx)(n.li,{children:"Validate voice interpretations with visual data"}),"\n",(0,o.jsx)(n.li,{children:"Refine understanding through multi-modal feedback"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"vision-llm-coordination",children:"Vision-LLM Coordination"}),"\n",(0,o.jsx)(n.p,{children:"The system will:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Provide visual context to LLM for better planning"}),"\n",(0,o.jsx)(n.li,{children:"Use LLM reasoning to interpret complex visual scenes"}),"\n",(0,o.jsx)(n.li,{children:"Validate LLM plans against visual observations"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"voice-llm-coordination",children:"Voice-LLM Coordination"}),"\n",(0,o.jsx)(n.p,{children:"The system will:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Use voice commands as input to LLM planning"}),"\n",(0,o.jsx)(n.li,{children:"Generate natural language feedback from LLM"}),"\n",(0,o.jsx)(n.li,{children:"Handle ambiguous commands through clarification"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"challenges-and-solutions",children:"Challenges and Solutions"}),"\n",(0,o.jsx)(n.h3,{id:"data-synchronization",children:"Data Synchronization"}),"\n",(0,o.jsx)(n.p,{children:"Multi-modal systems face synchronization challenges:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Time delays"}),": Different modalities process at different speeds"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Fusion timing"}),": Determine optimal fusion points"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Data freshness"}),": Ensure data is recent enough to be relevant"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Timestamp-based data validation"}),"\n",(0,o.jsx)(n.li,{children:"Asynchronous processing with buffering"}),"\n",(0,o.jsx)(n.li,{children:"Adaptive fusion based on data availability"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"confidence-integration",children:"Confidence Integration"}),"\n",(0,o.jsx)(n.p,{children:"Different modalities have different confidence levels:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Weight decisions based on modality confidence"}),"\n",(0,o.jsx)(n.li,{children:"Handle low-confidence situations gracefully"}),"\n",(0,o.jsx)(n.li,{children:"Implement fallback strategies"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"computational-complexity",children:"Computational Complexity"}),"\n",(0,o.jsx)(n.p,{children:"Multi-modal processing is computationally intensive:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Optimize individual modalities for efficiency"}),"\n",(0,o.jsx)(n.li,{children:"Use hierarchical processing"}),"\n",(0,o.jsx)(n.li,{children:"Implement selective attention mechanisms"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"evaluation-metrics",children:"Evaluation Metrics"}),"\n",(0,o.jsx)(n.h3,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Task completion rate"}),": Percentage of tasks successfully completed"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Response time"}),": Time from command to action initiation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Accuracy"}),": Correctness of object identification and actions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robustness"}),": Performance under various environmental conditions"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"quality-metrics",children:"Quality Metrics"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Naturalness"}),": How natural the interaction feels"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Efficiency"}),": How efficiently tasks are completed"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Adaptability"}),": Ability to handle novel situations"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Reliability"}),": Consistency of performance"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsx)(n.p,{children:"In the final chapter of Module 4, we'll bring together all components into a complete autonomous humanoid system. We'll create a capstone project that demonstrates the full Vision-Language-Action (VLA) capabilities we've developed throughout this module."}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"In this chapter, we've learned how to integrate multiple sensory modalities in our humanoid robot system. We created a multi-modal fusion architecture that combines voice input, visual perception, and LLM reasoning to create intelligent robot behaviors. We implemented adaptive decision-making systems that can handle complex, real-world scenarios by leveraging the strengths of each modality. This multi-modal integration forms the core of our Vision-Language-Action (VLA) framework and enables our robot to perform sophisticated tasks that require understanding of language, vision, and physical interaction."})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);