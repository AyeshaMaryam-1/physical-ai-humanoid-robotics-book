"use strict";(globalThis.webpackChunkbook_temp=globalThis.webpackChunkbook_temp||[]).push([[737],{2152:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>o,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"module_3/3_2_isaac_ros_perception","title":"Isaac ROS Perception Pipeline","description":"Purpose","source":"@site/docs/03_module_3/3_2_isaac_ros_perception.mdx","sourceDirName":"03_module_3","slug":"/module_3/3_2_isaac_ros_perception","permalink":"/physical-ai-humanoid-robotics-book/docs/module_3/3_2_isaac_ros_perception","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/03_module_3/3_2_isaac_ros_perception.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Isaac ROS Perception Pipeline"},"sidebar":"tutorialSidebar","previous":{"title":"NVIDIA Isaac Sim Essentials","permalink":"/physical-ai-humanoid-robotics-book/docs/module_3/3_1_isaac_sim_essentials"},"next":{"title":"Navigation & Motion Planning (Nav2)","permalink":"/physical-ai-humanoid-robotics-book/docs/module_3/3_3_nav2"}}');var t=i(4848),s=i(8453);const o={sidebar_position:2,title:"Isaac ROS Perception Pipeline"},r="Isaac ROS Perception Pipeline",c={},l=[{value:"Purpose",id:"purpose",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Inputs",id:"inputs",level:2},{value:"Outputs",id:"outputs",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Chapter Outline",id:"chapter-outline",level:2},{value:"1. Isaac ROS Setup and Installation",id:"1-isaac-ros-setup-and-installation",level:3},{value:"2. Isaac Sim to ROS Bridge",id:"2-isaac-sim-to-ros-bridge",level:3},{value:"3. Isaac ROS Perception Nodes",id:"3-isaac-ros-perception-nodes",level:3},{value:"4. Perception Integration",id:"4-perception-integration",level:3},{value:"Hands-On Lab",id:"hands-on-lab",level:2},{value:"Setting up Isaac ROS Perception Pipeline",id:"setting-up-isaac-ros-perception-pipeline",level:3},{value:"Safety Notes",id:"safety-notes",level:2},{value:"Evaluation Criteria",id:"evaluation-criteria",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"isaac-ros-perception-pipeline",children:"Isaac ROS Perception Pipeline"})}),"\n",(0,t.jsx)(n.h2,{id:"purpose",children:"Purpose"}),"\n",(0,t.jsx)(n.p,{children:"This chapter builds on the Isaac Sim fundamentals to create a high-performance perception pipeline using Isaac ROS. We'll integrate Isaac Sim's photorealistic sensors with ROS 2 to create realistic perception data, implement object detection algorithms, and demonstrate how to process and visualize sensor data for humanoid robots in complex environments."}),"\n",(0,t.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,t.jsx)(n.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Configure Isaac Sim sensors to publish data to ROS 2 topics"}),"\n",(0,t.jsx)(n.li,{children:"Install and use Isaac ROS perception packages (object detection, depth estimation, etc.)"}),"\n",(0,t.jsx)(n.li,{children:"Implement GPU-accelerated perception pipelines for humanoid robots"}),"\n",(0,t.jsx)(n.li,{children:"Integrate perception results with robot control and navigation"}),"\n",(0,t.jsx)(n.li,{children:"Visualize perception outputs in RViz2 and Isaac Sim"}),"\n",(0,t.jsx)(n.li,{children:"Optimize perception pipelines for real-time performance"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Completed Module 1 (ROS 2 fundamentals and humanoid URDF)"}),"\n",(0,t.jsx)(n.li,{children:"Completed Module 2 (Gazebo & Unity digital twin)"}),"\n",(0,t.jsx)(n.li,{children:"Completed Module 3.1 (Isaac Sim essentials)"}),"\n",(0,t.jsx)(n.li,{children:"Understanding of computer vision concepts"}),"\n",(0,t.jsx)(n.li,{children:"Experience with ROS 2 message types (sensor_msgs, vision_msgs)"}),"\n",(0,t.jsx)(n.li,{children:"NVIDIA GPU with CUDA support"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"inputs",children:"Inputs"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Isaac Sim project with humanoid robot and sensors"}),"\n",(0,t.jsx)(n.li,{children:"Isaac ROS extensions installed"}),"\n",(0,t.jsx)(n.li,{children:"ROS 2 environment with perception tools"}),"\n",(0,t.jsx)(n.li,{children:"Sample objects/scene for perception testing"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"outputs",children:"Outputs"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Isaac Sim scene publishing sensor data to ROS 2"}),"\n",(0,t.jsx)(n.li,{children:"ROS 2 perception pipeline with Isaac ROS nodes"}),"\n",(0,t.jsx)(n.li,{children:"Object detection and depth estimation results"}),"\n",(0,t.jsx)(n.li,{children:"Visualization of perception outputs"}),"\n",(0,t.jsx)(n.li,{children:"Performance benchmarks for perception pipeline"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac ROS Extensions"}),": GPU-accelerated perception algorithms for ROS 2"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sensor Bridge"}),": Connecting Isaac Sim sensors to ROS 2 topics"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Object Detection"}),": Using Isaac ROS for real-time object detection"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Depth Estimation"}),": Processing depth camera data with Isaac ROS"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Perception Pipeline"}),": End-to-end processing from sensors to actionable data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Performance Optimization"}),": Leveraging GPU acceleration for real-time perception"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ROS 2 Integration"}),": Connecting perception results to robot control systems"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"chapter-outline",children:"Chapter Outline"}),"\n",(0,t.jsx)(n.p,{children:"This chapter builds Isaac ROS perception capabilities:"}),"\n",(0,t.jsx)(n.h3,{id:"1-isaac-ros-setup-and-installation",children:"1. Isaac ROS Setup and Installation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Installing Isaac ROS extensions and dependencies"}),"\n",(0,t.jsx)(n.li,{children:"Configuring GPU acceleration for perception tasks"}),"\n",(0,t.jsx)(n.li,{children:"Setting up development environment"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2-isaac-sim-to-ros-bridge",children:"2. Isaac Sim to ROS Bridge"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Configuring Isaac Sim sensors for ROS 2 publishing"}),"\n",(0,t.jsx)(n.li,{children:"Setting up camera, LiDAR, and IMU bridges"}),"\n",(0,t.jsx)(n.li,{children:"Optimizing data transfer and synchronization"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"3-isaac-ros-perception-nodes",children:"3. Isaac ROS Perception Nodes"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Object detection with Isaac ROS Detection NITROS"}),"\n",(0,t.jsx)(n.li,{children:"Depth estimation and stereo processing"}),"\n",(0,t.jsx)(n.li,{children:"Point cloud processing and segmentation"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"4-perception-integration",children:"4. Perception Integration"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Processing perception outputs for robot control"}),"\n",(0,t.jsx)(n.li,{children:"Fusing perception data with other sensors"}),"\n",(0,t.jsx)(n.li,{children:"Handling perception uncertainty and failures"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"hands-on-lab",children:"Hands-On Lab"}),"\n",(0,t.jsx)(n.h3,{id:"setting-up-isaac-ros-perception-pipeline",children:"Setting up Isaac ROS Perception Pipeline"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Isaac ROS Installation:"}),"\nIsaac ROS requires specific installation steps. In a real environment, you would:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Add NVIDIA package repository\nsudo apt update\nsudo apt install -y software-properties-common\nwget https://repo.download.nvidia.com/jetson-agx-xavier/jetson-agx-xavier-public.key -O - | sudo apt-key add -\n\n# Install Isaac ROS packages\nsudo apt update\nsudo apt install -y ros-humble-isaac-ros-perception\nsudo apt install -y ros-humble-isaac-ros-visual-slam\nsudo apt install -y ros-humble-isaac-ros-point-cloud-pipeline\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"ROS Package Creation:"}),"\nCreate a ROS 2 package for Isaac perception examples: ",(0,t.jsx)(n.code,{children:"robotics_ws/src/isaac_perception_examples/package.xml"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<?xml-model href="http://download.ros.org/schema/package_format3.xsd" schematypens="http://www.w3.org/2001/XMLSchema"?>\n<package format="3">\n  <name>isaac_perception_examples</name>\n  <version>0.0.0</version>\n  <description>Isaac ROS perception examples for the Physical AI & Humanoid Robotics book</description>\n  <maintainer email="humanoid-robotics-book@example.com">Humanoid Robotics Book</maintainer>\n  <license>Apache-2.0</license>\n\n  <depend>rclpy</depend>\n  <depend>std_msgs</depend>\n  <depend>sensor_msgs</depend>\n  <depend>vision_msgs</depend>\n  <depend>geometry_msgs</depend>\n  <depend>cv_bridge</depend>\n  <depend>message_filters</depend>\n  <exec_depend>python3-opencv</exec_depend>\n\n  <test_depend>ament_copyright</test_depend>\n  <test_depend>ament_flake8</test_depend>\n  <test_depend>ament_pep257</test_depend>\n  <test_depend>python3-pytest</test_depend>\n\n  <export>\n    <build_type>ament_python</build_type>\n  </export>\n</package>\n'})}),"\n",(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Setup File:"}),"\nCreate ",(0,t.jsx)(n.code,{children:"robotics_ws/src/isaac_perception_examples/setup.py"}),":"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import os\nfrom glob import glob\nfrom setuptools import setup\nfrom setuptools import find_packages\n\npackage_name = 'isaac_perception_examples'\n\nsetup(\n    name=package_name,\n    version='0.0.0',\n    packages=find_packages(exclude=['test']),\n    data_files=[\n        ('share/ament_index/resource_index/packages',\n            ['resource/' + package_name]),\n        ('share/' + package_name, ['package.xml']),\n    ],\n    install_requires=['setuptools'],\n    zip_safe=True,\n    maintainer='Humanoid Robotics Book',\n    maintainer_email='humanoid-robotics-book@example.com',\n    description='Isaac ROS perception examples for the Physical AI & Humanoid Robotics book',\n    license='Apache License 2.0',\n    tests_require=['pytest'],\n    entry_points={\n        'console_scripts': [\n            'object_detector = isaac_perception_examples.object_detector_node:main',\n            'depth_estimator = isaac_perception_examples.depth_estimator_node:main',\n        ],\n    },\n)\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"4",children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Object Detection Node:"}),"\nCreate the main object detection node: ",(0,t.jsx)(n.code,{children:"robotics_ws/src/isaac_perception_examples/isaac_perception_examples/object_detector_node.py"})]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\n"""\nIsaac ROS Object Detection Node\n\nThis node demonstrates object detection using Isaac ROS extensions.\nIn a real implementation, this would connect to Isaac Sim camera data\nand perform GPU-accelerated object detection.\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom vision_msgs.msg import Detection2DArray\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\n\nclass IsaacObjectDetectorNode(Node):\n    def __init__(self):\n        super().__init__(\'isaac_object_detector\')\n\n        # Create subscribers for camera data\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo,\n            \'/camera/camera_info\',\n            self.camera_info_callback,\n            10\n        )\n\n        # Create publisher for detection results\n        self.detection_pub = self.create_publisher(\n            Detection2DArray,\n            \'/isaac_ros/detections\',\n            10\n        )\n\n        # Initialize CV bridge\n        self.bridge = CvBridge()\n\n        # In a real Isaac ROS implementation, we would initialize\n        # Isaac\'s detection pipeline here\n        self.get_logger().info("Isaac Object Detection Node initialized")\n\n    def image_callback(self, msg):\n        """Process incoming camera image and perform object detection"""\n        try:\n            # Convert ROS Image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n\n            # In a real Isaac ROS implementation, we would use\n            # Isaac\'s GPU-accelerated detection pipeline here\n            # For this example, we\'ll simulate detection with OpenCV\n\n            # Simulate object detection (in real implementation, use Isaac ROS Detection)\n            detections = self.simulate_object_detection(cv_image)\n\n            # Create detection message\n            detection_msg = Detection2DArray()\n            detection_msg.header = msg.header\n            detection_msg.detections = detections\n\n            # Publish detection results\n            self.detection_pub.publish(detection_msg)\n\n            self.get_logger().info(f"Published {len(detections)} detections")\n\n        except Exception as e:\n            self.get_logger().error(f"Error processing image: {str(e)}")\n\n    def simulate_object_detection(self, image):\n        """Simulate object detection results (in real implementation, use Isaac ROS)"""\n        # This is a placeholder that simulates detection results\n        # In a real Isaac ROS implementation, we would use Isaac\'s\n        # GPU-accelerated detection pipeline\n        import random\n\n        detections = []\n\n        # Simulate detecting some objects (randomly for demonstration)\n        for i in range(random.randint(1, 3)):\n            detection = Detection2D()\n            detection.header.stamp = self.get_clock().now().to_msg()\n\n            # Create a random bounding box\n            bbox = BoundingBox2D()\n            bbox.center.x = random.randint(50, image.shape[1]-50)\n            bbox.center.y = random.randint(50, image.shape[0]-50)\n            bbox.size_x = random.randint(30, 100)\n            bbox.size_y = random.randint(30, 100)\n            detection.bbox = bbox\n\n            # Create a detection result\n            result = ObjectHypothesisWithPose()\n            result.hypothesis.class_id = "object"\n            result.hypothesis.score = random.uniform(0.7, 0.95)\n            detection.results.append(result)\n\n            detections.append(detection)\n\n        return detections\n\n    def camera_info_callback(self, msg):\n        """Process camera information"""\n        # Store camera parameters for later use\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\n        self.distortion_coeffs = np.array(msg.d)\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    detector_node = IsaacObjectDetectorNode()\n\n    try:\n        rclpy.spin(detector_node)\n    except KeyboardInterrupt:\n        detector_node.get_logger().info("Shutting down Isaac Object Detection Node")\n    finally:\n        detector_node.destroy_node()\n        rclpy.shutdown()\n\n\n# Additional imports needed for the detection message\nfrom vision_msgs.msg import Detection2D, BoundingBox2D, ObjectHypothesisWithPose\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsxs)(n.ol,{start:"5",children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Depth Estimation Node:"}),"\nCreate the depth estimation node: ",(0,t.jsx)(n.code,{children:"robotics_ws/src/isaac_perception_examples/isaac_perception_examples/depth_estimator_node.py"})]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\n"""\nIsaac ROS Depth Estimation Node\n\nThis node demonstrates depth estimation using Isaac ROS extensions.\nIn a real implementation, this would process stereo camera data\nor depth camera data from Isaac Sim.\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom sensor_msgs.msg import PointCloud2, PointField\nfrom std_msgs.msg import Header\nimport sensor_msgs_py.point_cloud2 as pc2\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nfrom collections import namedtuple\n\n\nclass IsaacDepthEstimatorNode(Node):\n    def __init__(self):\n        super().__init__(\'isaac_depth_estimator\')\n\n        # Create subscribers for depth camera data\n        self.depth_image_sub = self.create_subscription(\n            Image,\n            \'/depth_camera/depth/image_raw\',\n            self.depth_image_callback,\n            10\n        )\n\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo,\n            \'/depth_camera/camera_info\',\n            self.camera_info_callback,\n            10\n        )\n\n        # Create publisher for point cloud\n        self.pointcloud_pub = self.create_publisher(\n            PointCloud2,\n            \'/isaac_ros/pointcloud\',\n            10\n        )\n\n        # Initialize CV bridge\n        self.bridge = CvBridge()\n\n        # Store camera parameters\n        self.camera_matrix = None\n        self.camera_info = None\n\n        # In a real Isaac ROS implementation, we would initialize\n        # Isaac\'s depth estimation pipeline here\n        self.get_logger().info("Isaac Depth Estimation Node initialized")\n\n    def depth_image_callback(self, msg):\n        """Process incoming depth image and create point cloud"""\n        try:\n            # Convert ROS Image to OpenCV\n            depth_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'32FC1\')\n\n            # In a real Isaac ROS implementation, we would use\n            # Isaac\'s GPU-accelerated depth processing pipeline\n            # For this example, we\'ll convert depth image to point cloud\n\n            if self.camera_matrix is not None:\n                # Create point cloud from depth image\n                pointcloud_msg = self.create_pointcloud_from_depth(\n                    depth_image, self.camera_matrix, msg.header\n                )\n\n                # Publish point cloud\n                self.pointcloud_pub.publish(pointcloud_msg)\n\n                self.get_logger().info("Published point cloud")\n\n        except Exception as e:\n            self.get_logger().error(f"Error processing depth image: {str(e)}")\n\n    def create_pointcloud_from_depth(self, depth_image, camera_matrix, header):\n        """Create point cloud from depth image using camera matrix"""\n        height, width = depth_image.shape\n\n        # Create coordinate grids\n        u_coords, v_coords = np.meshgrid(np.arange(width), np.arange(height))\n\n        # Get camera parameters\n        fx = camera_matrix[0, 0]\n        fy = camera_matrix[1, 1]\n        cx = camera_matrix[0, 2]\n        cy = camera_matrix[1, 2]\n\n        # Calculate 3D coordinates\n        x = (u_coords - cx) * depth_image / fx\n        y = (v_coords - cy) * depth_image / fy\n        z = depth_image\n\n        # Flatten arrays and combine\n        points = np.column_stack((x.flatten(), y.flatten(), z.flatten()))\n\n        # Remove invalid points (where depth is 0 or NaN)\n        valid_mask = (z.flatten() > 0) & np.isfinite(points[:, 0]) & np.isfinite(points[:, 1]) & np.isfinite(points[:, 2])\n        points = points[valid_mask]\n\n        # Create PointCloud2 message\n        header = Header()\n        header.stamp = self.get_clock().now().to_msg()\n        header.frame_id = "depth_camera_frame"  # This should match your camera frame\n\n        # Define point fields\n        fields = [\n            PointField(name=\'x\', offset=0, datatype=PointField.FLOAT32, count=1),\n            PointField(name=\'y\', offset=4, datatype=PointField.FLOAT32, count=1),\n            PointField(name=\'z\', offset=8, datatype=PointField.FLOAT32, count=1)\n        ]\n\n        # Create point cloud\n        pointcloud_msg = pc2.create_cloud(header, fields, points)\n\n        return pointcloud_msg\n\n    def camera_info_callback(self, msg):\n        """Process camera information"""\n        # Store camera parameters for later use\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\n        self.camera_info = msg\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    depth_estimator = IsaacDepthEstimatorNode()\n\n    try:\n        rclpy.spin(depth_estimator)\n    except KeyboardInterrupt:\n        depth_estimator.get_logger().info("Shutting down Isaac Depth Estimation Node")\n    finally:\n        depth_estimator.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsxs)(n.ol,{start:"6",children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac Sim Configuration for ROS Bridge:"}),"\nCreate a configuration file to set up the ROS bridge in Isaac Sim: ",(0,t.jsx)(n.code,{children:"isaac_sim_assets/configs/ros_bridge_config.py"})]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'"""\nIsaac Sim ROS Bridge Configuration\n\nThis script configures Isaac Sim to publish sensor data to ROS 2 topics\ncompatible with Isaac ROS perception nodes.\n"""\n\nimport omni\nfrom pxr import Gf, Sdf, UsdGeom\nimport carb\n\n\ndef configure_ros_bridge():\n    """\n    Configure Isaac Sim to publish sensor data to ROS 2\n    """\n    carb.log_info("Configuring Isaac Sim ROS Bridge for perception")\n\n    # Enable ROS bridge extension\n    omni.kit.app.get_app().extension_manager.set_enabled("omni.isaac.ros2_bridge", True)\n\n    # Configure camera to publish to ROS topics\n    configure_camera_ros_publisher()\n\n    # Configure depth camera to publish to ROS topics\n    configure_depth_camera_ros_publisher()\n\n    # Configure LiDAR to publish to ROS topics\n    configure_lidar_ros_publisher()\n\n    carb.log_info("Isaac Sim ROS Bridge configured for perception pipeline")\n\n\ndef configure_camera_ros_publisher():\n    """\n    Configure RGB camera to publish to ROS topics\n    """\n    carb.log_info("Configuring RGB camera ROS publisher")\n\n    # In a real implementation, we would connect the camera to ROS topics\n    # like /camera/image_raw and /camera/camera_info\n    camera_config = {\n        "image_topic": "/camera/image_raw",\n        "camera_info_topic": "/camera/camera_info",\n        "frame_id": "camera_link",\n        "format": "rgb8"\n    }\n\n    carb.log_info(f"Camera configured with: {camera_config}")\n\n\ndef configure_depth_camera_ros_publisher():\n    """\n    Configure depth camera to publish to ROS topics\n    """\n    carb.log_info("Configuring depth camera ROS publisher")\n\n    # In a real implementation, we would connect the depth camera to ROS topics\n    # like /depth_camera/depth/image_raw and /depth_camera/camera_info\n    depth_camera_config = {\n        "depth_topic": "/depth_camera/depth/image_raw",\n        "image_topic": "/depth_camera/image_raw",\n        "camera_info_topic": "/depth_camera/camera_info",\n        "frame_id": "depth_camera_link",\n        "format": "32FC1"\n    }\n\n    carb.log_info(f"Depth camera configured with: {depth_camera_config}")\n\n\ndef configure_lidar_ros_publisher():\n    """\n    Configure LiDAR to publish to ROS topics\n    """\n    carb.log_info("Configuring LiDAR ROS publisher")\n\n    # In a real implementation, we would connect the LiDAR to ROS topics\n    # like /lidar/scan or /lidar/pointcloud\n    lidar_config = {\n        "scan_topic": "/lidar/scan",\n        "pointcloud_topic": "/lidar/pointcloud",\n        "frame_id": "lidar_link"\n    }\n\n    carb.log_info(f"LiDAR configured with: {lidar_config}")\n\n\ndef setup_perception_pipeline():\n    """\n    Set up the complete perception pipeline in Isaac Sim\n    """\n    carb.log_info("Setting up Isaac ROS perception pipeline")\n\n    # Configure ROS bridge\n    configure_ros_bridge()\n\n    # In a real implementation, we would also configure:\n    # - Isaac ROS Detection node\n    # - Isaac ROS Stereo Dense Reconstruction node\n    # - Isaac ROS Object Stereo Tracking node\n    # - Isaac ROS AprilTag Detection node\n\n    carb.log_info("Isaac ROS perception pipeline setup completed")\n\n\nif __name__ == "__main__":\n    setup_perception_pipeline()\n'})}),"\n",(0,t.jsxs)(n.ol,{start:"7",children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Launch File for Isaac ROS Perception:"}),"\nCreate a launch file to start the Isaac ROS perception pipeline: ",(0,t.jsx)(n.code,{children:"robotics_ws/src/isaac_perception_examples/launch/isaac_perception_pipeline.launch.py"})]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, RegisterEventHandler\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\nfrom launch.event_handlers import OnProcessStart\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\n\ndef generate_launch_description():\n    # Declare launch arguments\n    namespace_arg = DeclareLaunchArgument(\n        'namespace',\n        default_value='',\n        description='Namespace for all perception nodes'\n    )\n\n    # Isaac ROS Object Detection Node\n    object_detector_node = Node(\n        package='isaac_perception_examples',\n        executable='object_detector',\n        name='object_detector',\n        parameters=[\n            # Add any required parameters for Isaac ROS detection\n        ],\n        remappings=[\n            ('/camera/image_raw', '/camera/image_raw'),\n            ('/camera/camera_info', '/camera/camera_info'),\n            ('/isaac_ros/detections', '/isaac_ros/detections'),\n        ],\n        output='screen'\n    )\n\n    # Isaac ROS Depth Estimation Node\n    depth_estimator_node = Node(\n        package='isaac_perception_examples',\n        executable='depth_estimator',\n        name='depth_estimator',\n        parameters=[\n            # Add any required parameters for Isaac ROS depth estimation\n        ],\n        remappings=[\n            ('/depth_camera/depth/image_raw', '/depth_camera/depth/image_raw'),\n            ('/depth_camera/camera_info', '/depth_camera/camera_info'),\n            ('/isaac_ros/pointcloud', '/isaac_ros/pointcloud'),\n        ],\n        output='screen'\n    )\n\n    # Perception visualization node (RViz2)\n    rviz_config_dir = os.path.join(\n        get_package_share_directory('isaac_perception_examples'),\n        'rviz',\n        'perception_pipeline.rviz'\n    )\n\n    rviz_node = Node(\n        package='rviz2',\n        executable='rviz2',\n        name='rviz2',\n        arguments=['-d', rviz_config_dir],\n        output='screen'\n    )\n\n    return LaunchDescription([\n        namespace_arg,\n        object_detector_node,\n        depth_estimator_node,\n        rviz_node\n    ])\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"8",children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"RViz2 Configuration:"}),"\nCreate an RViz2 configuration file to visualize perception results: ",(0,t.jsx)(n.code,{children:"robotics_ws/src/isaac_perception_examples/rviz/perception_pipeline.rviz"})]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'Panels:\n  - Class: rviz_common/Displays\n    Help Height: 78\n    Name: Displays\n    Property Tree Widget:\n      Expanded:\n        - /Global Options1\n        - /Status1\n        - /Image1\n        - /PointCloud21\n        - /Detection2DArray1\n      Splitter Ratio: 0.5\n    Tree Height: 695\n  - Class: rviz_common/Selection\n    Name: Selection\n  - Class: rviz_common/Tool Properties\n    Expanded:\n      - /2D Goal Pose1\n      - /Publish Point1\n    Name: Tool Properties\n    Splitter Ratio: 0.5886790156364441\n  - Class: rviz_common/Views\n    Expanded:\n      - /Current View1\n    Name: Views\n    Splitter Ratio: 0.5\nVisualization Manager:\n  Class: ""\n  Displays:\n    - Class: rviz_default_plugins/TF\n      Enabled: true\n      Name: TF\n    - Class: rviz_default_plugins/Image\n      Enabled: true\n      Name: Image\n      Topic:\n        Depth: 5\n        Durability Policy: Volatile\n        History Policy: Keep Last\n        Reliability Policy: Reliable\n        Value: /camera/image_raw\n    - Class: rviz_default_plugins/PointCloud2\n      Enabled: true\n      Name: PointCloud2\n      Topic:\n        Depth: 5\n        Durability Policy: Volatile\n        History Policy: Keep Last\n        Reliability Policy: Reliable\n        Value: /isaac_ros/pointcloud\n    - Class: rviz_default_plugins/Detection2DArray\n      Enabled: true\n      Name: Detection2DArray\n      Topic:\n        Depth: 5\n        Durability Policy: Volatile\n        History Policy: Keep Last\n        Reliability Policy: Reliable\n        Value: /isaac_ros/detections\n  Enabled: true\n  Global Options:\n    Background Color: 48; 48; 48\n    Fixed Frame: base_link\n    Frame Rate: 30\n  Name: root\n  Tools:\n    - Class: rviz_default_plugins/Interact\n      Hide Inactive Objects: true\n    - Class: rviz_default_plugins/MoveCamera\n    - Class: rviz_default_plugins/Select\n    - Class: rviz_default_plugins/FocusCamera\n  Transformation:\n    Current:\n      Class: rviz_default_plugins/TF\n  Value: true\n  Views:\n    Current:\n      Class: rviz_default_plugins/Orbit\n      Name: Current View\n      Target Frame: <Fixed Frame>\n      Value: Orbit (rviz)\n    Saved: ~\nWindow Geometry:\n  Displays:\n    collapsed: false\n  Height: 915\n  Hide Left Dock: false\n  Hide Right Dock: false\n  Image:\n    collapsed: false\n  QMainWindow State: 000000ff00000000fd0000000400000000000001560000034bfc0200000008fb0000001200530065006c0065006300740069006f006e00000001e10000009b0000005c00fffffffb0000001e0054006f006f006c002000500072006f007000650072007400690065007302000001ed000001df00000185000000a3fb000000120056006900650077007300200054006f006f02000001df000002110000018500000122fb000000200054006f006f006c002000500072006f0070006500720074006900650073003203000002880000011d000002210000017afb000000100044006900730070006c006100790073010000003d0000029a000000c900fffffffb0000002000730065006c0065006300740069006f006e00200062007500660066006500720200000138000000aa0000023a00000294fb00000014005700690064006500530074006500720065006f02000000e6000000d2000003ee0000030bfb0000000c004b0069006e0065006300740200000186000001060000030c00000261000000010000010f0000034afc0200000003fb0000001e0054006f006f006c002000500072006f00700065007200740069006500730100000041000000780000000000000000fb0000000a00560069006500770073010000003d0000034a000000a400fffffffb0000001200530065006c0065006300740069006f006e010000025a000000b200000000000000000000000200000490000000a9fc0100000001fb0000000a00560069006500770073030000004e00000080000002e10000019700000003000004420000003efc0100000002fb0000000800540069006d00650100000000000004420000000000000000fb0000000800540069006d00650100000000000004500000000000000000000004ba0000034b00000004000000040000000800000008fc0000000100000002000000010000000a0054006f006f006c00730100000000ffffffff0000000000000000\n  Selection:\n    collapsed: false\n  Tool Properties:\n    collapsed: false\n  Views:\n    collapsed: false\n  Width: 1853\n  X: 67\n  Y: 27\n'})}),"\n",(0,t.jsxs)(n.ol,{start:"9",children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Testing the Perception Pipeline:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Launch Isaac Sim with the humanoid robot and configured sensors"}),"\n",(0,t.jsx)(n.li,{children:"Start the ROS bridge to publish sensor data"}),"\n",(0,t.jsx)(n.li,{children:"Launch the Isaac ROS perception nodes:"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"ros2 launch isaac_perception_examples isaac_perception_pipeline.launch.py\n"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Visualize the results in RViz2"}),"\n",(0,t.jsx)(n.li,{children:"Verify that object detection and depth estimation are working"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"safety-notes",children:"Safety Notes"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Ensure GPU resources are properly managed to avoid overheating"}),"\n",(0,t.jsx)(n.li,{children:"Validate perception results before using them for robot control"}),"\n",(0,t.jsx)(n.li,{children:"Implement fallback mechanisms when perception fails"}),"\n",(0,t.jsx)(n.li,{children:"Test perception pipeline under various lighting conditions"}),"\n",(0,t.jsx)(n.li,{children:"Monitor computational load to maintain real-time performance"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"evaluation-criteria",children:"Evaluation Criteria"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Can Isaac Sim publish camera data to ROS 2 topics correctly?"}),"\n",(0,t.jsx)(n.li,{children:"Are Isaac ROS perception nodes processing data successfully?"}),"\n",(0,t.jsx)(n.li,{children:"Can object detection results be visualized in RViz2?"}),"\n",(0,t.jsx)(n.li,{children:"Is the depth estimation pipeline producing accurate point clouds?"}),"\n",(0,t.jsx)(n.li,{children:"Does the perception pipeline run in real-time with acceptable latency?"}),"\n",(0,t.jsx)(n.li,{children:"Are the perception results accurate and reliable for robot control?"}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>r});var a=i(6540);const t={},s=a.createContext(t);function o(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);