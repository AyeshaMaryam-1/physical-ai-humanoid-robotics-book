"use strict";(globalThis.webpackChunkbook_temp=globalThis.webpackChunkbook_temp||[]).push([[432],{8379:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"module_4/4_1_whisper_voice","title":"Chapter 4.1: Voice Input with Whisper","description":"Implementing speech recognition for humanoid robots using OpenAI\'s Whisper model","source":"@site/docs/04_module_4/4_1_whisper_voice.mdx","sourceDirName":"04_module_4","slug":"/module_4/4_1_whisper_voice","permalink":"/physical-ai-humanoid-robotics-book/docs/module_4/4_1_whisper_voice","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/04_module_4/4_1_whisper_voice.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Chapter 4.1: Voice Input with Whisper","description":"Implementing speech recognition for humanoid robots using OpenAI\'s Whisper model"},"sidebar":"tutorialSidebar","previous":{"title":"Sim-to-Real Transfer","permalink":"/physical-ai-humanoid-robotics-book/docs/module_3/3_4_sim_to_real"},"next":{"title":"Chapter 4.2: LLM Cognitive Planning","permalink":"/physical-ai-humanoid-robotics-book/docs/module_4/4_2_llm_planning"}}');var r=i(4848),t=i(8453);const o={sidebar_position:1,title:"Chapter 4.1: Voice Input with Whisper",description:"Implementing speech recognition for humanoid robots using OpenAI's Whisper model"},a="Chapter 4.1: Voice Input with Whisper",l={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Overview of Speech Recognition in Robotics",id:"overview-of-speech-recognition-in-robotics",level:2},{value:"Why Whisper?",id:"why-whisper",level:3},{value:"Whisper Architecture and Capabilities",id:"whisper-architecture-and-capabilities",level:2},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:2},{value:"Audio Data Format",id:"audio-data-format",level:3},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"Step 1: Setting Up Dependencies",id:"step-1-setting-up-dependencies",level:3},{value:"Step 2: Creating the Whisper Node",id:"step-2-creating-the-whisper-node",level:3},{value:"Step 3: Audio Data Handling",id:"step-3-audio-data-handling",level:3},{value:"Step 4: Real-time Processing Considerations",id:"step-4-real-time-processing-considerations",level:3},{value:"Practical Implementation",id:"practical-implementation",level:2},{value:"Audio Subscription",id:"audio-subscription",level:3},{value:"Audio Processing Pipeline",id:"audio-processing-pipeline",level:3},{value:"Model Optimization",id:"model-optimization",level:3},{value:"Challenges and Solutions",id:"challenges-and-solutions",level:2},{value:"Real-time Processing",id:"real-time-processing",level:3},{value:"Noise Reduction",id:"noise-reduction",level:3},{value:"Language Adaptation",id:"language-adaptation",level:3},{value:"Hardware Constraints",id:"hardware-constraints",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"Performance Metrics",id:"performance-metrics",level:3},{value:"Integration with Robot Systems",id:"integration-with-robot-systems",level:2},{value:"Audio Input Pipeline",id:"audio-input-pipeline",level:3},{value:"ROS 2 Message Types",id:"ros-2-message-types",level:3},{value:"Deployment Considerations",id:"deployment-considerations",level:2},{value:"Edge Deployment",id:"edge-deployment",level:3},{value:"Configuration Management",id:"configuration-management",level:3},{value:"Next Steps",id:"next-steps",level:2},{value:"Summary",id:"summary",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"chapter-41-voice-input-with-whisper",children:"Chapter 4.1: Voice Input with Whisper"})}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsx)(n.p,{children:"In this chapter, we'll explore how to enable humanoid robots to understand human speech using OpenAI's Whisper model. Whisper is a state-of-the-art speech recognition model that can transcribe speech to text with high accuracy across multiple languages. Integrating voice input capabilities into our humanoid robot will allow for more natural human-robot interaction."}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this chapter, you will:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Understand the fundamentals of speech recognition and Whisper"}),"\n",(0,r.jsx)(n.li,{children:"Learn how to integrate Whisper with ROS 2 for real-time voice input"}),"\n",(0,r.jsx)(n.li,{children:"Implement a voice input system for your humanoid robot"}),"\n",(0,r.jsx)(n.li,{children:"Create a ROS 2 node that processes audio and publishes transcribed text"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"overview-of-speech-recognition-in-robotics",children:"Overview of Speech Recognition in Robotics"}),"\n",(0,r.jsx)(n.p,{children:"Speech recognition is a critical component for natural human-robot interaction. It enables robots to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Understand verbal commands"}),"\n",(0,r.jsx)(n.li,{children:"Engage in conversational interfaces"}),"\n",(0,r.jsx)(n.li,{children:"Respond to voice-activated triggers"}),"\n",(0,r.jsx)(n.li,{children:"Process natural language instructions"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"why-whisper",children:"Why Whisper?"}),"\n",(0,r.jsx)(n.p,{children:"OpenAI's Whisper model offers several advantages for robotics applications:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"High accuracy across multiple languages"}),"\n",(0,r.jsx)(n.li,{children:"Robustness to accents and background noise"}),"\n",(0,r.jsx)(n.li,{children:"Open-source implementation"}),"\n",(0,r.jsx)(n.li,{children:"Pre-trained on diverse audio data"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"whisper-architecture-and-capabilities",children:"Whisper Architecture and Capabilities"}),"\n",(0,r.jsx)(n.p,{children:"Whisper is a transformer-based model that can handle various speech recognition tasks:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Automatic speech recognition (ASR)"}),"\n",(0,r.jsx)(n.li,{children:"Language identification"}),"\n",(0,r.jsx)(n.li,{children:"Speech translation"}),"\n",(0,r.jsx)(n.li,{children:"Voice activity detection"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The model is available in multiple sizes, from tiny (39M parameters) to large (1550M parameters), allowing for trade-offs between accuracy and computational requirements."}),"\n",(0,r.jsx)(n.h2,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,r.jsx)(n.p,{children:"To integrate Whisper with ROS 2, we'll create a custom node that:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Subscribes to audio data from the robot's microphones"}),"\n",(0,r.jsx)(n.li,{children:"Processes the audio through the Whisper model"}),"\n",(0,r.jsx)(n.li,{children:"Publishes the transcribed text as ROS 2 messages"}),"\n",(0,r.jsx)(n.li,{children:"Handles real-time processing constraints"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"audio-data-format",children:"Audio Data Format"}),"\n",(0,r.jsx)(n.p,{children:"For optimal Whisper performance, we'll use the following audio format:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Sample rate: 16 kHz (Whisper's native rate)"}),"\n",(0,r.jsx)(n.li,{children:"Channels: Mono"}),"\n",(0,r.jsx)(n.li,{children:"Bit depth: 16-bit PCM"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,r.jsx)(n.h3,{id:"step-1-setting-up-dependencies",children:"Step 1: Setting Up Dependencies"}),"\n",(0,r.jsx)(n.p,{children:"First, we need to install the required dependencies for Whisper in our ROS 2 workspace:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Install Whisper and audio processing libraries\npip install openai-whisper\npip install sounddevice  # For audio input\npip install pydub        # For audio processing\npip install numpy       # For audio manipulation\npip install torch       # For model inference\n\n# For ROS 2 audio messages (if using standard audio_msgs)\nsudo apt-get install ros-humble-audio-common\n"})}),"\n",(0,r.jsx)(n.h3,{id:"step-2-creating-the-whisper-node",children:"Step 2: Creating the Whisper Node"}),"\n",(0,r.jsx)(n.p,{children:"We'll create a ROS 2 node that handles audio input and transcription. The node will:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Subscribe to audio data from the robot's microphones"}),"\n",(0,r.jsx)(n.li,{children:"Buffer audio chunks for processing"}),"\n",(0,r.jsx)(n.li,{children:"Transcribe audio using Whisper"}),"\n",(0,r.jsx)(n.li,{children:"Publish transcribed text to a ROS 2 topic"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"step-3-audio-data-handling",children:"Step 3: Audio Data Handling"}),"\n",(0,r.jsx)(n.p,{children:"For optimal Whisper performance, we need to handle audio data properly:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sample Rate"}),": Convert input to 16 kHz (Whisper's native rate)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Format"}),": Mono, 16-bit PCM"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Chunking"}),": Process audio in appropriate time segments"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Buffering"}),": Maintain audio buffer for real-time processing"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"step-4-real-time-processing-considerations",children:"Step 4: Real-time Processing Considerations"}),"\n",(0,r.jsx)(n.p,{children:"For real-time voice input, we need to consider:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Audio buffering strategies (sliding window approach)"}),"\n",(0,r.jsx)(n.li,{children:"Processing latency requirements (target under 200ms)"}),"\n",(0,r.jsx)(n.li,{children:"Computational resource constraints (GPU acceleration)"}),"\n",(0,r.jsx)(n.li,{children:"Accuracy vs. speed trade-offs (model size selection)"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"practical-implementation",children:"Practical Implementation"}),"\n",(0,r.jsx)(n.p,{children:"Let's implement the Whisper transcription node in our ROS 2 workspace."}),"\n",(0,r.jsx)(n.h3,{id:"audio-subscription",children:"Audio Subscription"}),"\n",(0,r.jsx)(n.p,{children:"The node will subscribe to audio data from the robot's microphone system. We'll use a custom message type or standard audio messages depending on our system architecture. Here's the basic structure of our Whisper node:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nimport whisper\nimport numpy as np\nimport torch\nfrom audio_common_msgs.msg import AudioData\nfrom std_msgs.msg import String\nimport io\nimport wave\n\nclass WhisperNode(Node):\n    def __init__(self):\n        super().__init__(\'whisper_node\')\n\n        # Load Whisper model (using smaller model for real-time performance)\n        self.get_logger().info("Loading Whisper model...")\n        self.model = whisper.load_model("base")  # or "tiny" for faster inference\n\n        # Audio buffer for accumulating audio chunks\n        self.audio_buffer = np.array([], dtype=np.float32)\n\n        # Parameters\n        self.buffer_duration = self.declare_parameter(\'buffer_duration\', 2.0).value  # seconds\n        self.sample_rate = self.declare_parameter(\'sample_rate\', 16000).value\n        self.min_audio_length = self.declare_parameter(\'min_audio_length\', 0.5).value  # seconds\n\n        # Publishers and subscribers\n        self.audio_sub = self.create_subscription(\n            AudioData,\n            \'audio_input\',\n            self.audio_callback,\n            10\n        )\n\n        self.text_pub = self.create_publisher(\n            String,\n            \'transcribed_text\',\n            10\n        )\n\n        self.get_logger().info("Whisper node initialized")\n\n    def audio_callback(self, msg):\n        """Process incoming audio data"""\n        # Convert audio data to numpy array\n        audio_data = np.frombuffer(msg.data, dtype=np.int16).astype(np.float32) / 32768.0\n\n        # Add to buffer\n        self.audio_buffer = np.concatenate([self.audio_buffer, audio_data])\n\n        # Check if buffer has enough data for transcription\n        required_samples = int(self.buffer_duration * self.sample_rate)\n        if len(self.audio_buffer) >= required_samples:\n            self.transcribe_audio()\n\n    def transcribe_audio(self):\n        """Transcribe buffered audio using Whisper"""\n        if len(self.audio_buffer) < self.min_audio_length * self.sample_rate:\n            # Not enough audio data, return early\n            return\n\n        # Ensure audio is in the right format\n        audio = self.audio_buffer.copy()\n\n        # Clear buffer for next round\n        self.audio_buffer = np.array([], dtype=np.float32)\n\n        try:\n            # Transcribe using Whisper\n            result = self.model.transcribe(audio, fp16=False)  # fp16 might cause issues on some systems\n            text = result["text"].strip()\n\n            if text:  # Only publish if there\'s text\n                msg = String()\n                msg.data = text\n                self.text_pub.publish(msg)\n                self.get_logger().info(f"Transcribed: {text}")\n\n        except Exception as e:\n            self.get_logger().error(f"Transcription error: {e}")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = WhisperNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"audio-processing-pipeline",children:"Audio Processing Pipeline"}),"\n",(0,r.jsx)(n.p,{children:"The transcription pipeline will:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Receive audio chunks from the microphone"}),"\n",(0,r.jsx)(n.li,{children:"Convert audio to Whisper's required format (16kHz, mono, float32)"}),"\n",(0,r.jsx)(n.li,{children:"Buffer audio for optimal transcription length"}),"\n",(0,r.jsx)(n.li,{children:"Apply Whisper transcription"}),"\n",(0,r.jsx)(n.li,{children:"Publish results with timestamps and confidence scores"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"model-optimization",children:"Model Optimization"}),"\n",(0,r.jsx)(n.p,{children:"For real-time performance on robotics hardware:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'Use smaller Whisper models ("tiny" or "base")'}),"\n",(0,r.jsx)(n.li,{children:"Consider quantization for faster inference"}),"\n",(0,r.jsx)(n.li,{children:"Implement GPU acceleration if available"}),"\n",(0,r.jsx)(n.li,{children:"Optimize batch processing when possible"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"challenges-and-solutions",children:"Challenges and Solutions"}),"\n",(0,r.jsx)(n.h3,{id:"real-time-processing",children:"Real-time Processing"}),"\n",(0,r.jsx)(n.p,{children:"Whisper models can be computationally intensive. To handle real-time processing:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'Use smaller Whisper models ("tiny" or "base") for faster inference'}),"\n",(0,r.jsx)(n.li,{children:"Implement audio buffering to process longer segments"}),"\n",(0,r.jsx)(n.li,{children:"Consider edge computing solutions like NVIDIA Jetson"}),"\n",(0,r.jsx)(n.li,{children:"Use GPU acceleration when available (CUDA support)"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"noise-reduction",children:"Noise Reduction"}),"\n",(0,r.jsx)(n.p,{children:"Robot environments often have background noise. We'll implement:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Audio preprocessing filters (high-pass, low-pass)"}),"\n",(0,r.jsx)(n.li,{children:"Voice activity detection (VAD) to ignore silence"}),"\n",(0,r.jsx)(n.li,{children:"Noise suppression algorithms (like RNNoise)"}),"\n",(0,r.jsx)(n.li,{children:"Adaptive thresholding for speech detection"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"language-adaptation",children:"Language Adaptation"}),"\n",(0,r.jsx)(n.p,{children:"Whisper supports multiple languages. For robotics applications:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Identify the primary language(s) of interaction"}),"\n",(0,r.jsx)(n.li,{children:"Fine-tune models if domain-specific vocabulary is needed"}),"\n",(0,r.jsx)(n.li,{children:"Implement language detection for multilingual scenarios"}),"\n",(0,r.jsx)(n.li,{children:"Use language-specific punctuation and formatting"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"hardware-constraints",children:"Hardware Constraints"}),"\n",(0,r.jsx)(n.p,{children:"Robots have limited computational resources:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Optimize model size vs. accuracy trade-offs"}),"\n",(0,r.jsx)(n.li,{children:"Consider using ONNX Runtime for faster inference"}),"\n",(0,r.jsx)(n.li,{children:"Implement model quantization (int8) for reduced memory usage"}),"\n",(0,r.jsx)(n.li,{children:"Use streaming inference to reduce latency"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,r.jsx)(n.p,{children:"We'll test our Whisper integration with:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Various audio input scenarios (quiet, noisy, reverberant)"}),"\n",(0,r.jsx)(n.li,{children:"Different speakers and accents"}),"\n",(0,r.jsx)(n.li,{children:"Background noise conditions (fans, motors, conversations)"}),"\n",(0,r.jsx)(n.li,{children:"Real-time performance requirements (latency, throughput)"}),"\n",(0,r.jsx)(n.li,{children:"Edge cases (silence, overlapping speech, foreign languages)"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,r.jsx)(n.p,{children:"Key metrics for evaluation:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Word Error Rate (WER)"}),": Accuracy of transcription"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Latency"}),": Time from audio input to text output"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Throughput"}),": Number of audio samples processed per second"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Robustness"}),": Performance under various noise conditions"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"integration-with-robot-systems",children:"Integration with Robot Systems"}),"\n",(0,r.jsx)(n.h3,{id:"audio-input-pipeline",children:"Audio Input Pipeline"}),"\n",(0,r.jsx)(n.p,{children:"For the humanoid robot, we'll integrate with:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Microphone array for spatial audio processing"}),"\n",(0,r.jsx)(n.li,{children:"Audio preprocessing for noise reduction"}),"\n",(0,r.jsx)(n.li,{children:"Voice activity detection to minimize processing"}),"\n",(0,r.jsx)(n.li,{children:"Automatic gain control for consistent audio levels"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"ros-2-message-types",children:"ROS 2 Message Types"}),"\n",(0,r.jsx)(n.p,{children:"We'll use standard ROS 2 message types:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"audio_common_msgs/AudioData"})," for raw audio input"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"std_msgs/String"})," for transcribed text"]}),"\n",(0,r.jsx)(n.li,{children:"Custom messages for confidence scores and metadata"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"deployment-considerations",children:"Deployment Considerations"}),"\n",(0,r.jsx)(n.h3,{id:"edge-deployment",children:"Edge Deployment"}),"\n",(0,r.jsx)(n.p,{children:"For deployment on robot hardware:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Model optimization for target hardware"}),"\n",(0,r.jsx)(n.li,{children:"Memory usage optimization"}),"\n",(0,r.jsx)(n.li,{children:"Power consumption considerations"}),"\n",(0,r.jsx)(n.li,{children:"Thermal management for sustained operation"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"configuration-management",children:"Configuration Management"}),"\n",(0,r.jsx)(n.p,{children:"Parameters that can be tuned:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Model size selection"}),"\n",(0,r.jsx)(n.li,{children:"Audio buffer duration"}),"\n",(0,r.jsx)(n.li,{children:"Minimum audio length for processing"}),"\n",(0,r.jsx)(n.li,{children:"Confidence thresholds"}),"\n",(0,r.jsx)(n.li,{children:"Language selection"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsx)(n.p,{children:"In the next chapter, we'll integrate the transcribed text with our LLM planning system to enable the robot to understand and execute voice commands. This will form the foundation of our Vision-Language-Action (VLA) system by connecting voice input to cognitive planning capabilities."}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"In this chapter, we've learned how to implement voice input capabilities for our humanoid robot using OpenAI's Whisper model. We covered the complete pipeline from audio input to text transcription, including real-time processing considerations, hardware constraints, and integration with ROS 2. By integrating Whisper with our robot system, we enable natural language interaction that serves as a crucial component of our Vision-Language-Action (VLA) system. This voice input capability will be essential for creating more intuitive and natural human-robot interactions in our autonomous humanoid system."})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var s=i(6540);const r={},t=s.createContext(r);function o(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);