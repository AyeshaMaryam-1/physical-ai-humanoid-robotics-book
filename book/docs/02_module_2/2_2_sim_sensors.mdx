---
sidebar_position: 2
title: "Sensors in Simulation"
---

# Sensors in Simulation

## Purpose
This chapter focuses on integrating realistic sensors into our Gazebo simulation environment. We'll add simulated cameras, LiDAR, IMUs, and other sensors to our humanoid robot, enabling perception capabilities in simulation that mirror real-world sensor systems.

## Learning Outcomes
After completing this chapter, you will be able to:
- Add realistic sensor models to URDF for simulation
- Configure simulated cameras, LiDAR, and IMU sensors in Gazebo
- Process and interpret simulated sensor data in ROS 2
- Implement sensor fusion techniques for improved perception
- Validate sensor data quality and accuracy in simulation
- Integrate sensor data with robot state estimation systems

## Prerequisites
- Completed Module 1 (ROS 2 fundamentals and humanoid URDF)
- Completed Chapter 2.1 (Gazebo simulation fundamentals)
- Understanding of sensor types and their applications in robotics
- Knowledge of ROS 2 sensor message types and processing

## Inputs
- Working humanoid robot model with Gazebo integration
- Understanding of Gazebo sensor plugins
- Knowledge of ROS 2 sensor message types (sensor_msgs)
- Completed simulation environment from Chapter 2.1

## Outputs
- Humanoid robot model enhanced with simulated sensors
- Working sensor data streams from simulation
- Sensor processing nodes for data interpretation
- Integration with robot state estimation systems

## Key Concepts
- **Sensor Simulation**: Accurate modeling of real-world sensors in virtual environments
- **Gazebo Sensor Plugins**: Specialized plugins for different sensor types (camera, LiDAR, IMU, etc.)
- **Sensor Noise Models**: Realistic noise characteristics to match real sensor behavior
- **Sensor Data Processing**: ROS 2 nodes for interpreting and utilizing sensor data
- **Point Clouds**: 3D spatial data representation from depth sensors and LiDAR
- **Sensor Fusion**: Combining data from multiple sensors for improved perception
- **Calibration**: Ensuring simulated sensors match real-world characteristics

## Chapter Outline
This chapter builds sensor capabilities into our simulation:

### 1. Camera Integration
- RGB camera simulation with realistic parameters
- Depth camera simulation for 3D perception
- Stereo vision setup for depth estimation

### 2. Range Sensors
- LiDAR simulation with configurable parameters
- Sonar and infrared sensor simulation
- Multi-beam range sensor arrays

### 3. Inertial Sensors
- IMU simulation for orientation and acceleration
- Gyroscope and accelerometer modeling
- Sensor fusion for state estimation

### 4. Sensor Processing
- Real-time sensor data processing
- Noise filtering and validation
- Integration with perception systems

## Hands-On Lab
### Adding Sensors to Humanoid Robot in Gazebo

1. Update the URDF file to include simulated sensors in `humanoid_description/urdf/simple_humanoid.urdf`:

```xml
<?xml version="1.0"?>
<robot name="simple_humanoid_with_sensors">
  <!-- Base link -->
  <link name="base_link">
    <visual>
      <geometry>
        <box size="0.3 0.2 0.1"/>
      </geometry>
      <material name="blue">
        <color rgba="0 0 1 0.8"/>
      </material>
    </visual>
    <collision>
      <geometry>
        <box size="0.3 0.2 0.1"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="5.0"/>
      <inertia ixx="0.1" ixy="0.0" ixz="0.0" iyy="0.1" iyz="0.0" izz="0.1"/>
    </inertial>
  </link>

  <!-- Torso -->
  <link name="torso">
    <visual>
      <geometry>
        <box size="0.2 0.15 0.4"/>
      </geometry>
      <material name="gray">
        <color rgba="0.5 0.5 0.5 0.8"/>
      </material>
    </visual>
    <collision>
      <geometry>
        <box size="0.2 0.15 0.4"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="3.0"/>
      <inertia ixx="0.05" ixy="0.0" ixz="0.0" iyy="0.05" iyz="0.0" izz="0.05"/>
    </inertial>
  </link>

  <joint name="base_to_torso" type="fixed">
    <parent link="base_link"/>
    <child link="torso"/>
    <origin xyz="0 0 0.25"/>
  </joint>

  <!-- Head with sensors -->
  <link name="head">
    <visual>
      <geometry>
        <sphere radius="0.1"/>
      </geometry>
      <material name="white">
        <color rgba="1 1 1 0.8"/>
      </material>
    </visual>
    <collision>
      <geometry>
        <sphere radius="0.1"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="1.0"/>
      <inertia ixx="0.01" ixy="0.0" ixz="0.0" iyy="0.01" iyz="0.0" izz="0.01"/>
    </inertial>
  </link>

  <joint name="torso_to_head" type="fixed">
    <parent link="torso"/>
    <child link="head"/>
    <origin xyz="0 0 0.3"/>
  </joint>

  <!-- RGB Camera on head -->
  <link name="camera_link">
    <visual>
      <geometry>
        <box size="0.05 0.05 0.05"/>
      </geometry>
      <material name="black">
        <color rgba="0 0 0 0.8"/>
      </material>
    </visual>
    <collision>
      <geometry>
        <box size="0.05 0.05 0.05"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="0.1"/>
      <inertia ixx="0.001" ixy="0.0" ixz="0.0" iyy="0.001" iyz="0.0" izz="0.001"/>
    </inertial>
  </link>

  <joint name="head_to_camera" type="fixed">
    <parent link="head"/>
    <child link="camera_link"/>
    <origin xyz="0.05 0 0" rpy="0 0 0"/>
  </joint>

  <!-- Gazebo plugin for RGB camera -->
  <gazebo reference="camera_link">
    <sensor name="camera" type="camera">
      <always_on>true</always_on>
      <visualize>true</visualize>
      <camera>
        <horizontal_fov>1.047</horizontal_fov> <!-- 60 degrees -->
        <image>
          <width>640</width>
          <height>480</height>
          <format>R8G8B8</format>
        </image>
        <clip>
          <near>0.1</near>
          <far>30.0</far>
        </clip>
      </camera>
      <plugin name="camera_controller" filename="libgazebo_ros_camera.so">
        <frame_name>camera_link</frame_name>
        <topic_name>camera/image_raw</topic_name>
        <hack_baseline>0.07</hack_baseline>
      </plugin>
    </sensor>
  </gazebo>

  <!-- Depth Camera on head -->
  <link name="depth_camera_link">
    <visual>
      <geometry>
        <box size="0.05 0.05 0.05"/>
      </geometry>
      <material name="red">
        <color rgba="1 0 0 0.8"/>
      </material>
    </visual>
    <collision>
      <geometry>
        <box size="0.05 0.05 0.05"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="0.1"/>
      <inertia ixx="0.001" ixy="0.0" ixz="0.0" iyy="0.001" iyz="0.0" izz="0.001"/>
    </inertial>
  </link>

  <joint name="head_to_depth_camera" type="fixed">
    <parent link="head"/>
    <child link="depth_camera_link"/>
    <origin xyz="0.05 0.05 0" rpy="0 0 0"/>
  </joint>

  <!-- Gazebo plugin for depth camera -->
  <gazebo reference="depth_camera_link">
    <sensor name="depth_camera" type="depth">
      <always_on>true</always_on>
      <visualize>true</visualize>
      <camera>
        <horizontal_fov>1.047</horizontal_fov> <!-- 60 degrees -->
        <image>
          <width>640</width>
          <height>480</height>
          <format>R8G8B8</format>
        </image>
        <clip>
          <near>0.1</near>
          <far>10.0</far>
        </clip>
        <noise>
          <type>gaussian</type>
          <mean>0.0</mean>
          <stddev>0.007</stddev>
        </noise>
      </camera>
      <plugin name="depth_camera_controller" filename="libgazebo_ros_depth_camera.so">
        <frame_name>depth_camera_link</frame_name>
        <rgb_topic_name>depth_camera/image_raw</rgb_topic_name>
        <depth_image_topic_name>depth_camera/depth/image_raw</depth_image_topic_name>
        <point_cloud_topic_name>depth_camera/points</point_cloud_topic_name>
        <camera_info_topic_name>depth_camera/camera_info</camera_info_topic_name>
      </plugin>
    </sensor>
  </gazebo>

  <!-- 3D LiDAR on head -->
  <link name="lidar_link">
    <visual>
      <geometry>
        <cylinder radius="0.03" length="0.05"/>
      </geometry>
      <material name="green">
        <color rgba="0 1 0 0.8"/>
      </material>
    </visual>
    <collision>
      <geometry>
        <cylinder radius="0.03" length="0.05"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="0.2"/>
      <inertia ixx="0.001" ixy="0.0" ixz="0.0" iyy="0.001" iyz="0.0" izz="0.002"/>
    </inertial>
  </link>

  <joint name="head_to_lidar" type="fixed">
    <parent link="head"/>
    <child link="lidar_link"/>
    <origin xyz="0.05 -0.05 0.025" rpy="0 0 0"/>
  </joint>

  <!-- Gazebo plugin for 3D LiDAR -->
  <gazebo reference="lidar_link">
    <sensor name="lidar" type="ray">
      <always_on>true</always_on>
      <visualize>true</visualize>
      <ray>
        <scan>
          <horizontal>
            <samples>640</samples>
            <resolution>1</resolution>
            <min_angle>-1.570796</min_angle> <!-- -90 degrees -->
            <max_angle>1.570796</max_angle>   <!-- 90 degrees -->
          </horizontal>
          <vertical>
            <samples>16</samples>
            <resolution>1</resolution>
            <min_angle>-0.174533</min_angle> <!-- -10 degrees -->
            <max_angle>0.174533</max_angle>   <!-- 10 degrees -->
          </vertical>
        </scan>
        <range>
          <min>0.1</min>
          <max>30.0</max>
          <resolution>0.01</resolution>
        </range>
      </ray>
      <plugin name="lidar_controller" filename="libgazebo_ros_ray_sensor.so">
        <ros>
          <namespace>lidar</namespace>
          <remapping>~/out:=scan</remapping>
        </ros>
        <output_type>sensor_msgs/LaserScan</output_type>
      </plugin>
    </sensor>
  </gazebo>

  <!-- IMU sensor in torso -->
  <link name="imu_link">
    <inertial>
      <mass value="0.01"/>
      <inertia ixx="0.0001" ixy="0.0" ixz="0.0" iyy="0.0001" iyz="0.0" izz="0.0001"/>
    </inertial>
  </link>

  <joint name="torso_to_imu" type="fixed">
    <parent link="torso"/>
    <child link="imu_link"/>
    <origin xyz="0 0 0"/>
  </joint>

  <!-- Gazebo plugin for IMU -->
  <gazebo reference="imu_link">
    <sensor name="imu_sensor" type="imu">
      <always_on>true</always_on>
      <update_rate>100</update_rate>
      <visualize>false</visualize>
      <imu>
        <angular_velocity>
          <x>
            <noise type="gaussian">
              <mean>0.0</mean>
              <stddev>0.0017</stddev>
            </noise>
          </x>
          <y>
            <noise type="gaussian">
              <mean>0.0</mean>
              <stddev>0.0017</stddev>
            </noise>
          </y>
          <z>
            <noise type="gaussian">
              <mean>0.0</mean>
              <stddev>0.0017</stddev>
            </noise>
          </z>
        </angular_velocity>
        <linear_acceleration>
          <x>
            <noise type="gaussian">
              <mean>0.0</mean>
              <stddev>0.017</stddev>
            </noise>
          </x>
          <y>
            <noise type="gaussian">
              <mean>0.0</mean>
              <stddev>0.017</stddev>
            </noise>
          </y>
          <z>
            <noise type="gaussian">
              <mean>0.0</mean>
              <stddev>0.017</stddev>
            </noise>
          </z>
        </linear_acceleration>
      </imu>
      <plugin name="imu_controller" filename="libgazebo_ros_imu_sensor.so">
        <ros>
          <namespace>imu</namespace>
          <remapping>~/out:=data</remapping>
        </ros>
        <frame_name>imu_link</frame_name>
      </plugin>
    </sensor>
  </gazebo>

  <!-- Left Arm -->
  <link name="left_upper_arm">
    <visual>
      <geometry>
        <cylinder length="0.3" radius="0.05"/>
      </geometry>
      <material name="red">
        <color rgba="1 0 0 0.8"/>
      </material>
    </visual>
    <collision>
      <geometry>
        <cylinder length="0.3" radius="0.05"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="0.8"/>
      <inertia ixx="0.01" ixy="0.0" ixz="0.0" iyy="0.01" iyz="0.0" izz="0.001"/>
    </inertial>
  </link>

  <joint name="left_shoulder" type="revolute">
    <parent link="torso"/>
    <child link="left_upper_arm"/>
    <origin xyz="0.15 0.1 0.1"/>
    <axis xyz="0 1 0"/>
    <limit lower="-1.57" upper="1.57" effort="100" velocity="1"/>
  </joint>

  <link name="left_lower_arm">
    <visual>
      <geometry>
        <cylinder length="0.3" radius="0.04"/>
      </geometry>
      <material name="red">
        <color rgba="1 0 0 0.8"/>
      </material>
    </visual>
    <collision>
      <geometry>
        <cylinder length="0.3" radius="0.04"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="0.6"/>
      <inertia ixx="0.008" ixy="0.0" ixz="0.0" iyy="0.008" iyz="0.0" izz="0.001"/>
    </inertial>
  </link>

  <joint name="left_elbow" type="revolute">
    <parent link="left_upper_arm"/>
    <child link="left_lower_arm"/>
    <origin xyz="0 0 -0.3"/>
    <axis xyz="0 1 0"/>
    <limit lower="0" upper="2.35" effort="100" velocity="1"/>
  </joint>

  <!-- Right Arm -->
  <link name="right_upper_arm">
    <visual>
      <geometry>
        <cylinder length="0.3" radius="0.05"/>
      </geometry>
      <material name="red">
        <color rgba="1 0 0 0.8"/>
      </material>
    </visual>
    <collision>
      <geometry>
        <cylinder length="0.3" radius="0.05"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="0.8"/>
      <inertia ixx="0.01" ixy="0.0" ixz="0.0" iyy="0.01" iyz="0.0" izz="0.001"/>
    </inertial>
  </link>

  <joint name="right_shoulder" type="revolute">
    <parent link="torso"/>
    <child link="right_upper_arm"/>
    <origin xyz="0.15 -0.1 0.1"/>
    <axis xyz="0 1 0"/>
    <limit lower="-1.57" upper="1.57" effort="100" velocity="1"/>
  </joint>

  <link name="right_lower_arm">
    <visual>
      <geometry>
        <cylinder length="0.3" radius="0.04"/>
      </geometry>
      <material name="red">
        <color rgba="1 0 0 0.8"/>
      </material>
    </visual>
    <collision>
      <geometry>
        <cylinder length="0.3" radius="0.04"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="0.6"/>
      <inertia ixx="0.008" ixy="0.0" ixz="0.0" iyy="0.008" iyz="0.0" izz="0.001"/>
    </inertial>
  </link>

  <joint name="right_elbow" type="revolute">
    <parent link="right_upper_arm"/>
    <child link="right_lower_arm"/>
    <origin xyz="0 0 -0.3"/>
    <axis xyz="0 1 0"/>
    <limit lower="0" upper="2.35" effort="100" velocity="1"/>
  </joint>

  <!-- Left Leg -->
  <link name="left_upper_leg">
    <visual>
      <geometry>
        <cylinder length="0.4" radius="0.06"/>
      </geometry>
      <material name="green">
        <color rgba="0 1 0 0.8"/>
      </material>
    </visual>
    <collision>
      <geometry>
        <cylinder length="0.4" radius="0.06"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="1.2"/>
      <inertia ixx="0.02" ixy="0.0" ixz="0.0" iyy="0.02" iyz="0.0" izz="0.002"/>
    </inertial>
  </link>

  <joint name="left_hip" type="revolute">
    <parent link="base_link"/>
    <child link="left_upper_leg"/>
    <origin xyz="-0.1 0.1 -0.05"/>
    <axis xyz="0 1 0"/>
    <limit lower="-1.57" upper="1.57" effort="100" velocity="1"/>
  </joint>

  <link name="left_lower_leg">
    <visual>
      <geometry>
        <cylinder length="0.4" radius="0.05"/>
      </geometry>
      <material name="green">
        <color rgba="0 1 0 0.8"/>
      </material>
    </visual>
    <collision>
      <geometry>
        <cylinder length="0.4" radius="0.05"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="1.0"/>
      <inertia ixx="0.015" ixy="0.0" ixz="0.0" iyy="0.015" iyz="0.0" izz="0.0015"/>
    </inertial>
  </link>

  <joint name="left_knee" type="revolute">
    <parent link="left_upper_leg"/>
    <child link="left_lower_leg"/>
    <origin xyz="0 0 -0.4"/>
    <axis xyz="0 1 0"/>
    <limit lower="0" upper="2.35" effort="100" velocity="1"/>
  </joint>

  <!-- Right Leg -->
  <link name="right_upper_leg">
    <visual>
      <geometry>
        <cylinder length="0.4" radius="0.06"/>
      </geometry>
      <material name="green">
        <color rgba="0 1 0 0.8"/>
      </material>
    </visual>
    <collision>
      <geometry>
        <cylinder length="0.4" radius="0.06"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="1.2"/>
      <inertia ixx="0.02" ixy="0.0" ixz="0.0" iyy="0.02" iyz="0.0" izz="0.002"/>
    </inertial>
  </link>

  <joint name="right_hip" type="revolute">
    <parent link="base_link"/>
    <child link="right_upper_leg"/>
    <origin xyz="-0.1 -0.1 -0.05"/>
    <axis xyz="0 1 0"/>
    <limit lower="-1.57" upper="1.57" effort="100" velocity="1"/>
  </joint>

  <link name="right_lower_leg">
    <visual>
      <geometry>
        <cylinder length="0.4" radius="0.05"/>
      </geometry>
      <material name="green">
        <color rgba="0 1 0 0.8"/>
      </material>
    </visual>
    <collision>
      <geometry>
        <cylinder length="0.4" radius="0.05"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="1.0"/>
      <inertia ixx="0.015" ixy="0.0" ixz="0.0" iyy="0.015" iyz="0.0" izz="0.0015"/>
    </inertial>
  </link>

  <joint name="right_knee" type="revolute">
    <parent link="right_upper_leg"/>
    <child link="right_lower_leg"/>
    <origin xyz="0 0 -0.4"/>
    <axis xyz="0 1 0"/>
    <limit lower="0" upper="2.35" effort="100" velocity="1"/>
  </joint>

  <!-- ros2_control interface -->
  <ros2_control name="GazeboSystem" type="system">
    <hardware>
      <plugin>gazebo_ros2_control/GazeboSystem</plugin>
    </hardware>
    <joint name="left_shoulder">
      <command_interface name="position">
        <param name="min">-1.57</param>
        <param name="max">1.57</param>
      </command_interface>
      <state_interface name="position"/>
      <state_interface name="velocity"/>
    </joint>
    <joint name="left_elbow">
      <command_interface name="position">
        <param name="min">0</param>
        <param name="max">2.35</param>
      </command_interface>
      <state_interface name="position"/>
      <state_interface name="velocity"/>
    </joint>
    <joint name="right_shoulder">
      <command_interface name="position">
        <param name="min">-1.57</param>
        <param name="max">1.57</param>
      </command_interface>
      <state_interface name="position"/>
      <state_interface name="velocity"/>
    </joint>
    <joint name="right_elbow">
      <command_interface name="position">
        <param name="min">0</param>
        <param name="max">2.35</param>
      </command_interface>
      <state_interface name="position"/>
      <state_interface name="velocity"/>
    </joint>
    <joint name="left_hip">
      <command_interface name="position">
        <param name="min">-1.57</param>
        <param name="max">1.57</param>
      </command_interface>
      <state_interface name="position"/>
      <state_interface name="velocity"/>
    </joint>
    <joint name="left_knee">
      <command_interface name="position">
        <param name="min">0</param>
        <param name="max">2.35</param>
      </command_interface>
      <state_interface name="position"/>
      <state_interface name="velocity"/>
    </joint>
    <joint name="right_hip">
      <command_interface name="position">
        <param name="min">-1.57</param>
        <param name="max">1.57</param>
      </command_interface>
      <state_interface name="position"/>
      <state_interface name="velocity"/>
    </joint>
    <joint name="right_knee">
      <command_interface name="position">
        <param name="min">0</param>
        <param name="max">2.35</param>
      </command_interface>
      <state_interface name="position"/>
      <state_interface name="velocity"/>
    </joint>
  </ros2_control>
</robot>
```

2. Create a sensor processing node to demonstrate sensor data interpretation in `ai_control_agent/ai_control_agent/sensor_processor.py`:
```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, LaserScan, Imu, PointCloud2, CameraInfo
from cv_bridge import CvBridge
import numpy as np
import math


class SensorProcessorNode(Node):
    def __init__(self):
        super().__init__('sensor_processor')

        # Create subscribers for different sensor types
        self.camera_subscriber = self.create_subscription(
            Image,
            '/camera/image_raw',
            self.camera_callback,
            10
        )

        self.depth_camera_subscriber = self.create_subscription(
            Image,
            '/depth_camera/depth/image_raw',
            self.depth_camera_callback,
            10
        )

        self.lidar_subscriber = self.create_subscription(
            LaserScan,
            '/lidar/scan',
            self.lidar_callback,
            10
        )

        self.imu_subscriber = self.create_subscription(
            Imu,
            '/imu/data',
            self.imu_callback,
            10
        )

        # Create publishers for processed sensor data
        self.obstacle_publisher = self.create_publisher(
            LaserScan,
            '/processed_scan',
            10
        )

        # Initialize CV Bridge for image processing
        self.cv_bridge = CvBridge()

        # Statistics tracking
        self.scan_count = 0
        self.camera_count = 0
        self.imu_count = 0

        self.get_logger().info('Sensor Processor Node initialized - Subscribed to all sensor topics')

    def camera_callback(self, msg):
        """Process RGB camera data"""
        try:
            # Convert ROS Image message to OpenCV image
            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")

            # Process image (simple example: get image dimensions)
            height, width, channels = cv_image.shape

            self.camera_count += 1
            if self.camera_count % 30 == 0:  # Log every 30th image
                self.get_logger().info(f'Camera: Received image {self.camera_count} - Size: {width}x{height}')

        except Exception as e:
            self.get_logger().error(f'Error processing camera image: {str(e)}')

    def depth_camera_callback(self, msg):
        """Process depth camera data"""
        try:
            # Convert ROS Image message to OpenCV image (depth data)
            depth_image = self.cv_bridge.imgmsg_to_cv2(msg, "32FC1")

            # Get some statistics from the depth image
            valid_depths = depth_image[depth_image > 0]  # Filter out invalid depths

            if len(valid_depths) > 0:
                avg_depth = np.mean(valid_depths)
                min_depth = np.min(valid_depths)
                max_depth = np.max(valid_depths)

                if self.camera_count % 30 == 0:  # Log every 30th depth image
                    self.get_logger().info(
                        f'Depth Camera: Avg: {avg_depth:.2f}m, Min: {min_depth:.2f}m, Max: {max_depth:.2f}m'
                    )
        except Exception as e:
            self.get_logger().error(f'Error processing depth camera image: {str(e)}')

    def lidar_callback(self, msg):
        """Process LiDAR data for obstacle detection"""
        try:
            # Process laser scan data
            ranges = np.array(msg.ranges)

            # Filter out invalid ranges (inf, nan) and get valid distances
            valid_ranges = ranges[np.isfinite(ranges)]

            if len(valid_ranges) > 0:
                min_distance = np.min(valid_ranges)
                avg_distance = np.mean(valid_ranges)

                # Check for obstacles in front (first 30 degrees)
                front_ranges = ranges[:len(ranges)//12]  # First 30 degrees (360/12)
                front_valid = front_ranges[np.isfinite(front_ranges)]

                if len(front_valid) > 0 and np.min(front_valid) < 1.0:  # Obstacle within 1m
                    self.get_logger().warn(f'OBSTACLE DETECTED: {np.min(front_valid):.2f}m ahead!')
                else:
                    self.get_logger().info(f'Clear path ahead: min distance {min_distance:.2f}m')

                # Publish processed scan with obstacle information
                processed_scan = LaserScan()
                processed_scan.header = msg.header
                processed_scan.angle_min = msg.angle_min
                processed_scan.angle_max = msg.angle_max
                processed_scan.angle_increment = msg.angle_increment
                processed_scan.time_increment = msg.time_increment
                processed_scan.scan_time = msg.scan_time
                processed_scan.range_min = msg.range_min
                processed_scan.range_max = msg.range_max
                processed_scan.ranges = msg.ranges

                self.obstacle_publisher.publish(processed_scan)

                self.scan_count += 1
                if self.scan_count % 10 == 0:  # Log every 10th scan
                    self.get_logger().info(
                        f'LiDAR: {len(valid_ranges)} valid readings, min: {min_distance:.2f}m, avg: {avg_distance:.2f}m'
                    )
        except Exception as e:
            self.get_logger().error(f'Error processing LiDAR data: {str(e)}')

    def imu_callback(self, msg):
        """Process IMU data for orientation and acceleration"""
        try:
            # Extract orientation (quaternion)
            orientation = msg.orientation
            # Convert quaternion to Euler angles (simplified)
            # Note: In a real implementation, use tf2 for proper conversion
            roll = math.atan2(
                2.0 * (orientation.w * orientation.x + orientation.y * orientation.z),
                1.0 - 2.0 * (orientation.x * orientation.x + orientation.y * orientation.y)
            )
            pitch = math.asin(
                2.0 * (orientation.w * orientation.y - orientation.z * orientation.x)
            )
            yaw = math.atan2(
                2.0 * (orientation.w * orientation.z + orientation.x * orientation.y),
                1.0 - 2.0 * (orientation.y * orientation.y + orientation.z * orientation.z)
            )

            # Extract linear acceleration
            linear_acc = msg.linear_acceleration

            self.imu_count += 1
            if self.imu_count % 50 == 0:  # Log every 50th IMU reading
                self.get_logger().info(
                    f'IMU: Roll: {math.degrees(roll):.1f}°, Pitch: {math.degrees(pitch):.1f}°, '
                    f'Yaw: {math.degrees(yaw):.1f}°, Acc: [{linear_acc.x:.2f}, {linear_acc.y:.2f}, {linear_acc.z:.2f}]'
                )
        except Exception as e:
            self.get_logger().error(f'Error processing IMU data: {str(e)}')


def main(args=None):
    rclpy.init(args=args)
    node = SensorProcessorNode()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        node.get_logger().info('Shutting down Sensor Processor Node')
    finally:
        node.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

3. Update the setup.py file to include the new sensor processor:
```python
import os
from glob import glob
from setuptools import setup
from setuptools import find_packages

package_name = 'ai_control_agent'

setup(
    name=package_name,
    version='0.0.0',
    packages=find_packages(exclude=['test']),
    data_files=[
        ('share/ament_index/resource_index/packages',
            ['resource/' + package_name]),
        ('share/' + package_name, ['package.xml']),
    ],
    install_requires=['setuptools', 'cv-bridge', 'numpy'],
    zip_safe=True,
    maintainer='Humanoid Robotics Book',
    maintainer_email='humanoid-robotics-book@example.com',
    description='AI control agent for the Physical AI & Humanoid Robotics book',
    license='Apache License 2.0',
    tests_require=['pytest'],
    entry_points={
        'console_scripts': [
            'simple_ai_agent = ai_control_agent.simple_ai_agent:main',
            'gazebo_commander = ai_control_agent.gazebo_commander:main',
            'sensor_processor = ai_control_agent.sensor_processor:main',
        ],
    },
)
```

4. Create a launch file to start the sensor simulation with the humanoid robot:
```bash
mkdir -p ~/robotics_ws/src/ai_control_agent/launch
```

Create `ai_control_agent/launch/sensor_simulation.launch.py`:
```python
from launch import LaunchDescription
from launch.actions import DeclareLaunchArgument, IncludeLaunchDescription, RegisterEventHandler
from launch.launch_description_sources import PythonLaunchDescriptionSource
from launch.substitutions import LaunchConfiguration, PathJoinSubstitution
from launch_ros.actions import Node
from launch_ros.substitutions import FindPackageShare
from ament_index_python.packages import get_package_share_directory
from launch.event_handlers import OnProcessExit
from launch.actions import ExecuteProcess
import os


def generate_launch_description():
    # Get the package share directory
    pkg_share = get_package_share_directory('humanoid_description')
    default_model_path = os.path.join(pkg_share, 'urdf/simple_humanoid.urdf')
    default_world_path = os.path.join(pkg_share, 'worlds/empty_world.world')

    # Declare launch arguments
    model_arg = DeclareLaunchArgument(
        name='model',
        default_value=default_model_path,
        description='Absolute path to robot urdf file'
    )

    world_arg = DeclareLaunchArgument(
        name='world',
        default_value=default_world_path,
        description='Absolute path to world file'
    )

    # Start Gazebo server and client
    gazebo = IncludeLaunchDescription(
        PythonLaunchDescriptionSource([
            PathJoinSubstitution([
                FindPackageShare('gazebo_ros'),
                'launch',
                'gazebo.launch.py'
            ])
        ]),
        launch_arguments={
            'world': LaunchConfiguration('world'),
            'verbose': 'false',
        }.items()
    )

    # Robot State Publisher node
    robot_state_publisher_node = Node(
        package='robot_state_publisher',
        executable='robot_state_publisher',
        parameters=[{
            'robot_description': open(default_model_path).read()
        }]
    )

    # Spawn the robot in Gazebo
    spawn_entity_node = Node(
        package='gazebo_ros',
        executable='spawn_entity.py',
        arguments=[
            '-file', LaunchConfiguration('model'),
            '-entity', 'simple_humanoid',
            '-x', '0.0',
            '-y', '0.0',
            '-z', '1.0'
        ],
        output='screen'
    )

    # Sensor Processor node
    sensor_processor_node = Node(
        package='ai_control_agent',
        executable='sensor_processor',
        name='sensor_processor',
        output='screen'
    )

    # Joint State Publisher GUI node
    joint_state_publisher_gui_node = Node(
        package='joint_state_publisher_gui',
        executable='joint_state_publisher_gui',
        name='joint_state_publisher_gui',
        output='screen'
    )

    # RViz2 node
    rviz_node = Node(
        package='rviz2',
        executable='rviz2',
        name='rviz2',
        output='screen'
    )

    return LaunchDescription([
        model_arg,
        world_arg,
        gazebo,
        robot_state_publisher_node,
        spawn_entity_node,
        sensor_processor_node,
        joint_state_publisher_gui_node,
        rviz_node
    ])
```

5. Build the package:
```bash
cd ~/robotics_ws
colcon build --packages-select humanoid_description ai_control_agent
source install/setup.bash
```

6. Test the sensor simulation:
```bash
ros2 launch ai_control_agent sensor_simulation.launch.py
```

## Safety Notes
- Ensure sensor noise parameters match real-world characteristics for realistic simulation
- Validate sensor ranges and fields of view are appropriate for the robot's intended use
- Implement proper error handling for sensor data processing
- Monitor sensor data rates to avoid overwhelming the system
- Include sensor health checks and validation in robot control systems

## Evaluation Criteria
- Are all sensors properly integrated into the URDF model?
- Do sensor data streams publish correctly from the simulation?
- Can the sensor processor node interpret and process sensor data?
- Are sensor noise characteristics realistic?
- Is the simulation running at acceptable performance with all sensors active?