---
sidebar_position: 2
title: "Chapter 4.2: LLM Cognitive Planning"
description: "Implementing large language model cognitive planning for humanoid robots"
---

# Chapter 4.2: LLM Cognitive Planning

## Introduction

In this chapter, we'll explore how to implement cognitive planning for humanoid robots using Large Language Models (LLMs). LLMs provide powerful reasoning capabilities that can be leveraged to generate high-level plans and strategies for robot behavior. By integrating LLMs with our ROS 2 system, we create a cognitive layer that can interpret human instructions and generate executable robot plans.

## Learning Objectives

By the end of this chapter, you will:
- Understand the fundamentals of LLMs and their applications in robotics
- Learn how to integrate LLMs with ROS 2 for cognitive planning
- Implement an LLM planning node that generates robot plans from text
- Create custom ROS 2 messages for plan representation
- Design prompt engineering techniques for robotics applications

## Overview of LLMs in Robotics

Large Language Models have revolutionized artificial intelligence by demonstrating remarkable capabilities in understanding and generating human language. In robotics, LLMs can serve as cognitive planners that:

- Interpret natural language commands
- Generate high-level task plans
- Reason about object affordances and spatial relationships
- Handle ambiguous or incomplete instructions
- Adapt to new situations through few-shot learning

### Why LLMs for Robot Planning?

LLMs offer several advantages for robot planning:
- **Natural Language Understanding**: Direct interpretation of human commands
- **Common-Sense Reasoning**: Understanding of physical and social world
- **Generalization**: Ability to handle novel situations
- **Knowledge Integration**: Access to vast amounts of world knowledge

## LLM Integration Architecture

### System Overview

Our LLM planning system will consist of:
1. **Input Processing**: Receive text commands from voice recognition or direct input
2. **LLM Interface**: Query the LLM with appropriate prompts
3. **Plan Generation**: Convert LLM responses to executable robot plans
4. **Plan Execution**: Interface with ROS 2 navigation and manipulation systems

### Plan Representation

Robot plans will be represented as sequences of:
- Navigation goals
- Manipulation actions
- Conditional behaviors
- Safety constraints

## Implementation Approach

### LLM Selection

For our implementation, we'll consider several LLM options:
- **OpenAI GPT**: High quality but requires API access
- **Open Source Models**: Llama, Mistral, or Phi for local deployment
- **Specialized Robotics Models**: Models fine-tuned for robotics tasks

### Prompt Engineering

Effective prompt engineering is crucial for robotics applications:
- Provide clear examples of desired behavior
- Include robot capabilities and constraints
- Structure requests for consistent output format
- Handle error cases and edge conditions

## Technical Implementation

### Custom ROS 2 Message Types

We'll define custom message types for plan representation. First, let's create the message definition files:

```python
# In robotics_ws/src/vla_llm_planner/msg/LLMPlan.msg
string plan_id
string description
builtin_interfaces/Time timestamp
float32 confidence
PlanStep[] steps

# In robotics_ws/src/vla_llm_planner/msg/PlanStep.msg
string action_type  # "navigation", "manipulation", "perception", "wait", etc.
string action_name   # "move_to", "pick_object", "detect_person", "open_door", etc.
string[] action_params  # Additional parameters for the action
geometry_msgs/Pose target_pose
builtin_interfaces/Time expected_duration
string[] preconditions  # Conditions that must be true before executing
string[] effects        # Effects that will be true after executing
float32 success_probability  # Estimated probability of successful execution
```

### LLM Planning Node Implementation

The LLM planning node will handle the complete pipeline from text commands to structured robot plans:

```python
#!/usr/bin/env python3

import rclpy
from rclpy.node import Node
import openai
import json
import time
from std_msgs.msg import String
from vla_msgs.msg import LLMPlan, PlanStep
from geometry_msgs.msg import Pose
from builtin_interfaces.msg import Time
import re
import os
from ament_index_python.packages import get_package_share_directory

class LLMPlannerNode(Node):
    def __init__(self):
        super().__init__('llm_planner_node')

        # Initialize LLM client (multiple options supported)
        self.llm_provider = self.declare_parameter('llm_provider', 'openai').value
        self.setup_llm_client()

        # Parameters
        self.model_name = self.declare_parameter('model_name', 'gpt-3.5-turbo').value
        self.max_tokens = self.declare_parameter('max_tokens', 1000).value
        self.temperature = self.declare_parameter('temperature', 0.3).value
        self.robot_capabilities_file = self.declare_parameter(
            'robot_capabilities_file',
            os.path.join(get_package_share_directory('vla_llm_planner'), 'config', 'robot_capabilities.json')
        ).value

        # Load robot capabilities
        self.robot_capabilities = self.load_robot_capabilities()

        # Publishers and subscribers
        self.command_sub = self.create_subscription(
            String,
            'command_input',
            self.command_callback,
            10
        )

        self.plan_pub = self.create_publisher(
            LLMPlan,
            'robot_plan',
            10
        )

        # Optional plan validation feedback
        self.validation_sub = self.create_subscription(
            String,
            'plan_validation',
            self.validation_callback,
            10
        )

        self.get_logger().info(f"LLM Planner node initialized with {self.llm_provider} provider")

    def setup_llm_client(self):
        """Setup LLM client based on provider"""
        if self.llm_provider == 'openai':
            api_key = self.declare_parameter('openai_api_key', '').value
            if api_key:
                openai.api_key = api_key
            else:
                self.get_logger().warn("No OpenAI API key provided - using mock responses")
        elif self.llm_provider == 'local':
            # Setup for local model (e.g., using transformers)
            from transformers import pipeline
            self.local_pipeline = pipeline("text-generation", model="microsoft/DialoGPT-medium")
        # Add other providers as needed

    def load_robot_capabilities(self):
        """Load robot capabilities from configuration file"""
        try:
            with open(self.robot_capabilities_file, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            self.get_logger().warn(f"Robot capabilities file not found: {self.robot_capabilities_file}")
            # Return default capabilities
            return {
                "navigation": {
                    "max_speed": 1.0,
                    "min_turn_radius": 0.5,
                    "supported_terrains": ["indoor", "flat", "carpet", "tile"]
                },
                "manipulation": {
                    "max_payload": 5.0,
                    "reach": 1.2,
                    "gripper_types": ["parallel", "suction"]
                },
                "perception": {
                    "camera_range": 10.0,
                    "object_detection": ["person", "cup", "bottle", "chair", "table"],
                    "language_understanding": True
                }
            }

    def command_callback(self, msg):
        """Process incoming text commands and generate plans"""
        command = msg.data
        self.get_logger().info(f"Received command: {command}")

        # Generate plan using LLM
        start_time = time.time()
        plan = self.generate_plan(command)
        generation_time = time.time() - start_time

        if plan:
            self.get_logger().info(f"Generated plan in {generation_time:.2f}s with {len(plan.steps)} steps")
            self.plan_pub.publish(plan)
            self.get_logger().info("Published plan to execution system")
        else:
            self.get_logger().error("Failed to generate plan for command")

    def generate_plan(self, command):
        """Generate robot plan using LLM"""
        # Create prompt for the LLM with robot capabilities
        prompt = self.create_prompt(command)

        try:
            if self.llm_provider == 'openai' and openai.api_key:
                # Use OpenAI API
                response = openai.ChatCompletion.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": self.get_system_prompt()},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=self.max_tokens,
                    temperature=self.temperature
                )

                llm_response = response.choices[0].message['content']
            elif self.llm_provider == 'local':
                # Use local model (simplified example)
                llm_response = self.local_generate(prompt)
            else:
                # Mock response for demonstration
                llm_response = self.mock_plan_response(command)

            # Parse LLM response into structured plan
            return self.parse_plan_response(llm_response)

        except Exception as e:
            self.get_logger().error(f"LLM query failed: {e}")
            return None

    def create_prompt(self, command):
        """Create structured prompt for LLM with robot capabilities"""
        return f"""
        Given the following command: "{command}"

        Generate a step-by-step plan for a humanoid robot with these capabilities:
        {json.dumps(self.robot_capabilities, indent=2)}

        The robot can perform these high-level actions:
        - Navigation: move to specific locations, avoid obstacles
        - Manipulation: grasp objects, open doors, press buttons
        - Perception: detect objects, recognize people, assess environment
        - Communication: speak, gesture, acknowledge commands

        Respond in JSON format with the following structure:
        {{
            "description": "Brief description of the plan",
            "steps": [
                {{
                    "action_type": "navigation|manipulation|perception|communication",
                    "action_name": "move_to|pick_object|detect_person|speak|etc",
                    "action_params": ["param1", "param2"],
                    "target_pose": {{"x": 0.0, "y": 0.0, "z": 0.0, "qx": 0.0, "qy": 0.0, "qz": 0.0, "qw": 1.0}},
                    "expected_duration": 10.0,
                    "preconditions": ["robot_is_idle", "object_in_reach"],
                    "effects": ["robot_at_location", "object_grasped"],
                    "success_probability": 0.9
                }}
            ]
        }}

        Ensure all actions are feasible given the robot's capabilities.
        Include safety considerations in the plan.
        """

    def get_system_prompt(self):
        """Get system prompt for LLM"""
        return f"""
        You are a helpful assistant that generates robot action plans.
        The robot is a humanoid with the capabilities defined above.
        Always respond in the exact JSON format requested.
        Ensure all actions are feasible for the robot given its physical and sensory limitations.
        Include only actions that the robot is capable of performing based on the provided capabilities.
        Consider safety constraints in all plans.
        """

    def parse_plan_response(self, response):
        """Parse LLM response into structured plan"""
        try:
            # Extract JSON from response if needed
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                plan_data = json.loads(json_match.group())
            else:
                plan_data = json.loads(response)

            # Create LLMPlan message
            plan = LLMPlan()
            plan.plan_id = f"plan_{self.get_clock().now().nanoseconds}"
            plan.description = plan_data.get("description", "Generated plan")
            plan.timestamp = self.get_clock().now().to_msg()
            plan.confidence = 0.8  # Default confidence, could be derived from LLM response

            # Create plan steps
            for step_data in plan_data.get("steps", []):
                step = PlanStep()
                step.action_type = step_data.get("action_type", "unknown")
                step.action_name = step_data.get("action_name", "unknown")
                step.action_params = step_data.get("action_params", [])
                step.expected_duration = float(step_data.get("expected_duration", 10.0))
                step.preconditions = step_data.get("preconditions", [])
                step.effects = step_data.get("effects", [])
                step.success_probability = float(step_data.get("success_probability", 0.8))

                # Parse target pose if provided
                target_pose_data = step_data.get("target_pose", {})
                if target_pose_data:
                    step.target_pose.position.x = target_pose_data.get("x", 0.0)
                    step.target_pose.position.y = target_pose_data.get("y", 0.0)
                    step.target_pose.position.z = target_pose_data.get("z", 0.0)
                    step.target_pose.orientation.x = target_pose_data.get("qx", 0.0)
                    step.target_pose.orientation.y = target_pose_data.get("qy", 0.0)
                    step.target_pose.orientation.z = target_pose_data.get("qz", 0.0)
                    step.target_pose.orientation.w = target_pose_data.get("qw", 1.0)

                plan.steps.append(step)

            return plan

        except json.JSONDecodeError as e:
            self.get_logger().error(f"Failed to decode JSON response: {e}")
            self.get_logger().debug(f"Response was: {response}")
            return None
        except Exception as e:
            self.get_logger().error(f"Failed to parse plan response: {e}")
            return None

    def local_generate(self, prompt):
        """Generate response using local model (placeholder implementation)"""
        # This is a simplified placeholder - in practice you'd use transformers or similar
        return f'{{"description": "Local plan for: {prompt[:50]}...", "steps": [{{"action_type": "navigation", "action_name": "move_to", "action_params": ["location"], "target_pose": {{"x": 1.0, "y": 1.0, "z": 0.0, "qx": 0.0, "qy": 0.0, "qz": 0.0, "qw": 1.0}}, "expected_duration": 30.0, "preconditions": ["robot_is_idle"], "effects": ["robot_at_location"], "success_probability": 0.85}}]}}'

    def mock_plan_response(self, command):
        """Generate mock plan for demonstration"""
        return f"""
        {{
            "description": "Mock plan for: {command}",
            "steps": [
                {{
                    "action_type": "navigation",
                    "action_name": "move_to",
                    "action_params": ["kitchen"],
                    "target_pose": {{"x": 2.0, "y": 1.5, "z": 0.0, "qx": 0.0, "qy": 0.0, "qz": 0.0, "qw": 1.0}},
                    "expected_duration": 30.0,
                    "preconditions": ["robot_is_idle"],
                    "effects": ["robot_at_kitchen"],
                    "success_probability": 0.9
                }}
            ]
        }}
        """

    def validation_callback(self, msg):
        """Handle plan validation feedback"""
        feedback = msg.data
        self.get_logger().info(f"Received plan validation feedback: {feedback}")
        # Could be used to improve future planning or adjust confidence

def main(args=None):
    rclpy.init(args=args)
    node = LLMPlannerNode()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        node.get_logger().info("LLM Planner node shutting down...")
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

### LLM Planning Node

The LLM planning node will:
1. Subscribe to text commands
2. Format prompts for the LLM
3. Process LLM responses into structured plans
4. Publish plans to execution systems

## Practical Implementation

Let's implement the LLM planning node in our ROS 2 workspace.

### Basic Node Structure

```python
#!/usr/bin/env python3

import rclpy
from rclpy.node import Node
import openai
import json
from std_msgs.msg import String
from vla_msgs.msg import LLMPlan, PlanStep
from geometry_msgs.msg import Pose
import re

class LLMPlannerNode(Node):
    def __init__(self):
        super().__init__('llm_planner_node')

        # Initialize LLM client (using OpenAI as example)
        api_key = self.declare_parameter('openai_api_key', '').value
        if api_key:
            openai.api_key = api_key
        else:
            self.get_logger().warn("No OpenAI API key provided - using mock responses")

        # Parameters
        self.model_name = self.declare_parameter('model_name', 'gpt-3.5-turbo').value
        self.max_tokens = self.declare_parameter('max_tokens', 1000).value

        # Publishers and subscribers
        self.command_sub = self.create_subscription(
            String,
            'command_input',
            self.command_callback,
            10
        )

        self.plan_pub = self.create_publisher(
            LLMPlan,
            'robot_plan',
            10
        )

        self.get_logger().info("LLM Planner node initialized")

    def command_callback(self, msg):
        """Process incoming text commands and generate plans"""
        command = msg.data
        self.get_logger().info(f"Received command: {command}")

        # Generate plan using LLM
        plan = self.generate_plan(command)

        if plan:
            self.plan_pub.publish(plan)
            self.get_logger().info(f"Published plan with {len(plan.steps)} steps")

    def generate_plan(self, command):
        """Generate robot plan using LLM"""
        # Create prompt for the LLM
        prompt = self.create_prompt(command)

        try:
            if openai.api_key:  # Use actual API if available
                response = openai.ChatCompletion.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": self.get_system_prompt()},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=self.max_tokens,
                    temperature=0.3
                )

                llm_response = response.choices[0].message['content']
            else:
                # Mock response for demonstration
                llm_response = self.mock_plan_response(command)

            # Parse LLM response into structured plan
            return self.parse_plan_response(llm_response)

        except Exception as e:
            self.get_logger().error(f"LLM query failed: {e}")
            return None

    def create_prompt(self, command):
        """Create structured prompt for LLM"""
        return f"""
        Given the following command: "{command}"

        Generate a step-by-step plan for a humanoid robot to execute this command.
        The robot has these capabilities:
        - Navigation to specific locations
        - Object manipulation (grasping, lifting, placing)
        - Door opening/closing
        - Human interaction

        Respond in JSON format with the following structure:
        {{
            "description": "Brief description of the plan",
            "steps": [
                {{
                    "action_type": "navigation|manipulation|wait",
                    "action_name": "move_to|pick_object|open_door|etc",
                    "action_params": ["param1", "param2"],
                    "target_pose": {{"x": 0.0, "y": 0.0, "z": 0.0, "qx": 0.0, "qy": 0.0, "qz": 0.0, "qw": 1.0}},
                    "expected_duration": 10.0,
                    "preconditions": ["condition1", "condition2"],
                    "effects": ["effect1", "effect2"]
                }}
            ]
        }}
        """

    def get_system_prompt(self):
        """Get system prompt for LLM"""
        return """
        You are a helpful assistant that generates robot action plans.
        Always respond in the exact JSON format requested.
        Ensure all actions are feasible for a humanoid robot.
        Include only actions that the robot is capable of performing.
        """

    def parse_plan_response(self, response):
        """Parse LLM response into structured plan"""
        try:
            # Extract JSON from response if needed
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                plan_data = json.loads(json_match.group())
            else:
                plan_data = json.loads(response)

            # Create LLMPlan message
            plan = LLMPlan()
            plan.plan_id = f"plan_{self.get_clock().now().nanoseconds}"
            plan.description = plan_data.get("description", "Generated plan")
            plan.timestamp = self.get_clock().now().to_msg()
            plan.confidence = 0.8  # Default confidence

            # Create plan steps
            for step_data in plan_data.get("steps", []):
                step = PlanStep()
                step.action_type = step_data.get("action_type", "unknown")
                step.action_name = step_data.get("action_name", "unknown")
                step.action_params = step_data.get("action_params", [])
                step.expected_duration = float(step_data.get("expected_duration", 10.0))
                step.preconditions = step_data.get("preconditions", [])
                step.effects = step_data.get("effects", [])

                # Parse target pose if provided
                target_pose_data = step_data.get("target_pose", {})
                if target_pose_data:
                    step.target_pose.position.x = target_pose_data.get("x", 0.0)
                    step.target_pose.position.y = target_pose_data.get("y", 0.0)
                    step.target_pose.position.z = target_pose_data.get("z", 0.0)
                    step.target_pose.orientation.x = target_pose_data.get("qx", 0.0)
                    step.target_pose.orientation.y = target_pose_data.get("qy", 0.0)
                    step.target_pose.orientation.z = target_pose_data.get("qz", 0.0)
                    step.target_pose.orientation.w = target_pose_data.get("qw", 1.0)

                plan.steps.append(step)

            return plan

        except Exception as e:
            self.get_logger().error(f"Failed to parse plan response: {e}")
            return None

    def mock_plan_response(self, command):
        """Generate mock plan for demonstration"""
        return f"""
        {{
            "description": "Mock plan for: {command}",
            "steps": [
                {{
                    "action_type": "navigation",
                    "action_name": "move_to",
                    "action_params": ["kitchen"],
                    "target_pose": {{"x": 2.0, "y": 1.5, "z": 0.0, "qx": 0.0, "qy": 0.0, "qz": 0.0, "qw": 1.0}},
                    "expected_duration": 30.0,
                    "preconditions": ["robot_is_idle"],
                    "effects": ["robot_at_kitchen"]
                }}
            ]
        }}
        """

def main(args=None):
    rclpy.init(args=args)
    node = LLMPlannerNode()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

### Plan Execution Integration

The generated plans will be executed by:
- Navigation systems for movement tasks
- Manipulation systems for object interaction
- State machines for complex behaviors
- Safety systems for constraint enforcement

## Challenges and Solutions

### Response Consistency

LLMs can generate inconsistent responses. Solutions include:
- **Structured prompting**: Use detailed examples and clear formatting requirements
- **Response validation**: Validate JSON structure and content before execution
- **Confidence scoring**: Assess plan quality and reliability
- **Fallback strategies**: Use default behaviors for invalid plans
- **Fine-tuning**: Adapt models to robotics-specific tasks and terminology

### Real-time Constraints

LLMs can be slow to respond. Consider:
- **Caching**: Store common plans for frequent commands
- **Asynchronous planning**: Generate plans in background
- **Hierarchical planning**: Use lightweight models for simple tasks, complex models for complex tasks
- **Pre-planning**: Generate plans for common scenarios in advance
- **Local execution**: Run smaller models on robot hardware for faster response

### Safety and Validation

Robot safety is paramount. Implement:
- **Plan validation**: Check feasibility against robot capabilities
- **Safety constraint checking**: Ensure plans comply with safety protocols
- **Human-in-the-loop**: Allow human override for critical decisions
- **Emergency stop capabilities**: Immediate stop functionality
- **Simulation validation**: Test plans in simulation before real execution

### Knowledge Integration

LLMs may lack current context about the environment:
- **World state integration**: Provide current environment state in prompts
- **Memory systems**: Implement episodic memory for ongoing tasks
- **Multi-modal grounding**: Combine LLM reasoning with real-time sensor data
- **Verification loops**: Confirm assumptions through perception

## Integration with VLA System

### Voice-to-Plan Pipeline

The complete pipeline connects:
1. Voice input → Whisper transcription
2. Text command → LLM planning
3. Plan → ROS 2 execution
4. Execution feedback → plan refinement

### Multi-modal Coordination

LLM plans will coordinate with:
- Vision systems for object recognition and scene understanding
- Navigation systems for path planning and obstacle avoidance
- Manipulation systems for grasping and interaction
- State monitoring for execution feedback and replanning

### Plan Execution and Monitoring

The execution system will:
- Monitor plan progress in real-time
- Detect execution failures and deviations
- Request plan revisions when needed
- Provide feedback to improve future planning

## Deployment Considerations

### Edge vs. Cloud Deployment

Consider the trade-offs between edge and cloud deployment:
- **Edge**: Lower latency, offline capability, privacy
- **Cloud**: More powerful models, easier updates, higher costs

### Resource Management

LLM inference requires significant resources:
- **Memory**: Large models require substantial RAM
- **Compute**: GPU acceleration for faster inference
- **Power**: Consider power consumption on mobile robots
- **Thermal**: Manage heat generation during sustained operation

### Model Optimization

For robotics deployment:
- **Quantization**: Reduce model size and improve speed
- **Pruning**: Remove unnecessary parameters
- **Distillation**: Create smaller, faster student models
- **Caching**: Store frequently used responses

## Testing and Validation

### Unit Testing

Test individual components:
- Prompt generation and formatting
- Response parsing and validation
- Message publishing and subscription
- Error handling and fallbacks

### Integration Testing

Test complete pipeline:
- End-to-end voice-to-action flow
- Plan feasibility validation
- Execution monitoring and feedback
- Safety system integration

### Performance Testing

Evaluate system performance:
- Planning latency under various conditions
- Success rates for different command types
- Resource utilization during operation
- Robustness to environmental variations

## Next Steps

In the next chapter, we'll integrate voice input and LLM planning with vision systems to create a complete multi-modal perception and decision-making system. This will form the core of our Vision-Language-Action (VLA) framework by connecting all three modalities into a unified cognitive system.

## Summary

In this chapter, we've learned how to implement LLM-based cognitive planning for humanoid robots. We covered the complete pipeline from text commands to executable robot plans, including prompt engineering, response parsing, and integration with ROS 2 systems. We explored technical implementation details, challenges, and solutions for deploying LLMs in robotics applications. This cognitive planning capability is essential for creating robots that can understand and execute complex human instructions in natural language, forming a critical component of our Vision-Language-Action (VLA) system.