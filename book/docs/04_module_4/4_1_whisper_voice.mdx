---
sidebar_position: 1
title: "Chapter 4.1: Voice Input with Whisper"
description: "Implementing speech recognition for humanoid robots using OpenAI's Whisper model"
---

# Chapter 4.1: Voice Input with Whisper

## Introduction

In this chapter, we'll explore how to enable humanoid robots to understand human speech using OpenAI's Whisper model. Whisper is a state-of-the-art speech recognition model that can transcribe speech to text with high accuracy across multiple languages. Integrating voice input capabilities into our humanoid robot will allow for more natural human-robot interaction.

## Learning Objectives

By the end of this chapter, you will:
- Understand the fundamentals of speech recognition and Whisper
- Learn how to integrate Whisper with ROS 2 for real-time voice input
- Implement a voice input system for your humanoid robot
- Create a ROS 2 node that processes audio and publishes transcribed text

## Overview of Speech Recognition in Robotics

Speech recognition is a critical component for natural human-robot interaction. It enables robots to:
- Understand verbal commands
- Engage in conversational interfaces
- Respond to voice-activated triggers
- Process natural language instructions

### Why Whisper?

OpenAI's Whisper model offers several advantages for robotics applications:
- High accuracy across multiple languages
- Robustness to accents and background noise
- Open-source implementation
- Pre-trained on diverse audio data

## Whisper Architecture and Capabilities

Whisper is a transformer-based model that can handle various speech recognition tasks:
- Automatic speech recognition (ASR)
- Language identification
- Speech translation
- Voice activity detection

The model is available in multiple sizes, from tiny (39M parameters) to large (1550M parameters), allowing for trade-offs between accuracy and computational requirements.

## Integration with ROS 2

To integrate Whisper with ROS 2, we'll create a custom node that:
1. Subscribes to audio data from the robot's microphones
2. Processes the audio through the Whisper model
3. Publishes the transcribed text as ROS 2 messages
4. Handles real-time processing constraints

### Audio Data Format

For optimal Whisper performance, we'll use the following audio format:
- Sample rate: 16 kHz (Whisper's native rate)
- Channels: Mono
- Bit depth: 16-bit PCM

## Implementation Steps

### Step 1: Setting Up Dependencies

First, we need to install the required dependencies for Whisper in our ROS 2 workspace:

```bash
# Install Whisper and audio processing libraries
pip install openai-whisper
pip install sounddevice  # For audio input
pip install pydub        # For audio processing
pip install numpy       # For audio manipulation
pip install torch       # For model inference

# For ROS 2 audio messages (if using standard audio_msgs)
sudo apt-get install ros-humble-audio-common
```

### Step 2: Creating the Whisper Node

We'll create a ROS 2 node that handles audio input and transcription. The node will:

1. Subscribe to audio data from the robot's microphones
2. Buffer audio chunks for processing
3. Transcribe audio using Whisper
4. Publish transcribed text to a ROS 2 topic

### Step 3: Audio Data Handling

For optimal Whisper performance, we need to handle audio data properly:

- **Sample Rate**: Convert input to 16 kHz (Whisper's native rate)
- **Format**: Mono, 16-bit PCM
- **Chunking**: Process audio in appropriate time segments
- **Buffering**: Maintain audio buffer for real-time processing

### Step 4: Real-time Processing Considerations

For real-time voice input, we need to consider:
- Audio buffering strategies (sliding window approach)
- Processing latency requirements (target under 200ms)
- Computational resource constraints (GPU acceleration)
- Accuracy vs. speed trade-offs (model size selection)

## Practical Implementation

Let's implement the Whisper transcription node in our ROS 2 workspace.

### Audio Subscription

The node will subscribe to audio data from the robot's microphone system. We'll use a custom message type or standard audio messages depending on our system architecture. Here's the basic structure of our Whisper node:

```python
#!/usr/bin/env python3

import rclpy
from rclpy.node import Node
import whisper
import numpy as np
import torch
from audio_common_msgs.msg import AudioData
from std_msgs.msg import String
import io
import wave

class WhisperNode(Node):
    def __init__(self):
        super().__init__('whisper_node')

        # Load Whisper model (using smaller model for real-time performance)
        self.get_logger().info("Loading Whisper model...")
        self.model = whisper.load_model("base")  # or "tiny" for faster inference

        # Audio buffer for accumulating audio chunks
        self.audio_buffer = np.array([], dtype=np.float32)

        # Parameters
        self.buffer_duration = self.declare_parameter('buffer_duration', 2.0).value  # seconds
        self.sample_rate = self.declare_parameter('sample_rate', 16000).value
        self.min_audio_length = self.declare_parameter('min_audio_length', 0.5).value  # seconds

        # Publishers and subscribers
        self.audio_sub = self.create_subscription(
            AudioData,
            'audio_input',
            self.audio_callback,
            10
        )

        self.text_pub = self.create_publisher(
            String,
            'transcribed_text',
            10
        )

        self.get_logger().info("Whisper node initialized")

    def audio_callback(self, msg):
        """Process incoming audio data"""
        # Convert audio data to numpy array
        audio_data = np.frombuffer(msg.data, dtype=np.int16).astype(np.float32) / 32768.0

        # Add to buffer
        self.audio_buffer = np.concatenate([self.audio_buffer, audio_data])

        # Check if buffer has enough data for transcription
        required_samples = int(self.buffer_duration * self.sample_rate)
        if len(self.audio_buffer) >= required_samples:
            self.transcribe_audio()

    def transcribe_audio(self):
        """Transcribe buffered audio using Whisper"""
        if len(self.audio_buffer) < self.min_audio_length * self.sample_rate:
            # Not enough audio data, return early
            return

        # Ensure audio is in the right format
        audio = self.audio_buffer.copy()

        # Clear buffer for next round
        self.audio_buffer = np.array([], dtype=np.float32)

        try:
            # Transcribe using Whisper
            result = self.model.transcribe(audio, fp16=False)  # fp16 might cause issues on some systems
            text = result["text"].strip()

            if text:  # Only publish if there's text
                msg = String()
                msg.data = text
                self.text_pub.publish(msg)
                self.get_logger().info(f"Transcribed: {text}")

        except Exception as e:
            self.get_logger().error(f"Transcription error: {e}")

def main(args=None):
    rclpy.init(args=args)
    node = WhisperNode()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

### Audio Processing Pipeline

The transcription pipeline will:
1. Receive audio chunks from the microphone
2. Convert audio to Whisper's required format (16kHz, mono, float32)
3. Buffer audio for optimal transcription length
4. Apply Whisper transcription
5. Publish results with timestamps and confidence scores

### Model Optimization

For real-time performance on robotics hardware:
- Use smaller Whisper models ("tiny" or "base")
- Consider quantization for faster inference
- Implement GPU acceleration if available
- Optimize batch processing when possible

## Challenges and Solutions

### Real-time Processing

Whisper models can be computationally intensive. To handle real-time processing:
- Use smaller Whisper models ("tiny" or "base") for faster inference
- Implement audio buffering to process longer segments
- Consider edge computing solutions like NVIDIA Jetson
- Use GPU acceleration when available (CUDA support)

### Noise Reduction

Robot environments often have background noise. We'll implement:
- Audio preprocessing filters (high-pass, low-pass)
- Voice activity detection (VAD) to ignore silence
- Noise suppression algorithms (like RNNoise)
- Adaptive thresholding for speech detection

### Language Adaptation

Whisper supports multiple languages. For robotics applications:
- Identify the primary language(s) of interaction
- Fine-tune models if domain-specific vocabulary is needed
- Implement language detection for multilingual scenarios
- Use language-specific punctuation and formatting

### Hardware Constraints

Robots have limited computational resources:
- Optimize model size vs. accuracy trade-offs
- Consider using ONNX Runtime for faster inference
- Implement model quantization (int8) for reduced memory usage
- Use streaming inference to reduce latency

## Testing and Validation

We'll test our Whisper integration with:
- Various audio input scenarios (quiet, noisy, reverberant)
- Different speakers and accents
- Background noise conditions (fans, motors, conversations)
- Real-time performance requirements (latency, throughput)
- Edge cases (silence, overlapping speech, foreign languages)

### Performance Metrics

Key metrics for evaluation:
- **Word Error Rate (WER)**: Accuracy of transcription
- **Latency**: Time from audio input to text output
- **Throughput**: Number of audio samples processed per second
- **Robustness**: Performance under various noise conditions

## Integration with Robot Systems

### Audio Input Pipeline

For the humanoid robot, we'll integrate with:
- Microphone array for spatial audio processing
- Audio preprocessing for noise reduction
- Voice activity detection to minimize processing
- Automatic gain control for consistent audio levels

### ROS 2 Message Types

We'll use standard ROS 2 message types:
- `audio_common_msgs/AudioData` for raw audio input
- `std_msgs/String` for transcribed text
- Custom messages for confidence scores and metadata

## Deployment Considerations

### Edge Deployment

For deployment on robot hardware:
- Model optimization for target hardware
- Memory usage optimization
- Power consumption considerations
- Thermal management for sustained operation

### Configuration Management

Parameters that can be tuned:
- Model size selection
- Audio buffer duration
- Minimum audio length for processing
- Confidence thresholds
- Language selection

## Next Steps

In the next chapter, we'll integrate the transcribed text with our LLM planning system to enable the robot to understand and execute voice commands. This will form the foundation of our Vision-Language-Action (VLA) system by connecting voice input to cognitive planning capabilities.

## Summary

In this chapter, we've learned how to implement voice input capabilities for our humanoid robot using OpenAI's Whisper model. We covered the complete pipeline from audio input to text transcription, including real-time processing considerations, hardware constraints, and integration with ROS 2. By integrating Whisper with our robot system, we enable natural language interaction that serves as a crucial component of our Vision-Language-Action (VLA) system. This voice input capability will be essential for creating more intuitive and natural human-robot interactions in our autonomous humanoid system.