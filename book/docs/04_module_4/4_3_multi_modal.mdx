---
sidebar_position: 3
title: "Chapter 4.3: Multi-Modal Perception & Decision Making"
description: "Synthesizing voice, vision, and LLM reasoning for intelligent robot behavior"
---

# Chapter 4.3: Multi-Modal Perception & Decision Making

## Introduction

In this chapter, we'll explore how to synthesize multi-modal sensor data with LLM reasoning to create intelligent robot behaviors. We'll combine voice input from Chapter 4.1, LLM planning from Chapter 4.2, and vision systems to create a comprehensive multi-modal perception and decision-making system. This integration forms the core of our Vision-Language-Action (VLA) framework.

## Learning Objectives

By the end of this chapter, you will:
- Understand how to integrate multiple sensory modalities in robotics
- Learn to implement multi-modal fusion for enhanced perception
- Create a system that combines voice, vision, and LLM reasoning
- Implement adaptive decision-making based on multi-modal inputs
- Design a demonstration task that showcases multi-modal capabilities

## Multi-Modal Integration Architecture

### System Overview

Our multi-modal system will integrate:
1. **Voice Input**: Natural language commands and queries
2. **Vision Systems**: Object recognition, scene understanding
3. **LLM Reasoning**: High-level planning and reasoning
4. **Robot Actions**: Navigation, manipulation, communication

### Fusion Strategies

There are several approaches to multi-modal fusion:
- **Early Fusion**: Combine raw sensor data before processing
- **Late Fusion**: Process modalities separately, combine decisions
- **Intermediate Fusion**: Combine at feature or semantic level
- **Decision-Level Fusion**: Combine final outputs from each modality

For our VLA system, we'll use a combination of intermediate and decision-level fusion.

## Technical Implementation

### Multi-Modal Fusion Node

We'll create a fusion node that receives inputs from all modalities and coordinates the overall behavior. First, let's create the ROS 2 package for the multi-modal system:

```bash
mkdir -p robotics_ws/src/vla_multi_modal/vla_multi_modal
```

The package.xml file:

```xml
<?xml version="1.0"?>
<?xml-model href="http://download.ros.org/schema/package_format3.xsd" schematypens="http://www.w3.org/2001/XMLSchema"?>
<package format="3">
  <name>vla_multi_modal</name>
  <version>0.0.1</version>
  <description>Vision-Language-Action multi-modal fusion package</description>
  <maintainer email="maintainer@todo.todo">Maintainer</maintainer>
  <license>TODO</license>

  <depend>rclpy</depend>
  <depend>std_msgs</depend>
  <depend>sensor_msgs</depend>
  <depend>vision_msgs</depend>
  <depend>geometry_msgs</depend>
  <depend>cv_bridge</depend>
  <depend>vla_msgs</depend>

  <exec_depend>python3-opencv</exec_depend>
  <exec_depend>python3-numpy</exec_depend>

  <test_depend>ament_copyright</test_depend>
  <test_depend>ament_flake8</test_depend>
  <test_depend>ament_pep257</test_depend>
  <test_depend>python3-pytest</test_depend>

  <export>
    <build_type>ament_python</build_type>
  </export>
</package>
```

Now the fusion node implementation:

```python
#!/usr/bin/env python3

import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from sensor_msgs.msg import Image
from vision_msgs.msg import Detection2DArray
from geometry_msgs.msg import Point
from builtin_interfaces.msg import Time
from vla_msgs.msg import LLMPlan, PlanStep
import cv2
from cv_bridge import CvBridge
import numpy as np
import json
from collections import deque
import time


class MultiModalFusionNode(Node):
    def __init__(self):
        super().__init__('multi_modal_fusion_node')

        # Initialize components
        self.bridge = CvBridge()

        # Parameters
        self.vision_timeout = self.declare_parameter('vision_timeout', 5.0).value
        self.voice_timeout = self.declare_parameter('voice_timeout', 10.0).value
        self.fusion_strategy = self.declare_parameter('fusion_strategy', 'intermediate').value
        self.confidence_threshold = self.declare_parameter('confidence_threshold', 0.7).value
        self.max_context_history = self.declare_parameter('max_context_history', 10).value

        # Data storage for multi-modal context
        self.latest_image = None
        self.latest_detections = None
        self.latest_voice_command = None
        self.latest_plan = None
        self.context_history = deque(maxlen=self.max_context_history)

        # Time stamps for data freshness
        self.image_timestamp = None
        self.detections_timestamp = None
        self.voice_timestamp = None
        self.plan_timestamp = None

        # Publishers and subscribers
        self.image_sub = self.create_subscription(
            Image,
            'camera/image_raw',
            self.image_callback,
            10
        )

        self.detections_sub = self.create_subscription(
            Detection2DArray,
            'vision/detections',
            self.detections_callback,
            10
        )

        self.voice_sub = self.create_subscription(
            String,
            'transcribed_text',
            self.voice_callback,
            10
        )

        self.plan_sub = self.create_subscription(
            LLMPlan,
            'robot_plan',
            self.plan_callback,
            10
        )

        # Publishers for fused decisions
        self.decision_pub = self.create_publisher(
            String,
            'fused_decision',
            10
        )

        # Publisher for task context
        self.task_context_pub = self.create_publisher(
            String,
            'task_context',
            10
        )

        # Publisher for refined commands to LLM
        self.refined_command_pub = self.create_publisher(
            String,
            'refined_command',
            10
        )

        # Timer for fusion processing
        self.fusion_timer = self.create_timer(0.1, self.fusion_callback)

        self.get_logger().info("Multi-modal fusion node initialized")

    def image_callback(self, msg):
        """Handle incoming camera images"""
        try:
            self.latest_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")
            self.image_timestamp = msg.header.stamp
        except Exception as e:
            self.get_logger().error(f"Error processing image: {e}")

    def detections_callback(self, msg):
        """Handle incoming object detections"""
        self.latest_detections = msg
        self.detections_timestamp = msg.header.stamp

    def voice_callback(self, msg):
        """Handle incoming voice commands"""
        self.latest_voice_command = msg.data
        self.voice_timestamp = self.get_clock().now().to_msg()

    def plan_callback(self, msg):
        """Handle incoming LLM plans"""
        self.latest_plan = msg
        self.plan_timestamp = msg.timestamp

    def fusion_callback(self):
        """Main fusion logic - called periodically"""
        # Check if we have recent data from multiple modalities
        current_time = self.get_clock().now().to_msg()

        # Check for recent voice command
        if (self.latest_voice_command and
            self.time_diff(current_time, self.voice_timestamp) < self.voice_timeout):

            # Create task context combining all available information
            task_context = self.create_task_context()

            # Store in history
            self.context_history.append(task_context)

            # Publish task context for decision making
            context_msg = String()
            context_msg.data = json.dumps(task_context)
            self.task_context_pub.publish(context_msg)

            # Make fused decision based on all modalities
            decision = self.make_fused_decision(task_context)
            if decision:
                decision_msg = String()
                decision_msg.data = decision
                self.decision_pub.publish(decision_msg)

                # If decision requires refined planning, send to LLM
                if self.should_refine_plan(decision, task_context):
                    refined_command = self.create_refined_command(decision, task_context)
                    refined_cmd_msg = String()
                    refined_cmd_msg.data = refined_command
                    self.refined_command_pub.publish(refined_cmd_msg)

    def create_task_context(self):
        """Create comprehensive task context from all modalities"""
        context = {
            'timestamp': self.get_clock().now().to_msg().sec,
            'voice_command': self.latest_voice_command,
            'vision_data': self.extract_vision_context(),
            'robot_state': self.get_robot_state(),
            'environment_context': self.extract_environment_context(),
            'previous_context': list(self.context_history)[-3:] if self.context_history else []  # Last 3 contexts
        }
        return context

    def extract_vision_context(self):
        """Extract relevant information from vision data"""
        if not self.latest_detections:
            return {}

        vision_context = {
            'objects': [],
            'spatial_relations': [],
            'scene_description': 'No detections available',
            'confidence_summary': 0.0
        }

        # Extract object information
        total_confidence = 0.0
        for detection in self.latest_detections.detections:
            if detection.results:  # Check if results exist
                hypothesis = detection.results[0].hypothesis
                obj_info = {
                    'label': hypothesis.class_id,
                    'confidence': hypothesis.score,
                    'bbox': {
                        'x': detection.bbox.center.x,
                        'y': detection.bbox.center.y,
                        'width': detection.bbox.size_x,
                        'height': detection.bbox.size_y
                    },
                    'center_point': {
                        'x': detection.bbox.center.x,
                        'y': detection.bbox.center.y
                    }
                }
                vision_context['objects'].append(obj_info)
                total_confidence += hypothesis.score

        # Calculate average confidence
        if vision_context['objects']:
            vision_context['confidence_summary'] = total_confidence / len(vision_context['objects'])

        # Analyze spatial relationships between objects
        if len(vision_context['objects']) > 1:
            vision_context['spatial_relations'] = self.compute_spatial_relations(
                vision_context['objects']
            )

        return vision_context

    def compute_spatial_relations(self, objects):
        """Compute spatial relationships between detected objects"""
        relations = []
        for i, obj1 in enumerate(objects):
            for j, obj2 in enumerate(objects):
                if i != j:
                    # Calculate relative positions
                    dx = obj2['center_point']['x'] - obj1['center_point']['x']
                    dy = obj2['center_point']['y'] - obj1['center_point']['y']

                    # Determine spatial relationship
                    distance = np.sqrt(dx*dx + dy*dy)
                    angle = np.arctan2(dy, dx) * 180 / np.pi  # Convert to degrees

                    relation = {
                        'from': obj1['label'],
                        'to': obj2['label'],
                        'distance': distance,
                        'angle_degrees': angle,
                        'relative_position': self.determine_relative_position(dx, dy)
                    }
                    relations.append(relation)
        return relations

    def determine_relative_position(self, dx, dy):
        """Determine relative position based on dx, dy"""
        # Define sectors (8 directions)
        angle = np.arctan2(dy, dx)
        angle_deg = angle * 180 / np.pi

        if -22.5 <= angle_deg < 22.5:
            return "right"
        elif 22.5 <= angle_deg < 67.5:
            return "bottom-right"
        elif 67.5 <= angle_deg < 112.5:
            return "bottom"
        elif 112.5 <= angle_deg < 157.5:
            return "bottom-left"
        elif 157.5 <= angle_deg or angle_deg < -157.5:
            return "left"
        elif -157.5 <= angle_deg < -112.5:
            return "top-left"
        elif -112.5 <= angle_deg < -67.5:
            return "top"
        else:  # -67.5 <= angle_deg < -22.5
            return "top-right"

    def get_robot_state(self):
        """Get current robot state (position, battery, etc.)"""
        # This would typically come from robot state publisher
        # For simulation purposes, return mock data
        return {
            'position': {'x': 0.0, 'y': 0.0, 'theta': 0.0},
            'battery_level': 0.8,
            'current_task': 'idle',
            'capabilities': ['navigation', 'manipulation', 'communication'],
            'manipulation_status': 'available',
            'navigation_status': 'ready'
        }

    def extract_environment_context(self):
        """Extract environment context from available data"""
        # This would integrate with mapping and localization systems
        return {
            'room_type': 'unknown',  # Would be determined from scene analysis
            'lighting_condition': 'normal',  # Would be analyzed from image
            'obstacles': [],  # Would come from navigation system
            'navigable_areas': [],  # Would come from mapping system
            'landmarks': []  # Would come from SLAM system
        }

    def make_fused_decision(self, context):
        """Make decision by fusing information from all modalities"""
        voice_cmd = context.get('voice_command', '').lower()
        vision_data = context.get('vision_data', {})
        robot_state = context.get('robot_state', {})

        # Analyze confidence levels
        vision_confidence = vision_data.get('confidence_summary', 0.0)
        voice_confidence = 1.0  # Assume voice input is reliable

        # If vision confidence is too low, request clarification or exploration
        if vision_confidence < self.confidence_threshold and 'where' not in voice_cmd and 'what' not in voice_cmd:
            return f"request_visual_clarification: {voice_cmd}"

        # Example decision logic based on multi-modal input
        decision = ""

        if 'bring me' in voice_cmd or 'get me' in voice_cmd or 'fetch' in voice_cmd:
            # Object retrieval task
            target_object = self.extract_target_object(voice_cmd)
            if target_object:
                # Check if object is visible
                visible_objects = [obj['label'] for obj in vision_data.get('objects', [])]
                if target_object in visible_objects:
                    # Find the specific object instance
                    target_obj_data = None
                    for obj in vision_data.get('objects', []):
                        if obj['label'] == target_object:
                            target_obj_data = obj
                            break

                    if target_obj_data:
                        decision = f"object_retrieval_confirmed: {target_object}, position: ({target_obj_data['center_point']['x']}, {target_obj_data['center_point']['y']})"
                    else:
                        decision = f"object_found: {target_object}"
                else:
                    decision = f"searching_for: {target_object}"
            else:
                decision = "object_not_specified"

        elif 'go to' in voice_cmd or 'move to' in voice_cmd or 'navigate to' in voice_cmd:
            # Navigation task
            target_location = self.extract_target_location(voice_cmd)
            if target_location:
                decision = f"navigate_to: {target_location}"
            else:
                decision = "location_not_specified"

        elif 'what do you see' in voice_cmd or 'describe scene' in voice_cmd:
            # Scene description task
            objects = [obj['label'] for obj in vision_data.get('objects', [])]
            if objects:
                decision = f"scene_description: I see {', '.join(objects)}"
            else:
                decision = "no_objects_detected"

        elif 'where is' in voice_cmd or 'find' in voice_cmd:
            # Object localization task
            target_object = self.extract_target_object(voice_cmd)
            if target_object:
                visible_objects = [obj for obj in vision_data.get('objects', []) if obj['label'] == target_object]
                if visible_objects:
                    obj_info = visible_objects[0]
                    decision = f"object_location: The {target_object} is at coordinates ({obj_info['center_point']['x']}, {obj_info['center_point']['y']}) with confidence {obj_info['confidence']:.2f}"
                else:
                    decision = f"object_not_found: {target_object}"
            else:
                decision = "object_not_specified"

        else:
            # Default case - pass to LLM planner with context
            decision = f"llm_planning_needed_with_context: {voice_cmd}"

        return decision

    def extract_target_object(self, command):
        """Extract target object from voice command using keyword matching"""
        # Extended list of common objects
        common_objects = [
            'cup', 'bottle', 'water bottle', 'coffee cup', 'mug', 'glass',
            'book', 'phone', 'mobile', 'cellphone', 'keys', 'wallet',
            'pen', 'pencil', 'paper', 'notebook', 'laptop', 'tablet',
            'chair', 'table', 'sofa', 'couch', 'bed', 'desk',
            'apple', 'banana', 'orange', 'fruit', 'snack', 'food',
            'ball', 'toy', 'remote', 'tv remote', 'bowl', 'plate',
            'box', 'bag', 'backpack', 'purse', 'hat', 'shoe'
        ]

        command_lower = command.lower()
        for obj in common_objects:
            if obj in command_lower:
                return obj

        # If no exact match, try partial matching
        for obj in common_objects:
            if any(word in command_lower for word in obj.split()):
                return obj

        return None

    def extract_target_location(self, command):
        """Extract target location from voice command"""
        # Extended list of common locations
        common_locations = [
            'kitchen', 'living room', 'bedroom', 'office', 'bathroom',
            'dining room', 'hallway', 'garage', 'garden', 'patio',
            'entrance', 'dining area', 'work area', 'study', 'library',
            'dormitory', 'classroom', 'laboratory', 'workshop'
        ]

        command_lower = command.lower()
        for loc in common_locations:
            if loc in command_lower:
                return loc

        # If no exact match, try partial matching
        for loc in common_locations:
            if any(word in command_lower for word in loc.split()):
                return loc

        return None

    def should_refine_plan(self, decision, context):
        """Determine if the decision requires refined planning"""
        return ('object_retrieval_confirmed' in decision or
                'navigate_to' in decision or
                'llm_planning_needed_with_context' in decision)

    def create_refined_command(self, decision, context):
        """Create a refined command for the LLM planner with multi-modal context"""
        original_command = context.get('voice_command', '')
        vision_data = context.get('vision_data', {})

        if 'object_retrieval_confirmed' in decision:
            # Extract object and position
            import re
            obj_match = re.search(r'object_retrieval_confirmed: ([^,]+)', decision)
            pos_match = re.search(r'position: \(([^)]+)\)', decision)

            if obj_match and pos_match:
                target_obj = obj_match.group(1)
                pos_str = pos_match.group(1)
                pos_parts = pos_str.split(', ')
                if len(pos_parts) >= 2:
                    x, y = float(pos_parts[0]), float(pos_parts[1])
                    return f"Go to position ({x}, {y}) and pick up the {target_obj}, then bring it to me"

        elif 'navigate_to' in decision:
            # Extract target location
            import re
            loc_match = re.search(r'navigate_to: (.+)', decision)
            if loc_match:
                target_loc = loc_match.group(1)
                return f"Navigate to the {target_loc} and wait for further instructions"

        # Default: use original command with context
        return f"{original_command}. The current context is: {json.dumps(vision_data, indent=2)[:500]}..."

    def time_diff(self, time1, time2):
        """Calculate time difference in seconds"""
        if not time2:
            return float('inf')
        return abs(time1.sec - time2.sec + (time1.nanosec - time2.nanosec) / 1e9)

def main(args=None):
    rclpy.init(args=args)
    node = MultiModalFusionNode()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        node.get_logger().info("Multi-modal fusion node shutting down...")
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

### Vision Processing Integration

Our vision system will provide:
- Object detection and recognition
- Spatial relationship analysis
- Scene understanding
- Visual attention mechanisms

### Adaptive Decision Making

The system will adapt its behavior based on:
- Confidence levels from different modalities
- Environmental conditions
- Task requirements
- Robot capabilities

## Demonstration Task

Let's implement a simple task that combines all three modalities: voice, vision, and LLM planning.

### Task: Fetch and Deliver

The robot will:
1. Listen for voice command: "Please bring me the red cup"
2. Use vision to locate the red cup in the environment
3. Use LLM to plan the sequence of actions needed
4. Execute the plan to fetch and deliver the object

### Implementation

```python
class MultiModalDemoNode(Node):
    def __init__(self):
        super().__init__('multi_modal_demo_node')

        # State machine for the demo
        self.state = 'waiting_for_command'  # waiting_for_command, locating_object, planning, executing, completed

        # Subscribers
        self.decision_sub = self.create_subscription(
            String,
            'fused_decision',
            self.decision_callback,
            10
        )

        # Publishers
        self.command_pub = self.create_publisher(
            String,
            'command_input',
            10
        )

        # Timer for state management
        self.demo_timer = self.create_timer(1.0, self.demo_callback)

    def decision_callback(self, msg):
        """Handle fused decisions"""
        decision = msg.data
        self.get_logger().info(f"Demo received decision: {decision}")

        if self.state == 'waiting_for_command':
            if 'object_found' in decision:
                self.state = 'planning'
                # Trigger LLM planning
                command_msg = String()
                command_msg.data = f"Pick up the object and bring it to the user"
                self.command_pub.publish(command_msg)
            elif 'searching_for' in decision:
                self.state = 'locating_object'
                # Robot should navigate to search for the object
                pass

    def demo_callback(self):
        """Manage demo state"""
        self.get_logger().info(f"Demo state: {self.state}")

def main(args=None):
    rclpy.init(args=args)
    node = MultiModalDemoNode()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()
```

## Integration with Existing Systems

### Voice-Vision Coordination

The system will:
- Use voice commands to direct visual attention
- Validate voice interpretations with visual data
- Refine understanding through multi-modal feedback

### Vision-LLM Coordination

The system will:
- Provide visual context to LLM for better planning
- Use LLM reasoning to interpret complex visual scenes
- Validate LLM plans against visual observations

### Voice-LLM Coordination

The system will:
- Use voice commands as input to LLM planning
- Generate natural language feedback from LLM
- Handle ambiguous commands through clarification

## Challenges and Solutions

### Data Synchronization

Multi-modal systems face synchronization challenges:
- **Time delays**: Different modalities process at different speeds
- **Fusion timing**: Determine optimal fusion points
- **Data freshness**: Ensure data is recent enough to be relevant

**Solutions**:
- Timestamp-based data validation
- Asynchronous processing with buffering
- Adaptive fusion based on data availability

### Confidence Integration

Different modalities have different confidence levels:
- Weight decisions based on modality confidence
- Handle low-confidence situations gracefully
- Implement fallback strategies

### Computational Complexity

Multi-modal processing is computationally intensive:
- Optimize individual modalities for efficiency
- Use hierarchical processing
- Implement selective attention mechanisms

## Evaluation Metrics

### Performance Metrics

- **Task completion rate**: Percentage of tasks successfully completed
- **Response time**: Time from command to action initiation
- **Accuracy**: Correctness of object identification and actions
- **Robustness**: Performance under various environmental conditions

### Quality Metrics

- **Naturalness**: How natural the interaction feels
- **Efficiency**: How efficiently tasks are completed
- **Adaptability**: Ability to handle novel situations
- **Reliability**: Consistency of performance

## Next Steps

In the final chapter of Module 4, we'll bring together all components into a complete autonomous humanoid system. We'll create a capstone project that demonstrates the full Vision-Language-Action (VLA) capabilities we've developed throughout this module.

## Summary

In this chapter, we've learned how to integrate multiple sensory modalities in our humanoid robot system. We created a multi-modal fusion architecture that combines voice input, visual perception, and LLM reasoning to create intelligent robot behaviors. We implemented adaptive decision-making systems that can handle complex, real-world scenarios by leveraging the strengths of each modality. This multi-modal integration forms the core of our Vision-Language-Action (VLA) framework and enables our robot to perform sophisticated tasks that require understanding of language, vision, and physical interaction.